{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1cefbb",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "* pdf files --> chunks --> embeddings --> vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2756a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document structure \n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "#loads different types of documets format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dc0c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\n",
    "\n",
    "    page_content=\"Hi this is Praveen Sunkara, this file contains page_content\",\n",
    "    metadata={\n",
    "        \"source\":\"written.txt\",\n",
    "        \"page\":1,\n",
    "        \"time\":\"3-11-2025\",\n",
    "        \"author\":\"praveensunkara\",\n",
    "        \"location\":\"Vijayawada\",\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "665481fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'written.txt', 'page': 1, 'time': '3-11-2025', 'author': 'praveensunkara', 'location': 'Vijayawada'}\n",
      "Hi this is Praveen Sunkara, this file contains page_content\n"
     ]
    }
   ],
   "source": [
    "print(doc.metadata)\n",
    "print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c770185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('source', 'written.txt')\n",
      "('page', 1)\n",
      "('time', '3-11-2025')\n",
      "('author', 'praveensunkara')\n",
      "('location', 'Vijayawada')\n"
     ]
    }
   ],
   "source": [
    "for i in doc.metadata.items():\n",
    "    # print(i[0])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48f25b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a folder\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b8b656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = {\n",
    "    \"../data/text_files/python_introduction.txt\":\"\"\"\n",
    "    python programming introduction\n",
    "\n",
    "Python is a high-level, general-purpose programming language known for its readability and simple syntax, making it an excellent choice for beginners and experienced developers alike. It is used for a wide range of applications, from web development and automation to data science and artificial intelligence. Key Features\n",
    "Easy to Learn and Use: Python's syntax uses plain English keywords and relies on indentation rather than semicolons or curly brackets, making it highly readable and reducing the learning curve.\n",
    "Interpreted Language: Python code is executed line by line by an interpreter, which allows for rapid prototyping and easier debugging, as errors can be identified quickly.\n",
    "Dynamically Typed: You do not need to declare variable types explicitly; Python determines the type at runtime, offering flexibility and faster code development.\n",
    "Large Standard Library and Ecosystem: Python comes with an extensive standard library and a vast collection of third-party libraries (accessible via the Python Package Index, or PyPI) for various tasks, so developers do not have to write code from scratch.\n",
    "Cross-Platform: Python runs on various operating systems, including Windows, macOS, and Linux, with minimal or no code modification.\n",
    "Multi-Paradigm Support: It supports multiple programming styles, including procedural, object-oriented, and functional programming, offering versatility for different project needs.\n",
    "Open Source: Python is free to use, modify, and distribute, fostering a large and active community that contributes to its continuous improvement and support. \n",
    "Primary Applications\n",
    "Web Development: Used for backend development with frameworks like Django and Flask.\n",
    "Data Science and Machine Learning (ML): A leading language in these fields due to powerful libraries such as NumPy, Pandas, TensorFlow, and scikit-learn.\n",
    "Automation and Scripting: Ideal for automating repetitive tasks, such as file management, data conversion, and software testing.\n",
    "Software Development: Used for bug tracking, build control, and developing desktop applications.\n",
    "Scientific Computing: Relied upon by researchers and engineers for complex calculations and simulations. \n",
    "History\n",
    "Python was created by the Dutch programmer Guido van Rossum in the late 1980s and first released in 1991. He started it as a hobby project to stay busy during the Christmas holidays. The name \"Python\" was inspired by the British comedy TV show Monty Python's Flying Circus, of which van Rossum was a fan. The language is currently maintained by the non-profit Python Software Foundation (PSF). \n",
    "    \n",
    "    \"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5c5f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('../data/text_files/python_introduction.txt', '\\n    python programming introduction\\n\\nPython is a high-level, general-purpose programming language known for its readability and simple syntax, making it an excellent choice for beginners and experienced developers alike. It is used for a wide range of applications, from web development and automation to data science and artificial intelligence. Key Features\\nEasy to Learn and Use: Python\\'s syntax uses plain English keywords and relies on indentation rather than semicolons or curly brackets, making it highly readable and reducing the learning curve.\\nInterpreted Language: Python code is executed line by line by an interpreter, which allows for rapid prototyping and easier debugging, as errors can be identified quickly.\\nDynamically Typed: You do not need to declare variable types explicitly; Python determines the type at runtime, offering flexibility and faster code development.\\nLarge Standard Library and Ecosystem: Python comes with an extensive standard library and a vast collection of third-party libraries (accessible via the Python Package Index, or PyPI) for various tasks, so developers do not have to write code from scratch.\\nCross-Platform: Python runs on various operating systems, including Windows, macOS, and Linux, with minimal or no code modification.\\nMulti-Paradigm Support: It supports multiple programming styles, including procedural, object-oriented, and functional programming, offering versatility for different project needs.\\nOpen Source: Python is free to use, modify, and distribute, fostering a large and active community that contributes to its continuous improvement and support. \\nPrimary Applications\\nWeb Development: Used for backend development with frameworks like Django and Flask.\\nData Science and Machine Learning (ML): A leading language in these fields due to powerful libraries such as NumPy, Pandas, TensorFlow, and scikit-learn.\\nAutomation and Scripting: Ideal for automating repetitive tasks, such as file management, data conversion, and software testing.\\nSoftware Development: Used for bug tracking, build control, and developing desktop applications.\\nScientific Computing: Relied upon by researchers and engineers for complex calculations and simulations. \\nHistory\\nPython was created by the Dutch programmer Guido van Rossum in the late 1980s and first released in 1991. He started it as a hobby project to stay busy during the Christmas holidays. The name \"Python\" was inspired by the British comedy TV show Monty Python\\'s Flying Circus, of which van Rossum was a fan. The language is currently maintained by the non-profit Python Software Foundation (PSF). \\n    \\n    ')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5b37995",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath,content in text1.items():\n",
    "    with open(filepath,'w',encoding='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a91215b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    python programming introduction\n",
      "\n",
      "Python is a high-level, general-purpose programming language known for its readability and simple syntax, making it an excellent choice for beginners and experienced developers alike. It is used for a wide range of applications, from web development and automation to data science and artificial intelligence. Key Features\n",
      "Easy to Learn and Use: Python's syntax uses plain English keywords and relies on indentation rather than semicolons or curly brackets, making it highly readable and reducing the learning curve.\n",
      "Interpreted Language: Python code is executed line by line by an interpreter, which allows for rapid prototyping and easier debugging, as errors can be identified quickly.\n",
      "Dynamically Typed: You do not need to declare variable types explicitly; Python determines the type at runtime, offering flexibility and faster code development.\n",
      "Large Standard Library and Ecosystem: Python comes with an extensive standard library and a vast collection of third-party libraries (accessible via the Python Package Index, or PyPI) for various tasks, so developers do not have to write code from scratch.\n",
      "Cross-Platform: Python runs on various operating systems, including Windows, macOS, and Linux, with minimal or no code modification.\n",
      "Multi-Paradigm Support: It supports multiple programming styles, including procedural, object-oriented, and functional programming, offering versatility for different project needs.\n",
      "Open Source: Python is free to use, modify, and distribute, fostering a large and active community that contributes to its continuous improvement and support. \n",
      "Primary Applications\n",
      "Web Development: Used for backend development with frameworks like Django and Flask.\n",
      "Data Science and Machine Learning (ML): A leading language in these fields due to powerful libraries such as NumPy, Pandas, TensorFlow, and scikit-learn.\n",
      "Automation and Scripting: Ideal for automating repetitive tasks, such as file management, data conversion, and software testing.\n",
      "Software Development: Used for bug tracking, build control, and developing desktop applications.\n",
      "Scientific Computing: Relied upon by researchers and engineers for complex calculations and simulations. \n",
      "History\n",
      "Python was created by the Dutch programmer Guido van Rossum in the late 1980s and first released in 1991. He started it as a hobby project to stay busy during the Christmas holidays. The name \"Python\" was inspired by the British comedy TV show Monty Python's Flying Circus, of which van Rossum was a fan. The language is currently maintained by the non-profit Python Software Foundation (PSF). \n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "with open(filepath,'r') as f:\n",
    "    con = f.read()\n",
    "print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb3664c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 ={\n",
    "    \"../data/text_files/machine_learning_intro.txt\":\"\"\"\n",
    "\n",
    "    Intro Machine Learning \n",
    "\n",
    "Machine learning (ML) is a subfield of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed [1, 2]. The core idea is to build systems that can automatically learn to identify patterns, make predictions, or generate insights based on historical information. \n",
    "Core Concepts\n",
    "Data: ML models learn from data, which can include images, text, numbers, or sound. The quality and quantity of this data are crucial for effective learning [1].\n",
    "Algorithms: These are the methods and mathematical formulas used by computers to process data and find patterns. Common examples include linear regression, decision trees, and neural networks [1, 3].\n",
    "Training: This is the process where the algorithm analyzes the data to learn patterns. The resulting learned patterns form a \"model\" [1].\n",
    "Model: The output of the training process. Once trained, the model can be used to make predictions or decisions on new, unseen data [1]. \n",
    "Types of Machine Learning\n",
    "Machine learning is typically divided into three main categories based on how the learning process is supervised [1, 2]: \n",
    "Supervised Learning: The model is trained on labeled data, meaning the input data has a corresponding \"correct\" output. The goal is to learn a mapping from inputs to outputs [1].\n",
    "Examples: Spam detection (identifying emails as \"spam\" or \"not spam\"), predicting house prices based on features like size and location [1].\n",
    "Unsupervised Learning: The model is given unlabeled data and must find its own patterns and structures within the data. There are no \"correct\" outputs provided [1].\n",
    "Examples: Customer segmentation (grouping customers with similar behaviors), anomaly detection (identifying unusual activities in a system) [1].\n",
    "Reinforcement Learning: An agent learns to make decisions through trial and error in an interactive environment. It receives rewards for desirable actions and penalties for undesirable ones, aiming to maximize its cumulative reward [1, 2].\n",
    "Examples: Training a self-driving car to navigate traffic, game AI (like AlphaGo or chess engines) [1, 2]. \n",
    "Key Applications\n",
    "Machine learning is ubiquitous in modern life [1]: \n",
    "Recommendation Systems: Services like Netflix and Amazon use ML to suggest movies or products you might like [1].\n",
    "Natural Language Processing (NLP): Enables computers to understand human language, used in virtual assistants (Siri, Alexa), translation tools (Google Translate), and chatbots [1].\n",
    "Computer Vision: Allows computers to \"see\" and interpret images and videos, used in facial recognition, medical imaging analysis, and self-driving cars [1, 2].\n",
    "Healthcare: ML models help diagnose diseases earlier, analyze medical images, and personalize treatment plans [1, 2]. \n",
    "In essence, machine learning is a powerful tool for extracting knowledge from data, enabling smarter, more efficient systems across a vast array of industries\n",
    "\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in text2.items():\n",
    "    with open(filepath,'w',encoding='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b957f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Intro Machine Learning \n",
      "\n",
      "Machine learning (ML) is a subfield of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed [1, 2]. The core idea is to build systems that can automatically learn to identify patterns, make predictions, or generate insights based on historical information. \n",
      "Core Concepts\n",
      "Data: ML models learn from data, which can include images, text, numbers, or sound. The quality and quantity of this data are crucial for effective learning [1].\n",
      "Algorithms: These are the methods and mathematical formulas used by computers to process data and find patterns. Common examples include linear regression, decision trees, and neural networks [1, 3].\n",
      "Training: This is the process where the algorithm analyzes the data to learn patterns. The resulting learned patterns form a \"model\" [1].\n",
      "Model: The output of the training process. Once trained, the model can be used to make predictions or decisions on new, unseen data [1]. \n",
      "Types of Machine Learning\n",
      "Machine learning is typically divided into three main categories based on how the learning process is supervised [1, 2]: \n",
      "Supervised Learning: The model is trained on labeled data, meaning the input data has a corresponding \"correct\" output. The goal is to learn a mapping from inputs to outputs [1].\n",
      "Examples: Spam detection (identifying emails as \"spam\" or \"not spam\"), predicting house prices based on features like size and location [1].\n",
      "Unsupervised Learning: The model is given unlabeled data and must find its own patterns and structures within the data. There are no \"correct\" outputs provided [1].\n",
      "Examples: Customer segmentation (grouping customers with similar behaviors), anomaly detection (identifying unusual activities in a system) [1].\n",
      "Reinforcement Learning: An agent learns to make decisions through trial and error in an interactive environment. It receives rewards for desirable actions and penalties for undesirable ones, aiming to maximize its cumulative reward [1, 2].\n",
      "Examples: Training a self-driving car to navigate traffic, game AI (like AlphaGo or chess engines) [1, 2]. \n",
      "Key Applications\n",
      "Machine learning is ubiquitous in modern life [1]: \n",
      "Recommendation Systems: Services like Netflix and Amazon use ML to suggest movies or products you might like [1].\n",
      "Natural Language Processing (NLP): Enables computers to understand human language, used in virtual assistants (Siri, Alexa), translation tools (Google Translate), and chatbots [1].\n",
      "Computer Vision: Allows computers to \"see\" and interpret images and videos, used in facial recognition, medical imaging analysis, and self-driving cars [1, 2].\n",
      "Healthcare: ML models help diagnose diseases earlier, analyze medical images, and personalize treatment plans [1, 2]. \n",
      "In essence, machine learning is a powerful tool for extracting knowledge from data, enabling smarter, more efficient systems across a vast array of industries\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "with open(filepath,'r') as f:\n",
    "    con = f.read()\n",
    "\n",
    "print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83a84408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "    python programming introduction\n",
      "\n",
      "Python is a high-level, general-purpose programming language known for its readability and simple syntax, making it an excellent choice for beginners and experienced developers alike. It is used for a wide range of applications, from web development and automation to data science and artificial intelligence. Key Features\n",
      "Easy to Learn and Use: Python's syntax uses plain English keywords and relies on indentation rather than semicolons or curly brackets, making it highly readable and reducing the learning curve.\n",
      "Interpreted Language: Python code is executed line by line by an interpreter, which allows for rapid prototyping and easier debugging, as errors can be identified quickly.\n",
      "Dynamically Typed: You do not need to declare variable types explicitly; Python determines the type at runtime, offering flexibility and faster code development.\n",
      "Large Standard Library and Ecosystem: Python comes with an extensive standard library and a vast collection of third-party libraries (accessible via the Python Package Index, or PyPI) for various tasks, so developers do not have to write code from scratch.\n",
      "Cross-Platform: Python runs on various operating systems, including Windows, macOS, and Linux, with minimal or no code modification.\n",
      "Multi-Paradigm Support: It supports multiple programming styles, including procedural, object-oriented, and functional programming, offering versatility for different project needs.\n",
      "Open Source: Python is free to use, modify, and distribute, fostering a large and active community that contributes to its continuous improvement and support. \n",
      "Primary Applications\n",
      "Web Development: Used for backend development with frameworks like Django and Flask.\n",
      "Data Science and Machine Learning (ML): A leading language in these fields due to powerful libraries such as NumPy, Pandas, TensorFlow, and scikit-learn.\n",
      "Automation and Scripting: Ideal for automating repetitive tasks, such as file management, data conversion, and software testing.\n",
      "Software Development: Used for bug tracking, build control, and developing desktop applications.\n",
      "Scientific Computing: Relied upon by researchers and engineers for complex calculations and simulations. \n",
      "History\n",
      "Python was created by the Dutch programmer Guido van Rossum in the late 1980s and first released in 1991. He started it as a hobby project to stay busy during the Christmas holidays. The name \"Python\" was inspired by the British comedy TV show Monty Python's Flying Circus, of which van Rossum was a fan. The language is currently maintained by the non-profit Python Software Foundation (PSF). \n",
      "    \n",
      "    ' metadata={'source': '../data/text_files/python_introduction.txt'}\n"
     ]
    }
   ],
   "source": [
    "# document_loader with langchain\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('../data/text_files/python_introduction.txt',encoding='utf-8')\n",
    "\n",
    "document = loader.load()\n",
    "\n",
    "print(document[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc9ea143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning_intro.txt'}, page_content='\\n\\n    Intro Machine Learning \\n\\nMachine learning (ML) is a subfield of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed [1, 2]. The core idea is to build systems that can automatically learn to identify patterns, make predictions, or generate insights based on historical information. \\nCore Concepts\\nData: ML models learn from data, which can include images, text, numbers, or sound. The quality and quantity of this data are crucial for effective learning [1].\\nAlgorithms: These are the methods and mathematical formulas used by computers to process data and find patterns. Common examples include linear regression, decision trees, and neural networks [1, 3].\\nTraining: This is the process where the algorithm analyzes the data to learn patterns. The resulting learned patterns form a \"model\" [1].\\nModel: The output of the training process. Once trained, the model can be used to make predictions or decisions on new, unseen data [1]. \\nTypes of Machine Learning\\nMachine learning is typically divided into three main categories based on how the learning process is supervised [1, 2]: \\nSupervised Learning: The model is trained on labeled data, meaning the input data has a corresponding \"correct\" output. The goal is to learn a mapping from inputs to outputs [1].\\nExamples: Spam detection (identifying emails as \"spam\" or \"not spam\"), predicting house prices based on features like size and location [1].\\nUnsupervised Learning: The model is given unlabeled data and must find its own patterns and structures within the data. There are no \"correct\" outputs provided [1].\\nExamples: Customer segmentation (grouping customers with similar behaviors), anomaly detection (identifying unusual activities in a system) [1].\\nReinforcement Learning: An agent learns to make decisions through trial and error in an interactive environment. It receives rewards for desirable actions and penalties for undesirable ones, aiming to maximize its cumulative reward [1, 2].\\nExamples: Training a self-driving car to navigate traffic, game AI (like AlphaGo or chess engines) [1, 2]. \\nKey Applications\\nMachine learning is ubiquitous in modern life [1]: \\nRecommendation Systems: Services like Netflix and Amazon use ML to suggest movies or products you might like [1].\\nNatural Language Processing (NLP): Enables computers to understand human language, used in virtual assistants (Siri, Alexa), translation tools (Google Translate), and chatbots [1].\\nComputer Vision: Allows computers to \"see\" and interpret images and videos, used in facial recognition, medical imaging analysis, and self-driving cars [1, 2].\\nHealthcare: ML models help diagnose diseases earlier, analyze medical images, and personalize treatment plans [1, 2]. \\nIn essence, machine learning is a powerful tool for extracting knowledge from data, enabling smarter, more efficient systems across a vast array of industries\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_introduction.txt'}, page_content='\\n    python programming introduction\\n\\nPython is a high-level, general-purpose programming language known for its readability and simple syntax, making it an excellent choice for beginners and experienced developers alike. It is used for a wide range of applications, from web development and automation to data science and artificial intelligence. Key Features\\nEasy to Learn and Use: Python\\'s syntax uses plain English keywords and relies on indentation rather than semicolons or curly brackets, making it highly readable and reducing the learning curve.\\nInterpreted Language: Python code is executed line by line by an interpreter, which allows for rapid prototyping and easier debugging, as errors can be identified quickly.\\nDynamically Typed: You do not need to declare variable types explicitly; Python determines the type at runtime, offering flexibility and faster code development.\\nLarge Standard Library and Ecosystem: Python comes with an extensive standard library and a vast collection of third-party libraries (accessible via the Python Package Index, or PyPI) for various tasks, so developers do not have to write code from scratch.\\nCross-Platform: Python runs on various operating systems, including Windows, macOS, and Linux, with minimal or no code modification.\\nMulti-Paradigm Support: It supports multiple programming styles, including procedural, object-oriented, and functional programming, offering versatility for different project needs.\\nOpen Source: Python is free to use, modify, and distribute, fostering a large and active community that contributes to its continuous improvement and support. \\nPrimary Applications\\nWeb Development: Used for backend development with frameworks like Django and Flask.\\nData Science and Machine Learning (ML): A leading language in these fields due to powerful libraries such as NumPy, Pandas, TensorFlow, and scikit-learn.\\nAutomation and Scripting: Ideal for automating repetitive tasks, such as file management, data conversion, and software testing.\\nSoftware Development: Used for bug tracking, build control, and developing desktop applications.\\nScientific Computing: Relied upon by researchers and engineers for complex calculations and simulations. \\nHistory\\nPython was created by the Dutch programmer Guido van Rossum in the late 1980s and first released in 1991. He started it as a hobby project to stay busy during the Christmas holidays. The name \"Python\" was inspired by the British comedy TV show Monty Python\\'s Flying Circus, of which van Rossum was a fan. The language is currently maintained by the non-profit Python Software Foundation (PSF). \\n    \\n    ')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading all the txt files from the directory\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/text_files',\n",
    "    glob ='**/*.txt',\n",
    "    loader_cls = TextLoader,\n",
    "    loader_kwargs ={'encoding':'utf-8'},\n",
    "    show_progress = False\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3e7042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Efﬁcient Estimation of Word Representations in\\nVector Space\\nTomas Mikolov\\nGoogle Inc., Mountain View, CA\\ntmikolov@google.com\\nKai Chen\\nGoogle Inc., Mountain View, CA\\nkaichen@google.com\\nGreg Corrado\\nGoogle Inc., Mountain View, CA\\ngcorrado@google.com\\nJeffrey Dean\\nGoogle Inc., Mountain View, CA\\njeff@google.com\\nAbstract\\nWe propose two novel model architectures for computing continuous vector repre-\\nsentations of words from very large data sets. The quality of these representations\\nis measured in a word similarity task, and the results are compared to the previ-\\nously best performing techniques based on different types of neural networks. We\\nobserve large improvements in accuracy at much lower computational cost, i.e. it\\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\\nmance on our test set for measuring syntactic and semantic word similarities.\\n1 Introduction\\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\\nity between words, as these are represented as indices in a vocabulary. This choice has several good\\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\\ndata outperform complex systems trained on less data. An example is the popular N-gram model\\nused for statistical language modeling - today, it is possible to train N-grams on virtually all available\\ndata (trillions of words [3]).\\nHowever, the simple techniques are at their limits in many tasks. For example, the amount of\\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\\ndominated by the size of high quality transcribed speech data (often just millions of words). In\\nmachine translation, the existing corpora for many languages contain only a few billions of words\\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\\nany signiﬁcant progress, and we have to focus on more advanced techniques.\\nWith progress of machine learning techniques in recent years, it has become possible to train more\\ncomplex models on much larger data set, and they typically outperform the simple models. Probably\\nthe most successful concept is to use distributed representations of words [10]. For example, neural\\nnetwork based language models signiﬁcantly outperform N-gram models [1, 27, 17].\\n1.1 Goals of the Paper\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word\\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\\nfar as we know, none of the previously proposed architectures has been successfully trained on more\\n1\\narXiv:1301.3781v3  [cs.CL]  7 Sep 2013'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='than a few hundred of millions of words, with a modest dimensionality of the word vectors between\\n50 - 100.\\nWe use recently proposed techniques for measuring the quality of the resulting vector representa-\\ntions, with the expectation that not only will similar words tend to be close to each other, but that\\nwords can have multiple degrees of similarity [20]. This has been observed earlier in the context\\nof inﬂectional languages - for example, nouns can have multiple word endings, and if we search for\\nsimilar words in a subspace of the original vector space, it is possible to ﬁnd words that have similar\\nendings [13, 14].\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple\\nsyntactic regularities. Using a word offset technique where simple algebraic operations are per-\\nformed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vec-\\ntor(”Woman”) results in a vector that is closest to the vector representation of the wordQueen [20].\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model\\narchitectures that preserve the linear regularities among words. We design a new comprehensive test\\nset for measuring both syntactic and semantic regularities 1, and show that many such regularities\\ncan be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\\non the dimensionality of the word vectors and on the amount of the training data.\\n1.2 Previous Work\\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to\\nlearn jointly the word vector representation and a statistical language model. This work has been\\nfollowed by many others.\\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\\nﬁrst learned using neural network with a single hidden layer. The word vectors are then used to train\\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this\\nwork, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\\nlearned using a simple model.\\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word\\nvectors were made available for future research and comparison2. However, as far as we know, these\\narchitectures were signiﬁcantly more computationally expensive for training than the one proposed\\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\\nare used [23].\\n2 Model Architectures\\nMany different types of models were proposed for estimating continuous representations of words,\\nincluding the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\\nIn this paper, we focus on distributed representations of words learned by neural networks, as it was\\npreviously shown that they perform signiﬁcantly better than LSA for preserving linear regularities\\namong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\\nSimilar to [18], to compare different model architectures we deﬁne ﬁrst the computational complex-\\nity of a model as the number of parameters that need to be accessed to fully train the model. Next,\\nwe will try to maximize the accuracy, while minimizing the computational complexity.\\n1The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt\\n2http://ronan.collobert.com/senna/\\nhttp://metaoptimize.com/projects/wordreprs/\\nhttp://www.fit.vutbr.cz/˜imikolov/rnnlm/\\nhttp://ai.stanford.edu/˜ehhuang/\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='For all the following models, the training complexity is proportional to\\nO = E × T × Q, (1)\\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\\ndeﬁned further for each model architecture. Common choice is E = 3− 50 and T up to one billion.\\nAll models are trained using stochastic gradient descent and backpropagation [26].\\n2.1 Feedforward Neural Net Language Model (NNLM)\\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\\nusing 1-of- V coding, where V is size of the vocabulary. The input layer is then projected to a\\nprojection layer P that has dimensionality N × D, using a shared projection matrix. As only N\\ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.\\nThe NNLM architecture becomes complex for computation between the projection and the hidden\\nlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of the\\nprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000\\nunits. Moreover, the hidden layer is used to compute probability distribution over all the words in the\\nvocabulary, resulting in an output layer with dimensionalityV . Thus, the computational complexity\\nper each training example is\\nQ = N × D + N × D × H + H × V, (2)\\nwhere the dominating term is H × V . However, several practical solutions were proposed for\\navoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized\\nmodels completely by using models that are not normalized during training [4, 9]. With binary tree\\nrepresentations of the vocabulary, the number of output units that need to be evaluated can go down\\nto around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\\ntree. This follows previous observations that the frequency of words works well for obtaining classes\\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and\\nthis further reduces the number of output units that need to be evaluated: while balanced binary tree\\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\\nneural network LMs as the computational bottleneck is in theN ×D×H term, we will later propose\\narchitectures that do not have hidden layers and thus depend heavily on the efﬁciency of the softmax\\nnormalization.\\n2.2 Recurrent Neural Net Language Model (RNNLM)\\nRecurrent neural network based language model has been proposed to overcome certain limitations\\nof the feedforward NNLM, such as the need to specify the context length (the order of the modelN),\\nand because theoretically RNNs can efﬁciently represent more complex patterns than the shallow\\nneural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and\\noutput layer. What is special for this type of model is the recurrent matrix that connects hidden\\nlayer to itself, using time-delayed connections. This allows the recurrent model to form some kind\\nof short term memory, as information from the past can be represented by the hidden layer state that\\ngets updated based on the current input and the state of the hidden layer in the previous time step.\\nThe complexity per training example of the RNN model is\\nQ = H × H + H × V, (3)\\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\\nterm H × V can be efﬁciently reduced to H × log2(V ) by using hierarchical softmax. Most of the\\ncomplexity then comes from H × H.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='2.3 Parallel Training of Neural Networks\\nTo train models on huge data sets, we have implemented several models on top of a large-scale\\ndistributed framework called DistBelief [6], including the feedforward NNLM and the new models\\nproposed in this paper. The framework allows us to run multiple replicas of the same model in\\nparallel, and each replica synchronizes its gradient updates through a centralized server that keeps\\nall the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with\\nan adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use\\none hundred or more model replicas, each using many CPU cores at different machines in a data\\ncenter.\\n3 New Log-linear Models\\nIn this section, we propose two new model architectures for learning distributed representations\\nof words that try to minimize computational complexity. The main observation from the previous\\nsection was that most of the complexity is caused by the non-linear hidden layer in the model. While\\nthis is what makes neural networks so attractive, we decided to explore simpler models that might\\nnot be able to represent the data as precisely as neural networks, but can possibly be trained on much\\nmore data efﬁciently.\\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\\nfound that neural network language model can be successfully trained in two steps: ﬁrst, continuous\\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\\ndistributed representations of words. While there has been later substantial amount of work that\\nfocuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\\nNote that related models have been proposed also much earlier [26, 8].\\n3.1 Continuous Bag-of-Words Model\\nThe ﬁrst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden\\nlayer is removed and the projection layer is shared for all words (not just the projection matrix);\\nthus, all words get projected into the same position (their vectors are averaged). We call this archi-\\ntecture a bag-of-words model as the order of words in the history does not inﬂuence the projection.\\nFurthermore, we also use words from the future; we have obtained the best performance on the task\\nintroduced in the next section by building a log-linear classiﬁer with four future and four history\\nwords at the input, where the training criterion is to correctly classify the current (middle) word.\\nTraining complexity is then\\nQ = N × D + D × log2(V ). (4)\\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\\nweight matrix between the input and the projection layer is shared for all word positions in the same\\nway as in the NNLM.\\n3.2 Continuous Skip-gram Model\\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\\ncontext, it tries to maximize classiﬁcation of a word based on another word in the same sentence.\\nMore precisely, we use each current word as an input to a log-linear classiﬁer with continuous\\nprojection layer, and predict words within a certain range before and after the current word. We\\nfound that increasing the range improves quality of the resulting word vectors, but it also increases\\nthe computational complexity. Since the more distant words are usually less related to the current\\nword than those close to it, we give less weight to the distant words by sampling less from those\\nwords in our training examples.\\nThe training complexity of this architecture is proportional to\\nQ = C × (D + D × log2(V )), (5)\\nwhere C is the maximum distance of the words. Thus, if we choose C = 5, for each training word\\nwe will select randomly a number R in range < 1; C >, and then use R words from history and\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='w(t-2)\\nw(t+1)\\nw(t-1)\\nw(t+2)\\nw(t)\\nSUM\\n       INPUT         PROJECTION         OUTPUT\\nw(t)\\n          INPUT         PROJECTION      OUTPUT\\nw(t-2)\\nw(t-1)\\nw(t+1)\\nw(t+2)\\n                   CBOW                                                   Skip-gram\\nFigure 1: New model architectures. The CBOW architecture predicts the current word based on the\\ncontext, and the Skip-gram predicts surrounding words given the current word.\\nR words from the future of the current word as correct labels. This will require us to do R × 2\\nword classiﬁcations, with the current word as input, and each of the R + R words as output. In the\\nfollowing experiments, we use C = 10.\\n4 Results\\nTo compare the quality of different versions of word vectors, previous papers typically use a table\\nshowing example words and their most similar words, and understand them intuitively. Although\\nit is easy to show that word France is similar to Italy and perhaps some other countries, it is much\\nmore challenging when subjecting those vectors in a more complex similarity task, as follows. We\\nfollow previous observation that there can be many different types of similarities between words, for\\nexample, word big is similar to bigger in the same sense that small is similar to smaller. Example\\nof another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\\ndenote two pairs of words with the same relationship as a question, as we can ask: ”What is the\\nword that is similar to small in the same sense as biggest is similar to big?”\\nSomewhat surprisingly, these questions can be answered by performing simple algebraic operations\\nwith the vector representation of words. To ﬁnd a word that is similar to small in the same sense as\\nbiggest is similar to big, we can simply compute vectorX = vector(”biggest”) −vector(”big”) +\\nvector(”small”). Then, we search in the vector space for the word closest toX measured by cosine\\ndistance, and use it as the answer to the question (we discard the input question words during this\\nsearch). When the word vectors are well trained, it is possible to ﬁnd the correct answer (word\\nsmallest) using this method.\\nFinally, we found that when we train high dimensional word vectors on a large amount of data, the\\nresulting vectors can be used to answer very subtle semantic relationships between words, such as\\na city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\\nwith such semantic relationships could be used to improve many existing NLP applications, such\\nas machine translation, information retrieval and question answering systems, and may enable other\\nfuture applications yet to be invented.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='Table 1: Examples of ﬁve types of semantic and nine types of syntactic questions in the Semantic-\\nSyntactic Word Relationship test set.\\nType of relationship Word Pair 1 Word Pair 2\\nCommon capital city Athens Greece Oslo Norway\\nAll capital cities Astana Kazakhstan Harare Zimbabwe\\nCurrency Angola kwanza Iran rial\\nCity-in-state Chicago Illinois Stockton California\\nMan-Woman brother sister grandson granddaughter\\nAdjective to adverb apparent apparently rapid rapidly\\nOpposite possibly impossibly ethical unethical\\nComparative great greater tough tougher\\nSuperlative easy easiest lucky luckiest\\nPresent Participle think thinking read reading\\nNationality adjective Switzerland Swiss Cambodia Cambodian\\nPast tense walking walked swimming swam\\nPlural nouns mouse mice dollar dollars\\nPlural verbs work works speak speaks\\n4.1 Task Description\\nTo measure quality of the word vectors, we deﬁne a comprehensive test set that contains ﬁve types\\nof semantic questions, and nine types of syntactic questions. Two examples from each category are\\nshown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions\\nin each category were created in two steps: ﬁrst, a list of similar word pairs was created manually.\\nThen, a large list of questions is formed by connecting two word pairs. For example, we made a\\nlist of 68 large American cities and the states they belong to, and formed about 2.5K questions by\\npicking two word pairs at random. We have included in our test set only single token words, thus\\nmulti-word entities are not present (such as New York).\\nWe evaluate the overall accuracy for all question types, and for each question type separately (se-\\nmantic, syntactic). Question is assumed to be correctly answered only if the closest word to the\\nvector computed using the above method is exactly the same as the correct word in the question;\\nsynonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely\\nto be impossible, as the current models do not have any input information about word morphology.\\nHowever, we believe that usefulness of the word vectors for certain applications should be positively\\ncorrelated with this accuracy metric. Further progress can be achieved by incorporating information\\nabout structure of words, especially for the syntactic questions.\\n4.2 Maximization of Accuracy\\nWe have used a Google News corpus for training the word vectors. This corpus contains about\\n6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we\\nare facing time constrained optimization problem, as it can be expected that both using more data\\nand higher dimensional word vectors will improve the accuracy. To estimate the best choice of\\nmodel architecture for obtaining as good as possible results quickly, we have ﬁrst evaluated models\\ntrained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\\nThe results using the CBOW architecture with different choice of word vector dimensionality and\\nincreasing amount of the training data are shown in Table 2.\\nIt can be seen that after some point, adding more dimensions or adding more training data provides\\ndiminishing improvements. So, we have to increase both vector dimensionality and the amount\\nof the training data together. While this observation might seem trivial, it must be noted that it is\\ncurrently popular to train word vectors on relatively large amounts of data, but with insufﬁcient size\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word\\nvectors from the CBOW architecture with limited vocabulary. Only questions containing words from\\nthe most frequent 30k words are used.\\nDimensionality / Training words 24M 49M 98M 196M 391M 783M\\n50 13.4 15.7 18.6 19.1 22.5 23.2\\n100 19.4 23.1 27.8 28.7 33.4 32.2\\n300 23.2 29.2 35.3 38.6 43.7 45.9\\n600 24.0 30.1 36.5 40.8 46.6 50.4\\nTable 3: Comparison of architectures using models trained on the same data, with 640-dimensional\\nword vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,\\nand on the syntactic relationship test set of [20]\\nModel Semantic-Syntactic Word Relationship test set MSR Word Relatedness\\nArchitecture Semantic Accuracy [%] Syntactic Accuracy [%] Test Set [20]\\nRNNLM 9 36 35\\nNNLM 23 53 47\\nCBOW 24 64 61\\nSkip-gram 55 59 56\\n(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the\\nsame increase of computational complexity as increasing vector size twice.\\nFor the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-\\nent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so\\nthat it approaches zero at the end of the last training epoch.\\n4.3 Comparison of Model Architectures\\nFirst we compare different model architectures for deriving the word vectors using the same training\\ndata and using the same dimensionality of 640 of the word vectors. In the further experiments, we\\nuse full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to\\nthe 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic\\nsimilarity between words3.\\nThe training data consists of several LDC corpora and is described in detail in [18] (320M words,\\n82K vocabulary). We used these data to provide a comparison to a previously trained recurrent\\nneural network language model that took about 8 weeks to train on a single CPU. We trained a feed-\\nforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],\\nusing a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the\\nprojection layer has size 640 × 8).\\nIn Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly\\non the syntactic questions. The NNLM vectors perform signiﬁcantly better than the RNN - this is\\nnot surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden\\nlayer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the\\nsame on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic\\ntask than the CBOW model (but still better than the NNLM), and much better on the semantic part\\nof the test than all the other models.\\nNext, we evaluated our models trained using one CPU only and compared the results against publicly\\navailable word vectors. The comparison is given in Table 4. The CBOW model was trained on subset\\n3We thank Geoff Zweig for providing us the test set.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-\\nship test set, and word vectors from our models. Full vocabularies are used.\\nModel Vector Training Accuracy [%]\\nDimensionality words\\nSemantic Syntactic Total\\nCollobert-Weston NNLM 50 660M 9.3 12.3 11.0\\nTurian NNLM 50 37M 1.4 2.6 2.1\\nTurian NNLM 200 37M 1.4 2.2 1.8\\nMnih NNLM 50 37M 1.8 9.1 5.8\\nMnih NNLM 100 37M 3.3 13.2 8.8\\nMikolov RNNLM 80 320M 4.9 18.4 12.7\\nMikolov RNNLM 640 320M 8.6 36.5 24.6\\nHuang NNLM 50 990M 13.3 11.6 12.3\\nOur NNLM 20 6B 12.9 26.4 20.3\\nOur NNLM 50 6B 27.9 55.8 43.2\\nOur NNLM 100 6B 34.2 64.5 50.8\\nCBOW 300 783M 15.5 53.1 36.1\\nSkip-gram 300 783M 50.0 55.9 53.3\\nTable 5: Comparison of models trained for three epochs on the same data and models trained for\\none epoch. Accuracy is reported on the full Semantic-Syntactic data set.\\nModel Vector Training Accuracy [%] Training time\\nDimensionality words [days]\\nSemantic Syntactic Total\\n3 epoch CBOW 300 783M 15.5 53.1 36.1 1\\n3 epoch Skip-gram 300 783M 50.0 55.9 53.3 3\\n1 epoch CBOW 300 783M 13.8 49.9 33.6 0.3\\n1 epoch CBOW 300 1.6B 16.1 52.6 36.1 0.6\\n1 epoch CBOW 600 783M 15.4 53.3 36.2 0.7\\n1 epoch Skip-gram 300 783M 45.6 52.2 49.2 1\\n1 epoch Skip-gram 300 1.6B 52.2 55.1 53.8 2\\n1 epoch Skip-gram 600 783M 56.7 54.5 55.5 2.5\\nof the Google News data in about a day, while training time for the Skip-gram model was about three\\ndays.\\nFor experiments reported further, we used just one training epoch (again, we decrease the learning\\nrate linearly so that it approaches zero at the end of training). Training a model on twice as much\\ndata using one epoch gives comparable or better results than iterating over the same data for three\\nepochs, as is shown in Table 5, and provides additional small speedup.\\n4.4 Large Scale Parallel Training of Models\\nAs mentioned earlier, we have implemented various models in a distributed framework called Dis-\\ntBelief. Below we report the results of several models trained on the Google News 6B data set,\\nwith mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-\\ngrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Table 6: Comparison of models trained using the DistBelief distributed framework. Note that\\ntraining of NNLM with 1000-dimensional vectors would take too long to complete.\\nModel Vector Training Accuracy [%] Training time\\nDimensionality words [days x CPU cores]\\nSemantic Syntactic Total\\nNNLM 100 6B 34.2 64.5 50.8 14 x 180\\nCBOW 1000 6B 57.3 68.9 63.7 2 x 140\\nSkip-gram 1000 6B 66.1 65.1 65.6 2.5 x 125\\nTable 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\\nArchitecture Accuracy [%]\\n4-gram [32] 39\\nAverage LSA similarity [32] 49\\nLog-bilinear model [24] 54.8\\nRNNLMs [19] 55.4\\nSkip-gram 48.0\\nSkip-gram + RNNLMs 58.9\\nestimate since the data center machines are shared with other production tasks, and the usage can\\nﬂuctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of\\nthe CBOW model and the Skip-gram model are much closer to each other than their single-machine\\nimplementations. The result are reported in Table 6.\\n4.5 Microsoft Research Sentence Completion Challenge\\nThe Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing\\nlanguage modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one\\nword is missing in each sentence and the goal is to select word that is the most coherent with the\\nrest of the sentence, given a list of ﬁve reasonable choices. Performance of several techniques has\\nbeen already reported on this set, including N-gram models, LSA-based model [32], log-bilinear\\nmodel [24] and a combination of recurrent neural networks that currently holds the state of the art\\nperformance of 55.4% accuracy on this benchmark [19].\\nWe have explored the performance of Skip-gram architecture on this task. First, we train the 640-\\ndimensional model on 50M words provided in [32]. Then, we compute score of each sentence in\\nthe test set by using the unknown word at the input, and predict all surrounding words in a sentence.\\nThe ﬁnal sentence score is then the sum of these individual predictions. Using the sentence scores,\\nwe choose the most likely sentence.\\nA short summary of some previous results together with the new results is presented in Table 7.\\nWhile the Skip-gram model itself does not perform on this task better than LSA similarity, the scores\\nfrom this model are complementary to scores obtained with RNNLMs, and a weighted combination\\nleads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and\\n58.7% on the test part of the set).\\n5 Examples of the Learned Relationships\\nTable 8 shows words that follow various relationships. We follow the approach described above: the\\nrelationship is deﬁned by subtracting two word vectors, and the result is added to another word. Thus\\nfor example, Paris - France + Italy = Rome . As it can be seen, accuracy is quite good, although\\nthere is clearly a lot of room for further improvements (note that using our accuracy metric that\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\\ngram model trained on 783M words with 300 dimensionality).\\nRelationship Example 1 Example 2 Example 3\\nFrance - Paris Italy: Rome Japan: Tokyo Florida: Tallahassee\\nbig - bigger small: larger cold: colder quick: quicker\\nMiami - Florida Baltimore: Maryland Dallas: Texas Kona: Hawaii\\nEinstein - scientist Messi: midﬁelder Mozart: violinist Picasso: painter\\nSarkozy - France Berlusconi: Italy Merkel: Germany Koizumi: Japan\\ncopper - Cu zinc: Zn gold: Au uranium: plutonium\\nBerlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: Barack\\nMicrosoft - Windows Google: Android IBM: Linux Apple: iPhone\\nMicrosoft - Ballmer Google: Yahoo IBM: McNealy Apple: Jobs\\nJapan - sushi Germany: bratwurst France: tapas USA: pizza\\nassumes exact match, the results in Table 8 would score only about 60%). We believe that word\\nvectors trained on even larger data sets with larger dimensionality will perform signiﬁcantly better,\\nand will enable the development of new innovative applications. Another way to improve accuracy is\\nto provide more than one example of the relationship. By using ten examples instead of one to form\\nthe relationship vector (we average the individual vectors together), we have observed improvement\\nof accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\\nIt is also possible to apply the vector operations to solve different tasks. For example, we have\\nobserved good accuracy for selecting out-of-the-list words, by computing average vector for a list of\\nwords, and ﬁnding the most distant word vector. This is a popular type of problems in certain human\\nintelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.\\n6 Conclusion\\nIn this paper we studied the quality of vector representations of words derived by various models on\\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\\nquality word vectors using very simple model architectures, compared to the popular neural network\\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it\\nis possible to compute very accurate high dimensional word vectors from a much larger data set.\\nUsing the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram\\nmodels even on corpora with one trillion words, for basically unlimited size of the vocabulary. That\\nis several orders of magnitude larger than the best previously published results for similar models.\\nAn interesting task where the word vectors have recently been shown to signiﬁcantly outperform the\\nprevious state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were\\nused together with other techniques to achieve over 50% increase in Spearman’s rank correlation\\nover the previous best result [31]. The neural network based word vectors were previously applied\\nto many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can\\nbe expected that these applications can beneﬁt from the model architectures described in this paper.\\nOur ongoing work shows that the word vectors can be successfully applied to automatic extension\\nof facts in Knowledge Bases, and also for veriﬁcation of correctness of existing facts. Results\\nfrom machine translation experiments also look very promising. In the future, it would be also\\ninteresting to compare our techniques to Latent Relational Analysis [30] and others. We believe that\\nour comprehensive test set will help the research community to improve the existing techniques for\\nestimating the word vectors. We also expect that high quality word vectors will become an important\\nbuilding block for future NLP applications.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='7 Follow-Up Work\\nAfter the initial version of this paper was written, we published single-machine multi-threaded C++\\ncode for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-\\ntectures4. The training speed is signiﬁcantly higher than reported earlier in this paper, i.e. it is in the\\norder of billions of words per hour for typical hyperparameter choices. We also published more than\\n1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of\\nour follow-up work will be published in an upcoming NIPS 2013 paper [21].\\nReferences\\n[1] Y . Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-\\nchine Learning Research, 3:1137-1155, 2003.\\n[2] Y . Bengio, Y . LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-\\nchines, MIT Press, 2007.\\n[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine\\ntranslation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language\\nProcessing and Computational Language Learning, 2007.\\n[4] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep\\nNeural Networks with Multitask Learning. In International Conference on Machine Learning,\\nICML, 2008.\\n[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-\\nguage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-\\n2537, 2011.\\n[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V . Le, M.Z. Mao, M.A. Ranzato, A.\\nSenior, P. Tucker, K. Yang, A. Y . Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\\n[7] J.C. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and\\nstochastic optimization. Journal of Machine Learning Research, 2011.\\n[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.\\n[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y . Ng. Improving Word Representations\\nvia Global Context and Multiple Word Prototypes. In: Proc. Association for Computational\\nLinguistics, 2012.\\n[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-\\ntributed processing: Explorations in the microstructure of cognition. V olume 1: Foundations,\\nMIT Press, 1986.\\n[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring\\ndegrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic\\nEvaluation (SemEval 2012), 2012.\\n[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y . Ng, and C. Potts. Learning word vectors for\\nsentiment analysis. In Proceedings of ACL, 2011.\\n[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-\\nversity of Technology, 2007.\\n[14] T. Mikolov, J. Kopeck ´y, L. Burget, O. Glembek and J. ˇCernock´y. Neural network based lan-\\nguage models for higly inﬂective languages, In: Proc. ICASSP 2009.\\n[15] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock´y, S. Khudanpur. Recurrent neural network\\nbased language model, In: Proceedings of Interspeech, 2010.\\n[16] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, S. Khudanpur. Extensions of recurrent neural\\nnetwork language model, In: Proceedings of ICASSP 2011.\\n[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock´y. Empirical Evaluation and Com-\\nbination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\\n4The code is available at https://code.google.com/p/word2vec/\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇCernock´y. Strategies for Training Large Scale\\nNeural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-\\ning, 2011.\\n[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-\\nsity of Technology, 2012.\\n[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-\\ntations. NAACL HLT 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\\nWords and Phrases and their Compositionality. Accepted to NIPS 2013.\\n[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,\\n2007.\\n[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural\\nInformation Processing Systems 21, MIT Press, 2009.\\n[24] A. Mnih, Y .W. Teh. A fast and simple algorithm for training neural probabilistic language\\nmodels. ICML, 2012.\\n[25] F. Morin, Y . Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,\\n2005.\\n[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-\\npropagating errors. Nature, 323:533.536, 1986.\\n[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,\\n2007.\\n[28] R. Socher, E.H. Huang, J. Pennington, A.Y . Ng, and C.D. Manning. Dynamic Pooling and\\nUnfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.\\n[29] J. Turian, L. Ratinov, Y . Bengio. Word Representations: A Simple and General Method for\\nSemi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.\\n[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-\\ntional Joint Conference on Artiﬁcial Intelligence, 2005.\\n[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for\\nMeasuring Relational Similarity. NAACL HLT 2013.\\n[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft\\nResearch Technical Report MSR-TR-2011-129, 2011.\\n12')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pdf loader\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader #or PyPDFLoader PyMuPDFLoader we can use any of them\n",
    "\n",
    "document = PyPDFLoader(\n",
    "\n",
    "    file_path='../data/pdf_files/1301.3781v3.pdf'\n",
    "\n",
    ")\n",
    "\n",
    "pdf_doc = document.load()\n",
    "\n",
    "pdf_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4ae0f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efﬁcient Estimation of Word Representations in\n",
      "Vector Space\n",
      "Tomas Mikolov\n",
      "Google Inc., Mountain View, CA\n",
      "tmikolov@google.com\n",
      "Kai Chen\n",
      "Google Inc., Mountain View, CA\n",
      "kaichen@google.com\n",
      "Greg Corrado\n",
      "Google Inc., Mountain View, CA\n",
      "gcorrado@google.com\n",
      "Jeffrey Dean\n",
      "Google Inc., Mountain View, CA\n",
      "jeff@google.com\n",
      "Abstract\n",
      "We propose two novel model architectures for computing continuous vector repre-\n",
      "sentations of words from very large data sets. The quality of these representations\n",
      "is measured in a word similarity task, and the results are compared to the previ-\n",
      "ously best performing techniques based on different types of neural networks. We\n",
      "observe large improvements in accuracy at much lower computational cost, i.e. it\n",
      "takes less than a day to learn high quality word vectors from a 1.6 billion words\n",
      "data set. Furthermore, we show that these vectors provide state-of-the-art perfor-\n",
      "mance on our test set for measuring syntactic and semantic word similarities.\n",
      "1 Introduction\n",
      "Many current NLP systems and techniques treat words as atomic units - there is no notion of similar-\n",
      "ity between words, as these are represented as indices in a vocabulary. This choice has several good\n",
      "reasons - simplicity, robustness and the observation that simple models trained on huge amounts of\n",
      "data outperform complex systems trained on less data. An example is the popular N-gram model\n",
      "used for statistical language modeling - today, it is possible to train N-grams on virtually all available\n",
      "data (trillions of words [3]).\n",
      "However, the simple techniques are at their limits in many tasks. For example, the amount of\n",
      "relevant in-domain data for automatic speech recognition is limited - the performance is usually\n",
      "dominated by the size of high quality transcribed speech data (often just millions of words). In\n",
      "machine translation, the existing corpora for many languages contain only a few billions of words\n",
      "or less. Thus, there are situations where simple scaling up of the basic techniques will not result in\n",
      "any signiﬁcant progress, and we have to focus on more advanced techniques.\n",
      "With progress of machine learning techniques in recent years, it has become possible to train more\n",
      "complex models on much larger data set, and they typically outperform the simple models. Probably\n",
      "the most successful concept is to use distributed representations of words [10]. For example, neural\n",
      "network based language models signiﬁcantly outperform N-gram models [1, 27, 17].\n",
      "1.1 Goals of the Paper\n",
      "The main goal of this paper is to introduce techniques that can be used for learning high-quality word\n",
      "vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\n",
      "far as we know, none of the previously proposed architectures has been successfully trained on more\n",
      "1\n",
      "arXiv:1301.3781v3  [cs.CL]  7 Sep 2013\n",
      "{'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'author': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/1301.3781v3.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Load documents asynchronously:\n",
    "\n",
    "docs = await document.aload()  \n",
    "print(docs[0].page_content)  \n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fc6beca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='Efﬁcient Estimation of Word Representations in\\nVector Space\\nTomas Mikolov\\nGoogle Inc., Mountain View, CA\\ntmikolov@google.com\\nKai Chen\\nGoogle Inc., Mountain View, CA\\nkaichen@google.com\\nGreg Corrado\\nGoogle Inc., Mountain View, CA\\ngcorrado@google.com\\nJeffrey Dean\\nGoogle Inc., Mountain View, CA\\njeff@google.com\\nAbstract\\nWe propose two novel model architectures for computing continuous vector repre-\\nsentations of words from very large data sets. The quality of these representations\\nis measured in a word similarity task, and the results are compared to the previ-\\nously best performing techniques based on different types of neural networks. We\\nobserve large improvements in accuracy at much lower computational cost, i.e. it\\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\\nmance on our test set for measuring syntactic and semantic word similarities.\\n1\\nIntroduction\\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\\nity between words, as these are represented as indices in a vocabulary. This choice has several good\\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\\ndata outperform complex systems trained on less data. An example is the popular N-gram model\\nused for statistical language modeling - today, it is possible to train N-grams on virtually all available\\ndata (trillions of words [3]).\\nHowever, the simple techniques are at their limits in many tasks. For example, the amount of\\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\\ndominated by the size of high quality transcribed speech data (often just millions of words). In\\nmachine translation, the existing corpora for many languages contain only a few billions of words\\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\\nany signiﬁcant progress, and we have to focus on more advanced techniques.\\nWith progress of machine learning techniques in recent years, it has become possible to train more\\ncomplex models on much larger data set, and they typically outperform the simple models. Probably\\nthe most successful concept is to use distributed representations of words [10]. For example, neural\\nnetwork based language models signiﬁcantly outperform N-gram models [1, 27, 17].\\n1.1\\nGoals of the Paper\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word\\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\\nfar as we know, none of the previously proposed architectures has been successfully trained on more\\n1\\narXiv:1301.3781v3  [cs.CL]  7 Sep 2013'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='than a few hundred of millions of words, with a modest dimensionality of the word vectors between\\n50 - 100.\\nWe use recently proposed techniques for measuring the quality of the resulting vector representa-\\ntions, with the expectation that not only will similar words tend to be close to each other, but that\\nwords can have multiple degrees of similarity [20]. This has been observed earlier in the context\\nof inﬂectional languages - for example, nouns can have multiple word endings, and if we search for\\nsimilar words in a subspace of the original vector space, it is possible to ﬁnd words that have similar\\nendings [13, 14].\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple\\nsyntactic regularities. Using a word offset technique where simple algebraic operations are per-\\nformed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vec-\\ntor(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model\\narchitectures that preserve the linear regularities among words. We design a new comprehensive test\\nset for measuring both syntactic and semantic regularities1, and show that many such regularities\\ncan be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\\non the dimensionality of the word vectors and on the amount of the training data.\\n1.2\\nPrevious Work\\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to\\nlearn jointly the word vector representation and a statistical language model. This work has been\\nfollowed by many others.\\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\\nﬁrst learned using neural network with a single hidden layer. The word vectors are then used to train\\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this\\nwork, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\\nlearned using a simple model.\\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word\\nvectors were made available for future research and comparison2. However, as far as we know, these\\narchitectures were signiﬁcantly more computationally expensive for training than the one proposed\\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\\nare used [23].\\n2\\nModel Architectures\\nMany different types of models were proposed for estimating continuous representations of words,\\nincluding the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\\nIn this paper, we focus on distributed representations of words learned by neural networks, as it was\\npreviously shown that they perform signiﬁcantly better than LSA for preserving linear regularities\\namong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\\nSimilar to [18], to compare different model architectures we deﬁne ﬁrst the computational complex-\\nity of a model as the number of parameters that need to be accessed to fully train the model. Next,\\nwe will try to maximize the accuracy, while minimizing the computational complexity.\\n1The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt\\n2http://ronan.collobert.com/senna/\\nhttp://metaoptimize.com/projects/wordreprs/\\nhttp://www.fit.vutbr.cz/˜imikolov/rnnlm/\\nhttp://ai.stanford.edu/˜ehhuang/\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='For all the following models, the training complexity is proportional to\\nO = E × T × Q,\\n(1)\\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\\ndeﬁned further for each model architecture. Common choice is E = 3 −50 and T up to one billion.\\nAll models are trained using stochastic gradient descent and backpropagation [26].\\n2.1\\nFeedforward Neural Net Language Model (NNLM)\\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\\nusing 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a\\nprojection layer P that has dimensionality N × D, using a shared projection matrix. As only N\\ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.\\nThe NNLM architecture becomes complex for computation between the projection and the hidden\\nlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of the\\nprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000\\nunits. Moreover, the hidden layer is used to compute probability distribution over all the words in the\\nvocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity\\nper each training example is\\nQ = N × D + N × D × H + H × V,\\n(2)\\nwhere the dominating term is H × V . However, several practical solutions were proposed for\\navoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized\\nmodels completely by using models that are not normalized during training [4, 9]. With binary tree\\nrepresentations of the vocabulary, the number of output units that need to be evaluated can go down\\nto around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\\ntree. This follows previous observations that the frequency of words works well for obtaining classes\\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and\\nthis further reduces the number of output units that need to be evaluated: while balanced binary tree\\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\\nneural network LMs as the computational bottleneck is in the N ×D×H term, we will later propose\\narchitectures that do not have hidden layers and thus depend heavily on the efﬁciency of the softmax\\nnormalization.\\n2.2\\nRecurrent Neural Net Language Model (RNNLM)\\nRecurrent neural network based language model has been proposed to overcome certain limitations\\nof the feedforward NNLM, such as the need to specify the context length (the order of the model N),\\nand because theoretically RNNs can efﬁciently represent more complex patterns than the shallow\\nneural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and\\noutput layer. What is special for this type of model is the recurrent matrix that connects hidden\\nlayer to itself, using time-delayed connections. This allows the recurrent model to form some kind\\nof short term memory, as information from the past can be represented by the hidden layer state that\\ngets updated based on the current input and the state of the hidden layer in the previous time step.\\nThe complexity per training example of the RNN model is\\nQ = H × H + H × V,\\n(3)\\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\\nterm H × V can be efﬁciently reduced to H × log2(V ) by using hierarchical softmax. Most of the\\ncomplexity then comes from H × H.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='2.3\\nParallel Training of Neural Networks\\nTo train models on huge data sets, we have implemented several models on top of a large-scale\\ndistributed framework called DistBelief [6], including the feedforward NNLM and the new models\\nproposed in this paper. The framework allows us to run multiple replicas of the same model in\\nparallel, and each replica synchronizes its gradient updates through a centralized server that keeps\\nall the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with\\nan adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use\\none hundred or more model replicas, each using many CPU cores at different machines in a data\\ncenter.\\n3\\nNew Log-linear Models\\nIn this section, we propose two new model architectures for learning distributed representations\\nof words that try to minimize computational complexity. The main observation from the previous\\nsection was that most of the complexity is caused by the non-linear hidden layer in the model. While\\nthis is what makes neural networks so attractive, we decided to explore simpler models that might\\nnot be able to represent the data as precisely as neural networks, but can possibly be trained on much\\nmore data efﬁciently.\\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\\nfound that neural network language model can be successfully trained in two steps: ﬁrst, continuous\\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\\ndistributed representations of words. While there has been later substantial amount of work that\\nfocuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\\nNote that related models have been proposed also much earlier [26, 8].\\n3.1\\nContinuous Bag-of-Words Model\\nThe ﬁrst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden\\nlayer is removed and the projection layer is shared for all words (not just the projection matrix);\\nthus, all words get projected into the same position (their vectors are averaged). We call this archi-\\ntecture a bag-of-words model as the order of words in the history does not inﬂuence the projection.\\nFurthermore, we also use words from the future; we have obtained the best performance on the task\\nintroduced in the next section by building a log-linear classiﬁer with four future and four history\\nwords at the input, where the training criterion is to correctly classify the current (middle) word.\\nTraining complexity is then\\nQ = N × D + D × log2(V ).\\n(4)\\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\\nweight matrix between the input and the projection layer is shared for all word positions in the same\\nway as in the NNLM.\\n3.2\\nContinuous Skip-gram Model\\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\\ncontext, it tries to maximize classiﬁcation of a word based on another word in the same sentence.\\nMore precisely, we use each current word as an input to a log-linear classiﬁer with continuous\\nprojection layer, and predict words within a certain range before and after the current word. We\\nfound that increasing the range improves quality of the resulting word vectors, but it also increases\\nthe computational complexity. Since the more distant words are usually less related to the current\\nword than those close to it, we give less weight to the distant words by sampling less from those\\nwords in our training examples.\\nThe training complexity of this architecture is proportional to\\nQ = C × (D + D × log2(V )),\\n(5)\\nwhere C is the maximum distance of the words. Thus, if we choose C = 5, for each training word\\nwe will select randomly a number R in range < 1; C >, and then use R words from history and\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='w(t-2)\\nw(t+1)\\nw(t-1)\\nw(t+2)\\nw(t)\\nSUM\\n       INPUT         PROJECTION         OUTPUT\\nw(t)\\n          INPUT         PROJECTION      OUTPUT\\nw(t-2)\\nw(t-1)\\nw(t+1)\\nw(t+2)\\n                   CBOW                                                   Skip-gram\\nFigure 1: New model architectures. The CBOW architecture predicts the current word based on the\\ncontext, and the Skip-gram predicts surrounding words given the current word.\\nR words from the future of the current word as correct labels. This will require us to do R × 2\\nword classiﬁcations, with the current word as input, and each of the R + R words as output. In the\\nfollowing experiments, we use C = 10.\\n4\\nResults\\nTo compare the quality of different versions of word vectors, previous papers typically use a table\\nshowing example words and their most similar words, and understand them intuitively. Although\\nit is easy to show that word France is similar to Italy and perhaps some other countries, it is much\\nmore challenging when subjecting those vectors in a more complex similarity task, as follows. We\\nfollow previous observation that there can be many different types of similarities between words, for\\nexample, word big is similar to bigger in the same sense that small is similar to smaller. Example\\nof another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\\ndenote two pairs of words with the same relationship as a question, as we can ask: ”What is the\\nword that is similar to small in the same sense as biggest is similar to big?”\\nSomewhat surprisingly, these questions can be answered by performing simple algebraic operations\\nwith the vector representation of words. To ﬁnd a word that is similar to small in the same sense as\\nbiggest is similar to big, we can simply compute vector X = vector(”biggest”)−vector(”big”)+\\nvector(”small”). Then, we search in the vector space for the word closest to X measured by cosine\\ndistance, and use it as the answer to the question (we discard the input question words during this\\nsearch). When the word vectors are well trained, it is possible to ﬁnd the correct answer (word\\nsmallest) using this method.\\nFinally, we found that when we train high dimensional word vectors on a large amount of data, the\\nresulting vectors can be used to answer very subtle semantic relationships between words, such as\\na city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\\nwith such semantic relationships could be used to improve many existing NLP applications, such\\nas machine translation, information retrieval and question answering systems, and may enable other\\nfuture applications yet to be invented.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='Table 1: Examples of ﬁve types of semantic and nine types of syntactic questions in the Semantic-\\nSyntactic Word Relationship test set.\\nType of relationship\\nWord Pair 1\\nWord Pair 2\\nCommon capital city\\nAthens\\nGreece\\nOslo\\nNorway\\nAll capital cities\\nAstana\\nKazakhstan\\nHarare\\nZimbabwe\\nCurrency\\nAngola\\nkwanza\\nIran\\nrial\\nCity-in-state\\nChicago\\nIllinois\\nStockton\\nCalifornia\\nMan-Woman\\nbrother\\nsister\\ngrandson\\ngranddaughter\\nAdjective to adverb\\napparent\\napparently\\nrapid\\nrapidly\\nOpposite\\npossibly\\nimpossibly\\nethical\\nunethical\\nComparative\\ngreat\\ngreater\\ntough\\ntougher\\nSuperlative\\neasy\\neasiest\\nlucky\\nluckiest\\nPresent Participle\\nthink\\nthinking\\nread\\nreading\\nNationality adjective\\nSwitzerland\\nSwiss\\nCambodia\\nCambodian\\nPast tense\\nwalking\\nwalked\\nswimming\\nswam\\nPlural nouns\\nmouse\\nmice\\ndollar\\ndollars\\nPlural verbs\\nwork\\nworks\\nspeak\\nspeaks\\n4.1\\nTask Description\\nTo measure quality of the word vectors, we deﬁne a comprehensive test set that contains ﬁve types\\nof semantic questions, and nine types of syntactic questions. Two examples from each category are\\nshown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions\\nin each category were created in two steps: ﬁrst, a list of similar word pairs was created manually.\\nThen, a large list of questions is formed by connecting two word pairs. For example, we made a\\nlist of 68 large American cities and the states they belong to, and formed about 2.5K questions by\\npicking two word pairs at random. We have included in our test set only single token words, thus\\nmulti-word entities are not present (such as New York).\\nWe evaluate the overall accuracy for all question types, and for each question type separately (se-\\nmantic, syntactic). Question is assumed to be correctly answered only if the closest word to the\\nvector computed using the above method is exactly the same as the correct word in the question;\\nsynonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely\\nto be impossible, as the current models do not have any input information about word morphology.\\nHowever, we believe that usefulness of the word vectors for certain applications should be positively\\ncorrelated with this accuracy metric. Further progress can be achieved by incorporating information\\nabout structure of words, especially for the syntactic questions.\\n4.2\\nMaximization of Accuracy\\nWe have used a Google News corpus for training the word vectors. This corpus contains about\\n6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we\\nare facing time constrained optimization problem, as it can be expected that both using more data\\nand higher dimensional word vectors will improve the accuracy. To estimate the best choice of\\nmodel architecture for obtaining as good as possible results quickly, we have ﬁrst evaluated models\\ntrained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\\nThe results using the CBOW architecture with different choice of word vector dimensionality and\\nincreasing amount of the training data are shown in Table 2.\\nIt can be seen that after some point, adding more dimensions or adding more training data provides\\ndiminishing improvements. So, we have to increase both vector dimensionality and the amount\\nof the training data together. While this observation might seem trivial, it must be noted that it is\\ncurrently popular to train word vectors on relatively large amounts of data, but with insufﬁcient size\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='Table 2:\\nAccuracy on subset of the Semantic-Syntactic Word Relationship test set, using word\\nvectors from the CBOW architecture with limited vocabulary. Only questions containing words from\\nthe most frequent 30k words are used.\\nDimensionality / Training words\\n24M\\n49M\\n98M\\n196M\\n391M\\n783M\\n50\\n13.4\\n15.7\\n18.6\\n19.1\\n22.5\\n23.2\\n100\\n19.4\\n23.1\\n27.8\\n28.7\\n33.4\\n32.2\\n300\\n23.2\\n29.2\\n35.3\\n38.6\\n43.7\\n45.9\\n600\\n24.0\\n30.1\\n36.5\\n40.8\\n46.6\\n50.4\\nTable 3: Comparison of architectures using models trained on the same data, with 640-dimensional\\nword vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,\\nand on the syntactic relationship test set of [20]\\nModel\\nSemantic-Syntactic Word Relationship test set\\nMSR Word Relatedness\\nArchitecture\\nSemantic Accuracy [%]\\nSyntactic Accuracy [%]\\nTest Set [20]\\nRNNLM\\n9\\n36\\n35\\nNNLM\\n23\\n53\\n47\\nCBOW\\n24\\n64\\n61\\nSkip-gram\\n55\\n59\\n56\\n(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the\\nsame increase of computational complexity as increasing vector size twice.\\nFor the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-\\nent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so\\nthat it approaches zero at the end of the last training epoch.\\n4.3\\nComparison of Model Architectures\\nFirst we compare different model architectures for deriving the word vectors using the same training\\ndata and using the same dimensionality of 640 of the word vectors. In the further experiments, we\\nuse full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to\\nthe 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic\\nsimilarity between words3.\\nThe training data consists of several LDC corpora and is described in detail in [18] (320M words,\\n82K vocabulary). We used these data to provide a comparison to a previously trained recurrent\\nneural network language model that took about 8 weeks to train on a single CPU. We trained a feed-\\nforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],\\nusing a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the\\nprojection layer has size 640 × 8).\\nIn Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly\\non the syntactic questions. The NNLM vectors perform signiﬁcantly better than the RNN - this is\\nnot surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden\\nlayer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the\\nsame on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic\\ntask than the CBOW model (but still better than the NNLM), and much better on the semantic part\\nof the test than all the other models.\\nNext, we evaluated our models trained using one CPU only and compared the results against publicly\\navailable word vectors. The comparison is given in Table 4. The CBOW model was trained on subset\\n3We thank Geoff Zweig for providing us the test set.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 7}, page_content='Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-\\nship test set, and word vectors from our models. Full vocabularies are used.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nDimensionality\\nwords\\nSemantic\\nSyntactic\\nTotal\\nCollobert-Weston NNLM\\n50\\n660M\\n9.3\\n12.3\\n11.0\\nTurian NNLM\\n50\\n37M\\n1.4\\n2.6\\n2.1\\nTurian NNLM\\n200\\n37M\\n1.4\\n2.2\\n1.8\\nMnih NNLM\\n50\\n37M\\n1.8\\n9.1\\n5.8\\nMnih NNLM\\n100\\n37M\\n3.3\\n13.2\\n8.8\\nMikolov RNNLM\\n80\\n320M\\n4.9\\n18.4\\n12.7\\nMikolov RNNLM\\n640\\n320M\\n8.6\\n36.5\\n24.6\\nHuang NNLM\\n50\\n990M\\n13.3\\n11.6\\n12.3\\nOur NNLM\\n20\\n6B\\n12.9\\n26.4\\n20.3\\nOur NNLM\\n50\\n6B\\n27.9\\n55.8\\n43.2\\nOur NNLM\\n100\\n6B\\n34.2\\n64.5\\n50.8\\nCBOW\\n300\\n783M\\n15.5\\n53.1\\n36.1\\nSkip-gram\\n300\\n783M\\n50.0\\n55.9\\n53.3\\nTable 5: Comparison of models trained for three epochs on the same data and models trained for\\none epoch. Accuracy is reported on the full Semantic-Syntactic data set.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nTraining time\\nDimensionality\\nwords\\n[days]\\nSemantic\\nSyntactic\\nTotal\\n3 epoch CBOW\\n300\\n783M\\n15.5\\n53.1\\n36.1\\n1\\n3 epoch Skip-gram\\n300\\n783M\\n50.0\\n55.9\\n53.3\\n3\\n1 epoch CBOW\\n300\\n783M\\n13.8\\n49.9\\n33.6\\n0.3\\n1 epoch CBOW\\n300\\n1.6B\\n16.1\\n52.6\\n36.1\\n0.6\\n1 epoch CBOW\\n600\\n783M\\n15.4\\n53.3\\n36.2\\n0.7\\n1 epoch Skip-gram\\n300\\n783M\\n45.6\\n52.2\\n49.2\\n1\\n1 epoch Skip-gram\\n300\\n1.6B\\n52.2\\n55.1\\n53.8\\n2\\n1 epoch Skip-gram\\n600\\n783M\\n56.7\\n54.5\\n55.5\\n2.5\\nof the Google News data in about a day, while training time for the Skip-gram model was about three\\ndays.\\nFor experiments reported further, we used just one training epoch (again, we decrease the learning\\nrate linearly so that it approaches zero at the end of training). Training a model on twice as much\\ndata using one epoch gives comparable or better results than iterating over the same data for three\\nepochs, as is shown in Table 5, and provides additional small speedup.\\n4.4\\nLarge Scale Parallel Training of Models\\nAs mentioned earlier, we have implemented various models in a distributed framework called Dis-\\ntBelief. Below we report the results of several models trained on the Google News 6B data set,\\nwith mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-\\ngrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='Table 6:\\nComparison of models trained using the DistBelief distributed framework. Note that\\ntraining of NNLM with 1000-dimensional vectors would take too long to complete.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nTraining time\\nDimensionality\\nwords\\n[days x CPU cores]\\nSemantic\\nSyntactic\\nTotal\\nNNLM\\n100\\n6B\\n34.2\\n64.5\\n50.8\\n14 x 180\\nCBOW\\n1000\\n6B\\n57.3\\n68.9\\n63.7\\n2 x 140\\nSkip-gram\\n1000\\n6B\\n66.1\\n65.1\\n65.6\\n2.5 x 125\\nTable 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\\nArchitecture\\nAccuracy [%]\\n4-gram [32]\\n39\\nAverage LSA similarity [32]\\n49\\nLog-bilinear model [24]\\n54.8\\nRNNLMs [19]\\n55.4\\nSkip-gram\\n48.0\\nSkip-gram + RNNLMs\\n58.9\\nestimate since the data center machines are shared with other production tasks, and the usage can\\nﬂuctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of\\nthe CBOW model and the Skip-gram model are much closer to each other than their single-machine\\nimplementations. The result are reported in Table 6.\\n4.5\\nMicrosoft Research Sentence Completion Challenge\\nThe Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing\\nlanguage modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one\\nword is missing in each sentence and the goal is to select word that is the most coherent with the\\nrest of the sentence, given a list of ﬁve reasonable choices. Performance of several techniques has\\nbeen already reported on this set, including N-gram models, LSA-based model [32], log-bilinear\\nmodel [24] and a combination of recurrent neural networks that currently holds the state of the art\\nperformance of 55.4% accuracy on this benchmark [19].\\nWe have explored the performance of Skip-gram architecture on this task. First, we train the 640-\\ndimensional model on 50M words provided in [32]. Then, we compute score of each sentence in\\nthe test set by using the unknown word at the input, and predict all surrounding words in a sentence.\\nThe ﬁnal sentence score is then the sum of these individual predictions. Using the sentence scores,\\nwe choose the most likely sentence.\\nA short summary of some previous results together with the new results is presented in Table 7.\\nWhile the Skip-gram model itself does not perform on this task better than LSA similarity, the scores\\nfrom this model are complementary to scores obtained with RNNLMs, and a weighted combination\\nleads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and\\n58.7% on the test part of the set).\\n5\\nExamples of the Learned Relationships\\nTable 8 shows words that follow various relationships. We follow the approach described above: the\\nrelationship is deﬁned by subtracting two word vectors, and the result is added to another word. Thus\\nfor example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although\\nthere is clearly a lot of room for further improvements (note that using our accuracy metric that\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\\ngram model trained on 783M words with 300 dimensionality).\\nRelationship\\nExample 1\\nExample 2\\nExample 3\\nFrance - Paris\\nItaly: Rome\\nJapan: Tokyo\\nFlorida: Tallahassee\\nbig - bigger\\nsmall: larger\\ncold: colder\\nquick: quicker\\nMiami - Florida\\nBaltimore: Maryland\\nDallas: Texas\\nKona: Hawaii\\nEinstein - scientist\\nMessi: midﬁelder\\nMozart: violinist\\nPicasso: painter\\nSarkozy - France\\nBerlusconi: Italy\\nMerkel: Germany\\nKoizumi: Japan\\ncopper - Cu\\nzinc: Zn\\ngold: Au\\nuranium: plutonium\\nBerlusconi - Silvio\\nSarkozy: Nicolas\\nPutin: Medvedev\\nObama: Barack\\nMicrosoft - Windows\\nGoogle: Android\\nIBM: Linux\\nApple: iPhone\\nMicrosoft - Ballmer\\nGoogle: Yahoo\\nIBM: McNealy\\nApple: Jobs\\nJapan - sushi\\nGermany: bratwurst\\nFrance: tapas\\nUSA: pizza\\nassumes exact match, the results in Table 8 would score only about 60%). We believe that word\\nvectors trained on even larger data sets with larger dimensionality will perform signiﬁcantly better,\\nand will enable the development of new innovative applications. Another way to improve accuracy is\\nto provide more than one example of the relationship. By using ten examples instead of one to form\\nthe relationship vector (we average the individual vectors together), we have observed improvement\\nof accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\\nIt is also possible to apply the vector operations to solve different tasks. For example, we have\\nobserved good accuracy for selecting out-of-the-list words, by computing average vector for a list of\\nwords, and ﬁnding the most distant word vector. This is a popular type of problems in certain human\\nintelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.\\n6\\nConclusion\\nIn this paper we studied the quality of vector representations of words derived by various models on\\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\\nquality word vectors using very simple model architectures, compared to the popular neural network\\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it\\nis possible to compute very accurate high dimensional word vectors from a much larger data set.\\nUsing the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram\\nmodels even on corpora with one trillion words, for basically unlimited size of the vocabulary. That\\nis several orders of magnitude larger than the best previously published results for similar models.\\nAn interesting task where the word vectors have recently been shown to signiﬁcantly outperform the\\nprevious state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were\\nused together with other techniques to achieve over 50% increase in Spearman’s rank correlation\\nover the previous best result [31]. The neural network based word vectors were previously applied\\nto many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can\\nbe expected that these applications can beneﬁt from the model architectures described in this paper.\\nOur ongoing work shows that the word vectors can be successfully applied to automatic extension\\nof facts in Knowledge Bases, and also for veriﬁcation of correctness of existing facts. Results\\nfrom machine translation experiments also look very promising. In the future, it would be also\\ninteresting to compare our techniques to Latent Relational Analysis [30] and others. We believe that\\nour comprehensive test set will help the research community to improve the existing techniques for\\nestimating the word vectors. We also expect that high quality word vectors will become an important\\nbuilding block for future NLP applications.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='7\\nFollow-Up Work\\nAfter the initial version of this paper was written, we published single-machine multi-threaded C++\\ncode for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-\\ntectures4. The training speed is signiﬁcantly higher than reported earlier in this paper, i.e. it is in the\\norder of billions of words per hour for typical hyperparameter choices. We also published more than\\n1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of\\nour follow-up work will be published in an upcoming NIPS 2013 paper [21].\\nReferences\\n[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-\\nchine Learning Research, 3:1137-1155, 2003.\\n[2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-\\nchines, MIT Press, 2007.\\n[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine\\ntranslation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language\\nProcessing and Computational Language Learning, 2007.\\n[4] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep\\nNeural Networks with Multitask Learning. In International Conference on Machine Learning,\\nICML, 2008.\\n[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-\\nguage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-\\n2537, 2011.\\n[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.\\nSenior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\\n[7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and\\nstochastic optimization. Journal of Machine Learning Research, 2011.\\n[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.\\n[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations\\nvia Global Context and Multiple Word Prototypes. In: Proc. Association for Computational\\nLinguistics, 2012.\\n[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-\\ntributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,\\nMIT Press, 1986.\\n[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring\\ndegrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic\\nEvaluation (SemEval 2012), 2012.\\n[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for\\nsentiment analysis. In Proceedings of ACL, 2011.\\n[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-\\nversity of Technology, 2007.\\n[14] T. Mikolov, J. Kopeck´y, L. Burget, O. Glembek and J. ˇCernock´y. Neural network based lan-\\nguage models for higly inﬂective languages, In: Proc. ICASSP 2009.\\n[15] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock´y, S. Khudanpur. Recurrent neural network\\nbased language model, In: Proceedings of Interspeech, 2010.\\n[16] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, S. Khudanpur. Extensions of recurrent neural\\nnetwork language model, In: Proceedings of ICASSP 2011.\\n[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock´y. Empirical Evaluation and Com-\\nbination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\\n4The code is available at https://code.google.com/p/word2vec/\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 11}, page_content='[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇCernock´y. Strategies for Training Large Scale\\nNeural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-\\ning, 2011.\\n[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-\\nsity of Technology, 2012.\\n[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-\\ntations. NAACL HLT 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\\nWords and Phrases and their Compositionality. Accepted to NIPS 2013.\\n[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,\\n2007.\\n[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural\\nInformation Processing Systems 21, MIT Press, 2009.\\n[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language\\nmodels. ICML, 2012.\\n[25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,\\n2005.\\n[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-\\npropagating errors. Nature, 323:533.536, 1986.\\n[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,\\n2007.\\n[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and\\nUnfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.\\n[29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for\\nSemi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.\\n[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-\\ntional Joint Conference on Artiﬁcial Intelligence, 2005.\\n[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for\\nMeasuring Relational Similarity. NAACL HLT 2013.\\n[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft\\nResearch Technical Report MSR-TR-2011-129, 2011.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 12}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 13}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1706.03762v7_Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 14}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 0}, page_content='arXiv:2506.02153v1  [cs.AI]  2 Jun 2025\\nSmall Language Models are the Future of Agentic AI\\nPeter Belcak1\\nGreg Heinrich1\\nShizhe Diao1\\nYonggan Fu1\\nXin Dong1\\nSaurav Muralidharan1\\nYingyan Celine Lin1,2\\nPavlo Molchanov1\\n1NVIDIA Research\\n2Georgia Institute of Technology\\nagents@nvidia.com\\nAbstract\\nLarge language models (LLMs) are often praised for exhibiting near-human per-\\nformance on a wide range of tasks and valued for their ability to hold a general\\nconversation. The rise of agentic AI systems is, however, ushering in a mass of\\napplications in which language models perform a small number of specialized tasks\\nrepetitively and with little variation.\\nHere we lay out the position that small language models (SLMs) are sufficiently\\npowerful, inherently more suitable, and necessarily more economical for many\\ninvocations in agentic systems, and are therefore the future of agentic AI. Our\\nargumentation is grounded in the current level of capabilities exhibited by SLMs,\\nthe common architectures of agentic systems, and the economy of LM deployment.\\nWe further argue that in situations where general-purpose conversational abilities\\nare essential, heterogeneous agentic systems (i.e., agents invoking multiple different\\nmodels) are the natural choice. We discuss the potential barriers for the adoption\\nof SLMs in agentic systems and outline a general LLM-to-SLM agent conversion\\nalgorithm.\\nOur position, formulated as a value statement, highlights the significance of\\nthe operational and economic impact even a partial shift from LLMs to SLMs\\nis to have on the AI agent industry. We aim to stimulate the discussion on\\nthe effective use of AI resources and hope to advance the efforts to lower\\nthe costs of AI of the present day. Calling for both contributions to and cri-\\ntique of our position, we commit to publishing all such correspondence at\\nresearch.nvidia.com/labs/lpr/slm-agents.\\n1\\nIntroduction\\nThe deployment of agentic artificial intelligence is on a meteoric rise. Recent surveys show that more\\nthan a half of large IT enterprises are actively using AI agents, with 21% having adopted just within\\nthe last year [12]. Aside from the users, markets also see substantial economic value in AI agents: As\\nof late 2024, the agentic AI sector had seen more than USD 2bn in startup funding, was valued at\\nUSD 5.2bn, and was expected to grow to nearly USD 200bn by 2034 [42, 47]. Put plainly, there is a\\ngrowing expectation that AI agents will play a substantial role in the modern economy.\\nThe core components powering most modern AI agents are (very) large language models [48, 44]. It\\nis the LLMs that provide the foundational intelligence that enables agents to make strategic decisions\\nabout when and how to use available tools, control the flow of operations needed to complete tasks,\\nand, if necessary, to break down complex tasks into manageable subtasks and to perform reasoning\\nfor action planning and problem-solving [48, 14]. A typical AI agent then simply communicates with\\na chosen LLM API endpoint by making requests to centralized cloud infrastructure that hosts these\\nmodels [48].\\nPreprint. Under review.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 1}, page_content='LLM API endpoints are specifically designed to serve a large volume of diverse requests using one\\ngeneralist LLM. This operational model is deeply ingrained in the industry — so deeply ingrained, in\\nfact, that it forms the foundation of substantial capital bets: While the market for the LLM API serving\\nthat underlies agentic applications was estimated at USD 5.6bn in 2024 [26], the investment into\\nthe hosting cloud infrastructure surged to USD 57bn in the same year [72]. The 10-fold discrepancy\\nbetween investment and market size has been accepted, because it is assumed that this operational\\nmodel will remain the cornerstone of the industry without any substantial alterations, and that the\\nlarge initial investment will deliver returns comparable to traditional software and internet solutions\\nwithin 3-4 years [53].\\nIn this work, we recognize the dominance of the standard operational model but verbally challenge\\none of its aspects, namely the custom that the agents’ requests to access language intelligence are –\\nin spite of their comparative simplicity – handled by singleton choices of generalist LLMs. We state\\n(Section 2), argue (Section 3), and defend (Section 4) the position that the small, rather than large,\\nlanguage models are the future of agentic AI. We, however, recognize the business commitment\\nand the now-legacy praxis that is the cause for the contrary state of the present (Section 5). In remedy,\\nwe provide an outline of a conversion algorithm for the migration of agentic applications from LLMs\\nto SLMs (Section 6), and call for a wider discussion (Section 7). If needed to concretize our stance,\\nwe attach a set of short case studies estimating the potential extent of LLM-to-SLM replacement in\\nselected popular open-source agents (Appendix B).\\n2\\nPosition\\n2.1\\nDefinitions\\nFor the purpose of concretizing our position, we use the following working definitions:\\nWD1 A SLM is a LM that can fit onto a common consumer electronic device and perform inference\\nwith latency sufficiently low to be practical when serving the agentic requests of one user.\\nWD2 An LLM is a LM that is not a SLM.\\nWe justify the wording of these definitions in Appendix A, but note that their choice has little bearing\\non the essence of our position. We note that as of 2025, we would be comfortable with considering\\nmost models below 10bn parameters in size to be SLMs.\\nWe use the words agent and agentic system interchangeably, preferring the former when emphasizing\\nthe software with some agency as a whole (e.g., “as seen in popular coding agents”) and the latter\\nwhen highlighting the systems aspect of the agentic application as a sum of its components (e.g.,\\n“not all LMs of an agentic system are replaceable by SLMs”). For brevity, we focus on LMs as the\\nbedrock of agentic applications and do not explicitly consider vision-language models, although we\\nnote that our position and most of our arguments readily extend to vision-language models as well.\\n2.2\\nStatement\\nWe contend that SLMs are\\nV1 principally sufficiently powerful to handle language modeling errands of agentic appli-\\ncations;\\nV2 inherently more operationally suitable for use in agentic systems than LLMs;\\nV3 necessarily more economical for the vast majority of LM uses in agentic systems than\\ntheir general-purpose LLM counterparts by the virtue of their smaller size;\\nand that on the basis of views V1–V3 SLMs are the future of agentic AI.\\nThe phrasing of our position is deliberate. In its statement, we wish to convey that the described\\nfuture development is ultimately a necessary consequence of the differences between SLMs and\\nLLMs if the natural priorities are followed. We do not make a recommendation or try to impose an\\nobligation — we make a statement of what we see as a faithful reflection of the community’s values\\nin this context.\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 2}, page_content='2.3\\nElaboration\\nWe assert that the dominance of LLMs in the design of AI agents is both excessive and misaligned\\nwith the functional demands of most agentic use cases. While LLMs offer impressive generality and\\nconversational fluency, the majority of agentic subtasks in deployed agentic systems are repetitive,\\nscoped, and non-conversational—calling for models that are efficient, predictable, and inexpensive.\\nIn this context, SLMs not only suffice, but are often preferable. They offer several advantages: lower\\nlatency, reduced memory and computational requirements, and significantly lower operational costs,\\nall while maintaining adequate task performance in constrained domains.\\nOur position stems from a pragmatic view of language model usage patterns within agentic architec-\\ntures. These systems typically decompose complex goals into modular sub-tasks, each of which can\\nbe reliably handled by specialized or fine-tuned SLMs. We argue that insisting on LLMs for all such\\ntasks reflects a misallocation of computational resources—one that is economically inefficient and\\nenvironmentally unsustainable at scale.\\nMoreover, in cases where general reasoning or open-domain dialogue is essential, we advocate for\\nheterogeneous agentic systems, where SLMs are used by default and LLMs are invoked selectively\\nand sparingly. This modular composition — combining the precision and efficiency of SLMs with\\nthe generality of LLMs — enables the construction of agents that are both cost-effective and capable.\\nUltimately, we observe that shifting the paradigm from LLM-centric to SLM-first architectures\\nrepresents to many not only a technical refinement but also a Humean moral ought. As the AI\\ncommunity grapples with rising infrastructure costs and environmental concerns, adopting and\\nnormalizing the use of SLMs in agentic workflows can play a crucial role in promoting responsible\\nand sustainable AI deployment.\\n3\\nPosition Arguments\\nWe support views V1–V3 by the following non-exclusive arguments.\\n3.1\\nSLMs are already sufficiently powerful for use in agents\\nA1 SLMs are sufficiently powerful to take the place of LLMs in agentic systems. This argument\\nstands in support of view V1.\\nOver the past few years, the capabilities of small language models have advanced significantly.\\nAlthough the LM scaling laws remain observed, the scaling curve between model size and capabilities\\nis becoming increasingly steeper, implying that the capabilities of newer small language models\\nare much closer to those of previous large language models. Indeed, recent advances show that\\nwell-designed small language models can meet or exceed the task performance previously attributed\\nonly to much larger models.\\nExtensive comparisons with large models have been conducted in the individual works cited below,\\nbut not all capabilities assessed by benchmarks are essential to their deployment in the agentic context.\\nHere we highlight their aptitude for commonsense reasoning (an indicator of basic understanding),\\ntool calling and code generation (both indicators of the ability to correctly communicate across the\\nmodel→tool/code interface; see Figure 1; [74, 75]), and instruction following (ability to correctly\\nrespond back across the code←model interface; [80]). In each case, we also quote the efficiency\\nincrease if stated by the authors.\\n• Microsoft Phi series. Phi-2 (2.7bn) achieves commonsense reasoning scores and code\\ngeneration scores on par with 30bn models while running ∼15× faster [34]. Phi-3 small\\n(7bn) [3] achieves language understanding and commonsense reasoning on par with and\\ncode generation scores running up to 70bn models of the same generation.\\n• NVIDIA Nemotron-H family. The 2/4.8/9bn hybrid Mamba-Transformer models achieve\\ninstruction following and code-generation accuracy comparable to dense 30bn LLMs of the\\nsame generation at an order-of-magnitude fraction of the inference FLOPs [7].\\n• Huggingface SmolLM2 series. SmolLM2 family of compact language models with sizes\\nranging from 125mn to 1.7bn parameters [6] each run up in their language understanding,\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 3}, page_content='Figure 1: An illustration of agentic systems with different modes of agency. Left: Language model\\nagency. The language model acts both as the HCI and the orchestrator of tool calls to carry out a\\ntask. Right: Code agency. The language model fills the role of the HCI (optionally) while a dedicated\\ncontroller code orchestrates all interactions.\\ntool calling, and instruction following performance to 14bn contemporaries while matching\\n70bn models of 2 years prior.\\n• NVIDIA Hymba-1.5B. This Mamba-attention hybrid-head SLM demonstrates best in-\\nstruction accuracy and 3.5× greater token throughput than comparably-sized transformer\\nmodels [20]. On instruction following, it outperforms larger 13bn models.\\n• DeepSeek-R1-Distill series. DeepSeek-R1-Distill is a family of reasoning models featuring\\n1.5-8bn sizes, trained on samples generated by DeepSeek-R1 [16]. They demonstrate strong\\ncommonsense reasoning capabilities. Notably, the DeepSeek-R1-Distill-Qwen-7B model\\noutperforms large proprietary models such as Claude-3.5-Sonnet-1022 and GPT-4o-0513.\\n• DeepMind RETRO-7.5B: Retrieval-Enhanced Transformer (RETRO) is a 7.5bn param-\\neter model augmented with an extensive external text database, achieving performance\\ncomparable to GPT-3 (175B) on language modeling while using 25× fewer parameters [8].\\n• Salesforce xLAM-2-8B. The 8bn model achieves state-of-the-art performance on tool\\ncalling despite is relatively modest size, surpassing frontier models like GPT-4o and Claude\\n3.5 [78].\\nNote that on top of competitive off-the-shelf performance, the reasoning capabilities of SLMs can\\nbe enhanced at inference time with self-consistency, verifier feedback, or tool augmentation — e.g.,\\nToolformer (6.7bn) outperforms GPT-3 (175bn) via API use [61], and 1-3bn models have rivaled\\n30bn+ LLMs on math problems via structured reasoning [81].\\nIn sum, with modern training, prompting, and agentic augmentation techniques, capability — not\\nthe parameter count — is the binding constraint. SLMs now supply sufficient reasoning power for\\na substantial portion of agentic invocations, making them not just viable, but comparatively more\\nsuitable than LLMs for modular and scalable agentic systems.\\n3.2\\nSLMs are more economical in agentic systems\\nA2 SLMs are more economical than LLMs in agentic systems. This argument supports view V3.\\nSmall models provide significant benefits in cost-efficiency, adaptability, and deployment flexibility.\\nThese advantages are specifically valuable in agentic workflows where specialization and iterative\\nrefinement are critical. Section 3.1 detailed a number of efficiency comparisons of the listed SLMs to\\nrelevant LLMs. Here we draw a more encompassing picture to support argument A2.\\n• Inference efficiency. Serving a 7bn SLM is 10–30× cheaper (in latency, energy consump-\\ntion, and FLOPs) than a 70–175bn LLM, enabling real-time agentic responses at scale\\n[66, 64, 33, 49]. Recent advances in inference operating systems such as NVIDIA Dy-\\nnamo [21] explicitly provide support for high-throughput, low-latency SLM inference in\\nboth cloud and edge deployments. In addition, since SLMs require less or no parallelization\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 4}, page_content='across GPUs and nodes, the maintenance and operation of the serving infrastructure comes\\nat a lower expense as well (see counter-argument CA4 and argument A13).\\n• Fine-tuning agility.\\nParameter-efficient (e.g., LoRA [30] and DoRA [40]) and full-\\nparameter finetuning for SLMs require only a few GPU-hours, allowing behaviors to be\\nadded, fixed, or specialized overnight rather than over weeks [66].\\n• Edge deployment. Advances in on-device inference systems such as ChatRTX [55] demon-\\nstrate local execution of SLMs on consumer-grade GPUs, showcasing real-time, offline\\nagentic inference with lower latency and stronger data control.\\n• Parameter utilization. At the outset, LLMs appear to operate as monoliths involving a large\\namount of parameters representing swathes of compressed information in the production of\\ntheir outputs. On a closer look, however, much of the signals passing through these systems\\nis sparse, engaging only a fraction of their parameters for any single input [65, 41]. That\\nthis behavior appears to be more subdued in SLMs [65, 71] suggests that SLMs may be\\nfundamentally more efficient by the virtue of having a smaller proportion of their parameters\\ncontribute to the inference cost without a tangible effect on the output.\\nModular system design. The position outlined in [52] presents a thorough argument in favor of\\ncomposite agentic systems. Here we note that the approach of leveraging several models of varying\\nsizes aligns well with the real-world heterogeneity of agentic tasks and is already slowly being\\nincorporated into major software development frameworks [25]. Furthermore, this newly discovered\\nsense for modularity in the context of agents allows for the easy addition of new skills and the ability\\nto adapt to changing requirements, and is consistent with the push for modularity in language model\\ndesign [24, 10, 37].\\nThe above-mentioned “Lego-like” composition of agentic intelligence—scaling out by adding small,\\nspecialized experts instead of scaling up monolithic models—yields systems that are cheaper, faster\\nto debug, easier to deploy, and better aligned with the operational diversity of real-world agents.\\nWhen combined with tool calling, caching, and fine-grained routing, SLM-first architectures appear\\nto offer the best path forward for cost-effective, modular, and sustainable agentic AI.\\n3.3\\nSLMs are more flexible\\nA3 SLMs possess greater operational flexibility in comparison to LLMs. This argument stands\\nin support of views V2 and V3.\\nDue to their small size and the associated reduction in pre-training and fine-tuning costs (Section 3.2),\\nSLMs are inherently more flexible than their large counterparts when appearing in agentic systems. As\\nsuch, it becomes much more affordable and practical to train, adapt, and deploy multiple specialized\\nexpert models for different agentic routines. This efficiency enables rapid iteration and adaptation,\\nmaking it feasible to address evolving user needs, including supporting new behaviors, meeting new\\noutput formatting requirements, and complying with changing local regulation in selected markets\\n[69, 38, 68].\\nDemocratization.\\nOne particularly notable and desirable consequence of SLM flexibility when put\\nin place of LLMs is the ensuing democratization of agents. When more individuals and organizations\\ncan participate in developing language models with the aim for deployment in agentic systems, the\\naggregate population of agents is more likely to represent a more diverse range of perspectives and\\nsocietal needs. This diversity can then help with reducing the risk of systemic biases and encourage\\ncompetition and innovation. With more actors entering the field to create and refine models, the field\\nwill advance more rapidly [35].\\n3.4\\nAgents expose only very narrow LM functionality\\nA4 Agentic applications are interfaces to a limited subset of LM capabilities. This supports\\nviews V1 and V2.\\nAn AI agent is essentially a heavily instructed and externally choreographed gateway to a language\\nmodel featuring a human-computer interface and a selection of tools that, when engaged correctly,\\ndo something of utility [69]. From this perspective, the underlying large language model that was\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 5}, page_content='engineered to be a powerful generalist is through a set of tediously written prompts and meticulously\\norchestrated context management restricted to operate within a small section of its otherwise large\\npallet of skills. Thus, we argue that a SLM appropriately fine-tuned for the selected prompts would\\nsuffice while having the above-mentioned benefits of increased efficiency and greater flexibility.\\nIt could be argued back that the careful interfacing with a generalist LLM is necessary for strong\\nperformance on the narrow task because of the LLM’s better understanding of the broader language\\nand the world (alternative view AV1). This is addressed in Section 4.1.\\n3.5\\nAgentic interactions necessitate close behavioral alignment\\nA5 Agentic interactions necessitate close behavioral alignment. This aligns with view V2.\\nA typical AI agent has frequent interactions with code, be it through LM tool calling or by returning\\noutput that is to be parsed by a piece of agentic code that makes the LM call [48]. It is essential for\\nthe success of these interactions that the generated tool call and the generated output conform to strict\\nformatting requirements imposed on it by the order, typing, and nature of the tool’s parameters, and\\nthe expectation of the code invoking the LM, respectively. In such cases, it becomes unnecessary\\nfor the model to handle multiple different formats (e.g. JSON/XML/Python for tool calls and\\nXML/YAML/Markdown/Latex for output [50]), as only one would be chosen for consistency across\\nthe agentic application. It is also undesirable for the model to make the occasional hallucinatory\\nmistake and respond in a format different from that being expected by the “code parts” of the agentic\\nsystem. It is because of this that the SLM trained with a single formatting decision enforced during\\nits post-training or encouraged through additional fine-tuning at a low cost is preferable over a\\ngeneral-purpose LLM in the context of AI agents.\\n3.6\\nAgentic systems are naturally heterogeneous\\nA6 Agentic systems naturally allow for heterogeneity in the selection of models that they use.\\nThis aligns with view V2.\\nA language model can itself be a tool called by another language model. Likewise, every time the\\nagent’s code invokes a language model, it can, in principle, choose any language model. This is\\nillustrated in Figure 1. We argue that incorporating multiple language models of different sizes and\\ncapabilities for queries or operations of different levels of complexity offers a natural way for the\\nintroduction of SLMs. In the context of Figure 1-Left, an LLM can be used for the model with\\nthe root agency, while a SLM could be used for the subordinate LM. In Figure 1-Right, all LMs\\ncould in principle be specialized SLMs: one for conversationality, another one for carrying out\\ncontroller-defined language modeling tasks.\\n3.7\\nAgentic interactions are natural pathways for gathering data for future improvement\\nA7 Agentic interactions are a good source for data for future model improvement. This is\\nfundamentally supportive of view V2.\\nAs noted in Section 3.4, invocations of tools and language models during an agentic process are\\noften accompanied by careful prompting that focuses the language model on delivering the narrow\\nfunctionality that is required at the time. Each one of these invocations is itself a natural source of\\ndata for future improvement (under the necessary assumption that no non-retainable confidential\\ndata is being processed). A listener decorating the tool/model call interface can gather specialized\\ninstruction data that can later be used to produce a fine-tune an expert SLM and lower the cost of that\\ncall in the future (see logger in Figure 1). We argue that this avenue is enabled by the architecture\\nof agents [48] and produces high-quality organic data (that can be further post-filtered by considering\\nthe overall success of the workflow), thus making the production of expert SLMs to stand in place of\\nLLMs a natural step in agent deployment — not just an auxiliary effort.\\n4\\nAlternative Views\\nThe following significant alternative views have been expressed in the academic and popular literature.\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 6}, page_content='4.1\\nLLM generalists will always have the advantage of more general language understanding\\nAV1 Let T be a single task using general language and let L, S be a large and a small language\\nmodel of the same generation, respectively. The performance of L on T will always trump\\nthat of S.\\nThis alternative view disputes view V2 and rests on the following counter-arguments:\\nCA1 There is a non-negligible body of empirical evidence of the superiority of large language\\nmodels in general language understanding over small language models of the same genera-\\ntion. LLMs acquire their language understanding capabilities in accordance with scaling\\nlaws [15]. Their larger scale then enables them to demonstrate better performance across a\\nwide array of specialized natural language tasks, including text generation, translation, and\\nreasoning, outperforming small models trained both (a) in the same general fashion and (b)\\nfrom scratch specifically for these tasks [54]. It can then be said that to claim otherwise is to\\ncontradict the LM scaling laws [29, 28].\\nCA2 Moreover, recent studies also purport that LLMs possess a “semantic hub” mechanism,\\nwhich has been hypothesized to enable them to integrate and abstract semantic information\\nfrom various modalities and languages in a generalized manner [77]. If true, LLMs could be\\nthought to generalize knowledge across languages and domains far more effectively than\\nsmaller models, which under the same study lack the capacity for the presence of such a\\nhub [77]. It can then argued that while small language models may be efficient for narrowly\\ndefined or highly specialized tasks, their limited scale fundamentally restricts their ability\\nto achieve the same level of general language understanding in these specialized as LLMs\\nbecause of the lack of room for the internalization of complex abstractions.\\nA conclusion can be then drawn that LLM generalist models will always retain the advantage of\\nuniversally better performance on language tasks, no matter how narrowly defined, over small\\nlanguage models of the same generation. This to be to their advantage over SLMs when deployed in\\nagentic applications.\\nRebuttal.\\nThe above alternative view is the most popularly cited belief against the use of SLMs,\\neven when only a narrow language task needs to be performed [2, 67, 27, 1].\\nWe believe that counter-argument CA1 is too limited to attack view V2, namely because\\nA8 Popular scaling law studies assume the model architecture to be kept constant [29, 28] within\\nthe same generation, whereas the recent work on small language model training demonstrates\\nthat there are distinct performance benefits to considering different architectures for different\\nmodel sizes [20, 7].\\nA9 The flexibility of small language models (Section 3.3) comes to the rescue. A small language\\nmodel can be easily fine-tuned for the task T of alternative view AV1 to perform to the\\ndesired level of reliability. This is unaccounted for in scaling law studies.\\nA10 Reasoning (or, more generally, test-time compute scaling; see Section 3.2) is significantly\\nmore affordable. A small language model, still retaining its benefits of greater cross-device\\nagility can be reasonable expected to be scalable at inference time to the desired level of\\nreliability.\\nWe also believe that counter-argument CA2 is too arcane to attack view V2 because\\nA11 The utility of the purported “semantic hub” shows itself when tasks or inputs at hand to be\\nprocessed by the LM are complex. However, advanced agentic systems are either designed in\\ntheir entirety or at least actively prompted to perform decompositions of complex problems\\nand inputs [48, 14]. Therefore, we argue to the contrary that invocations of small language\\nmodels within agentic systems would be on appropriately broken-down into sub-tasks so\\nsimple that any general abstract understanding due to the hub would be of little utility.\\n4.2\\nLLM inference will still be cheaper because of their centralization\\nAV2 The per-token inference cost benefit of the smallness of specialized SLMs in agentic\\napplications is dwarfed by the economy of scale.\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 7}, page_content='It could be argued that the analysis in argument A2 that put forth in favor of view V3 was ignorant of\\nthe wider business of AI model deployment:\\nCA3 It is more difficult to fully utilize and properly balance the load for an expert SLM inference\\nendpoint than it is for a generalist LLM endpoint [66, 22].\\nCA4 The costs of inference infrastructure setup combined with the costs of acquiring and retain-\\ning talent for its upkeep are often omitted in inference cost calculations but figure more\\nprominently if the deployment of (S)LMs became the responsibility of the agent service de-\\nveloper. Early industrial reports point to considerable costs associated with these operations\\n[36, 11, 63].\\nAcknowledgment. We acknowledge that alternative view AV2 is a valid view, with the exact\\neconomical considerations being highly case-specific. We believe that the jury is still out on alternative\\nview AV2, but that several factors hint that view V3 might prevail:\\nA12 Recent improvements in inference scheduling and large inference system modularization\\noffer unprecedented levels of inference system flexibility in monolithic computing clusters\\n[82, 56, 46], countering the traditional stance expressed in counter-argument CA3.\\nA13 The most recent analyses on the set-up costs of inference infrastructure indicate a consistent\\nfalling trend due to underlying technological reasons [79, 4].\\n4.3\\nEqually possible worlds\\nAV3 Both the agentic world utilizing SLMs and agentic world utilizing LLMs are equally possible\\nworlds, but the “LLM agentic world” has a considerable head start in terms of deployment\\npractice and optimization, and the industry inertia already funnels efforts into innovation\\nsolely in that direction.\\nAcknowledgment. We acknowledge alternative view AV3 as a distinct possibility, but maintain the\\nposition that the weight of advantages described across arguments A1–A7 can plausibily overturn the\\npresent state of affairs.\\n5\\nBarriers to Adoption\\nIt would be prudent to ask oneself: If the arguments A1–A7 are truly compelling, why do the ever\\nnewer generations of agents seemingly just perpetuate the status quo of using generalist LLMs?\\nWe believe the following to be among the today’s main barriers to wide-spread adoption of SLMs:\\nB1 Large amounts of upfront investment into centralized LLM inference infrastructure.\\nAs detailed in Section 1, large capital bets have been made on the centralized LLM inference\\nbeing the leading paradigm in providing AI services in the future. As such, the industry\\nhas been much quicker at building the tools and infrastructure to that end, omitting any\\nconsiderations for the possibility that more decentralized SLM or on-device inference might\\nbe equally feasible in the near future.\\nB2 Use of generalist benchmarks in SLM training, design, and evaluation. It must be\\npointed out that much of the work on SLM design and development follows the tracks of\\nLLM design, focusing on the same generalist benchmarks in their development [43, 57]. On\\nthis point, [20] notes that if one focuses solely on benchmarks measuring the agentic utility\\nof agents, the studied SLMs easily outperform larger models.\\nB3 Lack of popular awareness. SLMs often do not receive the level of marketing intensity\\nand press attention LLMs do, despite their better suitability in many industrial scenarios.\\nWe note that barriers B1–B3 are practical hurdles and far from being fundamental flaws of the SLM\\ntechnology in the context of agentic AI. With advanced inference scheduling systems such as Dynamo\\n[21], barrier B1 is being reduced to a mere effect of inertia. barrier B2 is becoming increasingly\\nrecognized in the field [20, 34], and it would be natural for barrier B3 to fall once the economic\\nbenefits of SLM deployment in agentic applications (argument A2) are better known. With the inertia\\nof barrier B1 in particular, we do not endeavor to give a timeline for the retreat of these barriers or\\nthe popular adoption of SLMs.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 8}, page_content='6\\nLLM-to-SLM Agent Conversion Algorithm\\nThe very nature of agentic applications enables them to eventually switch from using LLM generalists\\nto using SLM specialists at many of their interfaces. In the following steps, we outline an algorithm\\nthat describes one possible way to carry out the change of the underlying model painlessly.\\nS1 Secure usage data collection. The initial step involves deploying instrumentation to log\\nall non-HCI agent calls, capturing input prompts, output responses, contents of individual\\ntool calls, and optionally latency metrics for a later targeted optimization. In terms of\\nimplementation, it is the recommended practice to set up encrypted logging pipelines with\\nrole-based access controls [51] and anonymize all data with respect to its origins before\\nstorage [70]. See logger in Figure 1 for an illustration.\\nS2 Data curation and filtering. Begin collecting data through the pipelines of step S1. Once\\na satisfactory amount of data has been collected (10k-100k examples being sufficient for\\nfine-tuning of small models as a rule of thumb [5, 19]), it is necessary to remove any PII,\\nPHI, or any other application-specific sensitive data that could cause a data leak across\\nuser accounts once used to produce a SLM specialist. Many typical varieties of sensitive\\ndata can be detected and masked or removed using popular automated tools for dataset\\npreparation [60, 58]. Application specific inputs (e.g. legal or internal documents) can\\nbe often be automatically paraphrased to obfuscate named entities and numerical details\\nwithout compromising the general information content [9, 76, 73].\\nS3 Task clustering. Employ unsupervised clustering techniques on the collected prompts\\nand agent actions to identify recurring patterns of requests or internal agent operations\\n[32, 39, 18]. These clusters help define candidate tasks for SLM specialization. The\\ngranularity of tasks will depend on the diversity of operations; common examples include\\nintent recognition, data extraction, summarization of specific document types, or code\\ngeneration with respect to tools available to the agent.\\nS4 SLM selection. For each identified task, select one or more candidate SLMs. Criteria for\\nselection include the SLM’s inherent capabilities (e.g., instruction following, reasoning,\\ncontext window size), its performance on relevant benchmarks for the task type, its licensing,\\nand its deployment footprint (memory, computational requirements). Models of Section 3.2\\nserve as good starting candidates.\\nS5 Specialized SLM fine-tuning. For each selected task and corresponding SLM candidate,\\nprepare a task-specific dataset from the curated data collected in steps S2 and S3. Then, fine-\\ntune the chosen SLMs on these specialized datasets. PEFT techniques such as LoRA [31]\\nor QLoRA [17] can be leveraged to reduce computational costs and memory requirements\\nassociated with fine-tuning, making the process more accessible. Full fine-tuning can also\\nbe considered if resources permit and maximal adaptation is required. In some cases, it may\\nbe beneficial to use knowledge distillation, where the specialist SLM is trained to mimic\\nthe outputs of the more powerful generalist LLM on the task-specific dataset. This can help\\ntransfer some of the more nuanced capabilities of the LLM to the SLM.\\nS6 Iteration and refinement. One may retrain the SLMs and the router model periodically\\nwith new data to maintain performance and adapt to evolving usage patterns. This forms a\\ncontinuous improvement loop, returning to step S2 or step S4 as appropriate.\\n7\\nCall for Discussion\\nThe agentic AI industry is showing the signs of a promise to have a transformative effect on white\\ncollar work and beyond.\\nIt is the view of the authors that any expense savings or improvements on the sustainability of AI\\ninfrastructure would act as a catalyst for this transformation, and that it is thus eminently desirable to\\nexplore all options for doing so.\\nWe therefore call for both contributions to and critique of our position,\\nto be di-\\nrected to agents@nvidia.com,\\nand commit to publishing all such correspondence at\\nresearch.nvidia.com/labs/lpr/slm-agents.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 9}, page_content='References\\n[1] Aashima. Small language models vs. llms: Finding the right fit for your needs, October 2024.\\nAccessed: 2025-05-09.\\n[2] ABBYY. Small language models vs. large language models, November 2024. Accessed:\\n2025-05-09.\\n[3] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen\\nBach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report:\\nA highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219,\\n2024.\\n[4] Adyog. The economics of ai training and inference: How deepseek broke the cost curve,\\nFebruary 2025. Accessed: 2025-05-09.\\n[5] Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, and Marina Danilevksy. Delift: Data\\nefficient language model instruction fine tuning. arXiv preprint arXiv:2411.04425, 2024.\\n[6] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo,\\nLewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav,\\nJoshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw,\\nHugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra,\\nand Thomas Wolf. Smollm2: When smol goes big – data-centric training of a small language\\nmodel, 2025.\\n[7] Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich,\\nAleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil\\nMahabaleshwarkar, et al. Nemotron-h: A family of accurate and efficient hybrid mamba-\\ntransformer models. arXiv preprint arXiv:2504.03624, 2025.\\n[8] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\\nMillican, George van den Driessche, Bogdan Damoc, Aidan Clark, Jan Kramár, et al. Improving\\nlanguage models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426, 2022.\\n[9] Michael Brennan, Sadia Afroz, and Rachel Greenstadt. Adversarial stylometry: Circumventing\\nauthorship recognition to preserve privacy and anonymity. ACM Transactions on Information\\nand System Security (TISSEC), 15(3):1–22, 2012.\\n[10] Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, and\\nPavlo Molchanov. Flextron: Many-in-one flexible large language model. In Proceedings of the\\n41st International Conference on Machine Learning (ICML 2024), 2024.\\n[11] Michael Chui, Bryce Hall, Helen Mayhew, Alex Singla, and Alexander Sukharevsky. The state\\nof ai in 2022—and a half decade in review, December 2022. Accessed: 2025-05-09.\\n[12] Cloudera, Inc. 96% of enterprises are expanding use of ai agents, according to latest data from\\ncloudera, April 2025. Accessed: 2025-05-08.\\n[13] Planck Collaboration et al. Planck 2018 results. vi. cosmological parameters. Astronomy &\\nAstrophysics, 641:A6, 2020.\\n[14] DAIR.AI. Llm agents, April 2024. Accessed: 2025-05-08.\\n[15] Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu. Security and privacy challenges of\\nlarge language models: A survey. ACM Computing Surveys, 57(6):1–39, 2025.\\n[16] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\\nlearning, 2025.\\n[17] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\\nfinetuning of quantized llms. Advances in neural information processing systems, 36:10088–\\n10115, 2023.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 10}, page_content='[18] Shizhe Diao, Yu Yang, Yonggan Fu, Xin Dong, Dan Su, Markus Kliegl, Zijia Chen, Peter\\nBelcak, Yoshi Suhara, Hongxu Yin, et al. Climb: Clustering-based iterative data mixture\\nbootstrapping for language model pre-training. arXiv preprint arXiv:2504.13161, 2025.\\n[19] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu,\\nYulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale\\npre-trained language models. Nature Machine Intelligence, 5(3):220–235, 2023.\\n[20] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabalesh-\\nwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. Hymba:\\nA hybrid-head architecture for small language models. arXiv preprint arXiv:2411.13676, 2024.\\n[21] Amr Elmeleegy et al. Introducing nvidia dynamo, a low-latency distributed inference framework\\nfor scaling reasoning ai models, March 2025. NVIDIA Technical Blog.\\n[22] Henry Evans. Llms vs. slms: Balancing comprehensiveness and smart resource-saving, April\\n2025. Accessed: 2025-05-09.\\n[23] Barbara A Ferguson, Timothy A Dreisbach, Catherine G Parks, Gregory M Filip, and Craig L\\nSchmitt. Coarse-scale population structure of pathogenic armillaria species in a mixed-conifer\\nforest in the blue mountains of northeast oregon.\\nCanadian Journal of Forest Research,\\n33(4):612–623, 2003.\\n[24] Yonggan Fu, Zhongzhi Yu, Junwei Li, Jiayi Qian, Yongan Zhang, Xiangchi Yuan, Dachuan Shi,\\nRoman Yakunin, and Yingyan Celine Lin. Amoeballm: Constructing any-shape large language\\nmodels for efficient and instant deployment. In Proceedings of the 38th Annual Conference on\\nNeural Information Processing Systems (NeurIPS 2024), 2024.\\n[25] google. GitHub - google/A2A: An open protocol enabling communication and interoperability\\nbetween opaque agentic applications.\\n[26] Grand View Research, Inc. Large language models market size, share & trends analysis report\\nby application (customer service, content generation), by deployment (cloud, on-premise), by\\nindustry vertical, by region, and segment forecasts, 2025 - 2030, February 2025. Accessed:\\n2025-05-08.\\n[27] Harrison Clarke. Large language models vs. small language models, March 2024. Accessed:\\n2025-05-09.\\n[28] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for\\ntransfer. arXiv preprint arXiv:2102.01293, 2021.\\n[29] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\\n[30] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arxiv 2021.\\narXiv preprint arXiv:2106.09685, 2021.\\n[31] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\\nLu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR,\\n1(2):3, 2022.\\n[32] Shaohan Huang, Furu Wei, Lei Cui, Xingxing Zhang, and Ming Zhou. Unsupervised fine-tuning\\nfor text clustering. In Proceedings of the 28th international conference on computational\\nlinguistics, pages 5530–5534, 2020.\\n[33] Invisible Technologies. How small language models can outperform llms, March 2025. Ac-\\ncessed: 2025-05-21.\\n[34] Mojan Javaheripi and Sébastien Bubeck. Phi-2: The surprising power of small language models,\\n2023. Microsoft Research Blog.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 11}, page_content='[35] Andreas Jungherr. Artificial intelligence and democracy: A conceptual framework. Social\\nmedia+ society, 9(3):20563051231186353, 2023.\\n[36] Aviv Kaufmann. Understanding the total cost of inferencing large language models. Technical\\nreport, Enterprise Strategy Group, April 2024. Commissioned by Dell Technologies. Accessed:\\n2025-05-09.\\n[37] Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia\\nTsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain, et al. Matformer:\\nNested transformer for elastic inference. arXiv preprint arXiv:2310.07707, 2023.\\n[38] Akshi Kumar. From large to small: The rise of small language models (slms) in text analytics.\\n2025.\\n[39] Luying Liu, Jianchu Kang, Jing Yu, and Zhongliang Wang. A comparative study on unsupervised\\nfeature selection methods for text clustering. In 2005 International Conference on Natural\\nLanguage Processing and Knowledge Engineering, pages 597–601. IEEE, 2005.\\n[40] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang,\\nKwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation.\\narXiv preprint arXiv:2402.09353, 2024.\\n[41] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivas-\\ntava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient\\nllms at inference time. In International Conference on Machine Learning, pages 22137–22176.\\nPMLR, 2023.\\n[42] Jeff Loucks, Gillian Crossan, Baris Sarer, China Widener, and Ariane Bucaille. Autonomous\\ngenerative ai agents: Under development. Deloitte Insights, November 2024. Accessed:\\n2025-05-08.\\n[43] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D Lane,\\nand Mengwei Xu. Small language models: Survey, measurements, and insights. arXiv preprint\\narXiv:2409.15790, 2024.\\n[44] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu,\\nBinqi Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: A survey on\\nmethodology, applications and challenges. arXiv preprint arXiv:2503.21460, 2025.\\n[45] Georgina M Mace, Paul H Harvey, and Timothy H Clutton-Brock. Brain size and ecology in\\nsmall mammals. Journal of Zoology, 193(3):333–354, 1981.\\n[46] Tobias Mann. A closer look at dynamo, nvidia’s ’operating system’ for ai inference, March\\n2025. Accessed: 2025-05-09.\\n[47] Market.us. Global agentic ai market size, share analysis by product type, agent role, agent\\nsystem, end user, region and companies – industry segment outlook, market assessment, compe-\\ntition scenario, trends and forecast 2025–2034, March 2025. Accessed: 2025-05-08.\\n[48] Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging\\nai agent architectures for reasoning, planning, and tool calling: A survey. arXiv preprint\\narXiv:2404.11584, 2024.\\n[49] Sourabh Mehta. How much energy do llms consume? unveiling the power behind ai, July 2024.\\nAccessed: 2025-05-21.\\n[50] Meta Platforms, Inc. Model cards and prompt formats: Llama 3.3, April 2025. Accessed:\\n2025-05-08.\\n[51] Metomic. Understanding ai agents & data security, 2025. Accessed: 2025-05-13.\\n[52] Erik Miehling, Karthikeyan Natesan Ramamurthy, Kush R Varshney, Matthew Riemer, Djallel\\nBouneffouf, John T Richards, Amit Dhurandhar, Elizabeth M Daly, Michael Hind, Prasanna\\nSattigeri, et al. Agentic ai needs a systems theory. arXiv preprint arXiv:2503.00237, 2025.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 12}, page_content='[53] Morgan Stanley. Genai revenue growth and profitability, April 2025. Accessed: 2025-05-08.\\n[54] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad\\nUsman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. A comprehensive overview of large\\nlanguage models. arXiv preprint arXiv:2307.06435, 2023.\\n[55] NVIDIA. Chatrtx, 2024. NVIDIA AI Product.\\n[56] NVIDIA. Nvidia dynamo: A datacenter scale distributed inference serving framework. https:\\n//github.com/ai-dynamo/dynamo, 2025. Accessed: 2025-05-09.\\n[57] Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail\\nYurochkin.\\ntinybenchmarks:\\nevaluating llms with fewer examples.\\narXiv preprint\\narXiv:2402.14992, 2024.\\n[58] Lakshmi Radhakrishnan, Gundolf Schenk, Kathleen Muenzen, Boris Oskotsky, Habibeh\\nAshouri Choshali, Thomas Plunkett, Sharat Israni, and Atul J Butte. A certified de-identification\\nsystem for all clinical text documents for information extraction at scale.\\nJAMIA open,\\n6(3):ooad045, 2023.\\n[59] Martin J Rees. Before the Beginning: Our Universe and Others. Addison-Wesley, 1997.\\n[60] Judith Sáinz-Pardo Díaz and Álvaro López García. An open source python library for anonymiz-\\ning sensitive data. Scientific data, 11(1):1289, 2024.\\n[61] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\\nthemselves to use tools. In Advances in Neural Information Processing Systems (NeurIPS),\\n2023.\\n[62] J William Schopf. Microfossils of the early archean apex chert: New evidence of the antiquity\\nof life. Science, 260(5108):640–646, 1993.\\n[63] Tanya Seda. Cloud llm cost model: Breakdown for mid-market businesses, 2024. Accessed:\\n2025-05-09.\\n[64] Olivia Shone. Explore ai models: Key differences between small language models and large\\nlanguage models, November 2024. Accessed: 2025-05-21.\\n[65] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model\\nserving with a consumer-grade gpu. In Proceedings of the ACM SIGOPS 30th Symposium on\\nOperating Systems Principles, pages 590–606, 2024.\\n[66] Shreyas Subramanian, Vikram Elango, and Mecit Gungor. Small language models (slms) can\\nstill pack a punch: A survey. arXiv preprint arXiv:2501.05465, 2025.\\n[67] Synergy Technical. Small language models vs. large language models, 2025. Accessed:\\n2025-05-09.\\n[68] Brian G. Thamm. Trustworthy and secure ai: How small language models strengthen data\\nsecurity. Service Contractor Magazine, October 2024. Accessed: 2025-05-08.\\n[69] Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, Tzuhao Mo, Qiuhao Lu, Wanjing Wang,\\nRui Li, Junjie Xu, Xianfeng Tang, et al. A comprehensive survey of small language models in\\nthe era of large language models: Techniques, enhancements, applications, collaboration with\\nllms, and trustworthiness. arXiv preprint arXiv:2411.03350, 2024.\\n[70] WorkOS. Build secure ai agents, 2025. Accessed: 2025-05-13.\\n[71] Zhenliang Xue, Yixin Song, Zeyu Mi, Xinrui Zheng, Yubin Xia, and Haibo Chen. Powerinfer-2:\\nFast large language model inference on a smartphone. arXiv preprint arXiv:2406.06282, 2024.\\n[72] Niva Yadav. Ai drove record $57bn in data center investment in 2024, March 2025. Accessed:\\n2025-05-16.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 13}, page_content='[73] Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzhen\\nCheng. On protecting the data privacy of large language models (llms): A survey. arXiv preprint\\narXiv:2403.05156, 2024.\\n[74] Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and\\nJoseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.\\nedu/blogs/8_berkeley_function_calling_leaderboard.html, 2024.\\n[75] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. Tau-bench: A benchmark\\nfor tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024.\\n[76] Da Yu, Peter Kairouz, Sewoong Oh, and Zheng Xu. Privacy-preserving instructions for aligning\\nlarge language models. arXiv preprint arXiv:2402.13659, 2024.\\n[77] Adam Zewe. Like human brains, large language models reason about diverse data in a general\\nway. MIT News, February 19 2025. Accessed: 2025-05-09.\\n[78] Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao,\\nJuntao Tan, Akshara Prabhakar, Haolin Chen, et al. xlam: A family of large action models to\\nempower ai agent systems. arXiv preprint arXiv:2409.03215, 2024.\\n[79] Kevin Zhang. A deep dive on ai inference startups, 2024. Accessed: 2025-05-09.\\n[80] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\\nZhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint\\narXiv:2311.07911, 2023.\\n[81] Xuezhi Zhou, Nathanael Schärli, Yujie Hou, Jason Wei, Denny Zhou, Quoc V. Le, and Douwe\\nKiela. Least-to-most prompting enables complex reasoning in small language models. arXiv\\npreprint arXiv:2205.10625, 2022.\\n[82] David Zier and Harry Kim. Introducing nvidia dynamo, a low-latency distributed inference\\nframework for scaling reasoning ai models, March 2025. Accessed: 2025-05-09.\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 14}, page_content='A\\nDefinitions\\nThis appendix provides two justifications for the choice of definitions in Section 2.1.\\nA.1\\nPragmatic argument\\nIt is desirable to have a definition of SLMs that meets three key criteria:\\n• Timelessness.\\nThe definition should be timeless: It should avoiding dependence on\\nhardware-specific metrics like parameter count or FLOPs, which quickly become obsolete\\nas technology advances—what qualifies as “small” today may be “large” tomorrow.\\n• Practicality. The definition is likely to have much wider generality if it is grounded in\\npractical use, reflecting the real-world goal of deploying SLMs on widely available consumer\\ndevices, where they can serve the user in their proximity with low-latency inference.\\n• Motivation alignment. The definition should capture the fundamental motivation that\\ndrives the training of SLMs in the first place, which is to enable capable language models\\nthat can run on-device or within significantly constrained budgets compared to LLMs.\\nWe find definition WD1 to possess all three. Definition WD2 is then phrased to complement the set\\nof all language models.\\nA.2\\nLimit argument\\nTo explore the distinction between small and large language models in the context of agentic AI, let\\nus adopt the uncompromising lens of an extremalist, for whom intelligence must be either maximally\\nsmall or maximally large.\\nImagine a super-intelligent system spanning galactic scales, marshaling all available matter to\\noptimize its computations. Such a system, while theoretically capable of addressing profound\\nquestions would face insurmountable physical constraints. The speed of light limits communication,\\nwith round-trip delays across a galaxy potentially spanning tens of thousands of years [59]. This\\nlatency precludes real-time coordination, fragmenting the system into loosely coupled components\\nrather than a unified \"mind\". At cosmological scales, spanning millions or billions of light-years,\\ncommunication delays could approach or exceed the universe’s age of 13.8 billion years [13]. Such a\\nsystem, while vast, would be impractical for human-relevant applications, its computations unfolding\\nover eons.\\nConversely, consider an infinitely small intelligent system, reduced to the minimal substrate capable\\nof computation. Such a system, akin to the simplest biological organisms, would lack the sensors,\\neffectors, or computational capacity to meaningfully interact with its environment. Its intelligence\\nwould be constrained to rudimentary evolution, much like early life forms that emerged 3.5 billion\\nyears ago [62]. Yet, even in nature, scale varies dramatically: living organisms range from bacteria\\n(hundreds of nanometers) to blue whales (up to 30 meters), the heaviest ones being limited by heat\\ndissipation due to their high volume-to-surface ratio [23]. At the cosmic scale, all terrestrial life\\nappears microscopic, suggesting that absolute size is less critical than functional adaptability.\\nHereby: Humans, often regarded as a pinnacle of intelligence, offer a useful anchor for defining SLMs\\nand LLMs. With a brain-to-body mass ratio surpassed only by small mammals like mice [45], humans\\nbalance computational efficiency with practical embodiment. SLMs, by analogy, are systems compact\\nenough to run on personal devices, be trained with modest human interaction, or perform constrained,\\nverifiable tasks. LLMs, in contrast, demand datacenter-scale infrastructure, organization-level training,\\nand extensive validation, reflecting their computational load. The extremalist perspective hints at a\\nprofound truth: intelligence is not defined by size alone but by the balance of capability, efficiency,\\nand context. For agentic workflows, SLMs may offer agility and accessibility, while LLMs provide\\ndepth at the cost of scale.\\nIt is because of this apparent continuum that, if pressed to provide a definition of SLMs, we choose to\\nanchor it in characteristics of a model that can be deployed in a distributed fashion with present-day\\ntechnology and be interactive enough when engaging with a human to be of utility. Proceeding in\\nsuch a way, the contemporary instances of the definition will evolve as the technology underpinning\\nthese models advances, making the definition sufficiently timeless to be practical.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 15}, page_content='B\\nLLM-to-SLM Replacement Case Studies\\nThis appendix assesses the potential extent of replacing large language model invocations with small\\nlanguage models in three popular open-source agents: MetaGPT, Open Operator, and Cradle. Each\\ncase study examines the use of LLMs, evaluates where SLMs may be viable replacements, and\\nconcludes with an estimated percentage of replaceable queries.\\nB.1\\nCase study 1: MetaGPT\\nName.\\nMetaGPT\\nLicense.\\nApache 2.0\\nPurpose.\\nMetaGPT is a multi-agent framework designed to emulate a software company. It assigns\\nroles such as Product Manager, Architect, Engineer, and QA Engineer to collaboratively handle tasks\\nincluding requirement drafting, system design, implementation, and testing.\\nLLM Invocations.\\n• Role-Based Actions. Each agent role invokes LLMs to fulfill its specialized responsibilities\\n(e.g., coding, documentation).\\n• Prompt Templates. Structured prompts used for consistent outputs.\\n• Dynamic Intelligence. Used for planning, reasoning, and adaptation.\\n• Retrieval-Augmented Generation (RAG). Retrieves relevant documents to enhance genera-\\ntion.\\nAssessment for SLM Replacement.\\nSLMs would be well-suited for routine code generation\\nand boilerplate tasks, as well as for producing structured responses based on predefined templates.\\nHowever, they would require further fine-tuning data to reliably perform more complex tasks, such as\\narchitectural reasoning and adaptive planning or debugging, which would initially benefit from the\\nbroader contextual understanding and the generality of LLMs.\\nConclusion.\\nIn the case of MetaGPT, we estimate that about 60% of its LLM queries could be\\nreliably handled by appropriately specialized SLMs.\\nB.2\\nCase study 2: Open Operator\\nName.\\nOpen Operator\\nLicense.\\nMIT License\\nPurpose.\\nOpen Operator is a workflow automation agent enabling users to define behaviours of\\nagents that can perform tasks like API calls, monitoring, and orchestration using tools and services.\\nLLM Invocations\\n• Natural Language Processing. Parses user intent.\\n• Decision Making. Guides execution flow.\\n• Content Generation. Writes summaries, reports.\\nAssessment for SLM Replacement\\nSLMs would be well-suited for tasks such as simple command\\nparsing and routing, as well as generating messages based on predefined templates. They could\\nbe meeting their limitations when dealing with more complex tasks that would require multi-step\\nreasoning or the ability to maintain conversation flow and context over time—areas where LLMs\\nwould continue to offer significant advantages.\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-14T11:58:21+05:30', 'source': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1758078134582-sml_for_agentic_ai.pdf', 'total_pages': 17, 'format': 'PDF 1.6', 'title': 'Small Language Models are the Future of Agentic AI', 'author': 'Peter Belcak', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:01:22+05:30', 'trapped': '', 'modDate': \"D:20250915100122+05'30'\", 'creationDate': \"D:20250914115821+05'30'\", 'page': 16}, page_content='Conclusion.\\nIn the case of Open Operator, we estimate that about 40% of its LLM queries could be\\nreliably handled by appropriately specialized SLMs.\\nB.3\\nCase study 3: Cradle\\nName.\\nCradle\\nLicense.\\nMIT License\\nPurpose\\nCradle is designed for General Computer Control (GCC), enabling agents to operate GUI\\napplications via screenshot input and simulated user interaction.\\nLLM Invocations.\\n• Interface Interpretation. Understands visual context.\\n• Task Execution Planning. Determines sequences of GUI actions.\\n• Error Handling. Diagnoses and reacts to unexpected software states.\\nAssessment for SLM Replacement\\nSLMs would be well-suited for handling repetitive GUI\\ninteraction workflows and the execution of pre-learned click sequences. However, they would face\\nchallenges when it comes to tasks involving dynamic GUI adaptation or unstructured error resolution,\\nwhich would require a higher degree of contextual understanding typically provided by LLMs.\\nConclusion\\nIn the case of Cradle, we estimate that about 70% of its LLM queries could be reliably\\nhandled by appropriately specialized SLMs.\\n17'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 0}, page_content='LEAF-Net: A Uni¬ed Framework for Leaf Extraction\\nand Analysis in Multi-Crop Phenotyping Using\\nYOLOv11\\nAmeer Tamoor Khan\\xa0\\nUniversity of Copenhagen https://orcid.org/0000-0001-6838-992X\\nSigne Marie Jensen\\xa0\\nUniversity of Copenhagen https://orcid.org/0000-0002-7738-5231\\nResearch Article\\nKeywords: Plant Phenotyping, YOLOv11, Leaf Segmentation, Precision Agriculture, Deep Learning\\nPosted Date: December 6th, 2024\\nDOI: https://doi.org/10.21203/rs.3.rs-5582314/v1\\nLicense: \\uf25e \\uf4e7 This work is licensed under a Creative Commons Attribution 4.0 International License. \\xa0\\nRead Full License\\nAdditional Declarations: The authors declare no competing interests.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 1}, page_content='1\\nLEAF-Net: A Uniﬁed Framework for Leaf\\nExtraction and Analysis in Multi-Crop Phenotyping\\nUsing YOLOv11\\nAmeer Tamoor Khan, Signe Marie Jensen\\nAbstract—Accurate leaf segmentation and counting are crit-\\nical for advancing crop phenotyping and improving breeding\\nprograms in agriculture. This study evaluates YOLOv11-based\\nmodels for automated leaf detection and segmentation across\\nspring barley, spring wheat, winter wheat, winter rye, and winter\\ntriticale. The key focus is assessing whether a uniﬁed model\\ntrained on a combined multi-crop dataset can outperform crop-\\nspeciﬁc models. Results show that the uniﬁed model achieves\\nsuperior performance in bounding box tasks, with mAP@50\\nexceeding 0.85 for spring crops and above 0.7 for winter crops.\\nSegmentation tasks, however, reveal mixed results, with individ-\\nual models occasionally excelling in recall for winter crops. These\\nﬁndings highlight the beneﬁts of dataset diversity in improving\\ngeneralization, while emphasizing the need for larger annotated\\ndatasets to address variability in real-world conditions. This high-\\nlights that while the combined dataset improves generalization,\\nthe unique characteristics of individual crops may still beneﬁt\\nfrom specialized training. This work demonstrates the potential\\nof AI-driven models to advance automated phenotyping for large-\\nscale precision agriculture.\\nIndex Terms—Plant Phenotyping, YOLOv11, Leaf Segmenta-\\ntion, Precision Agriculture, Deep Learning.\\nI. INTRODUCTION\\nIn modern agriculture, precise plant phenotyping is essential\\nfor advancing crop breeding programs, optimizing agricultural\\npractices, and addressing the global challenge of food security\\n[1]. With the growing demand for food driven by an expanding\\npopulation, alongside challenges posed by climate change\\nand limited arable land, the development of efﬁcient and\\naccurate methods for analyzing plant traits has become a crit-\\nical priority [2]. Among various traits, accurate segmentation\\nand identiﬁcation of leaves during the early stages of crop\\ndevelopment are particularly important for understanding crop\\nhealth, growth dynamics, and development across different\\ncrop varieties. However, automated analysis of these traits re-\\nmains challenging due to occlusion, environmental variability,\\nand the complex morphology of crop plants.\\nTraditional plant phenotyping methods rely heavily on man-\\nual observations and measurements [3]. While these methods\\nhave served as the foundation of crop breeding for decades,\\nthey are inherently time-consuming, labor-intensive, and prone\\nto human error. These limitations make them unsuitable for\\nlarge-scale breeding programs, where evaluating numerous\\ntraits across extensive populations is required. Additionally,\\nthe increasing adoption of precision agriculture underscores\\nthe need for high-throughput and non-destructive phenotyping\\ntechniques that enhance the efﬁciency and sustainability of\\nmodern farming systems [4].\\nEarly-stage leaf identiﬁcation and counting play a crucial\\nrole in monitoring crop health, understanding growth patterns,\\nand evaluating early-stage crop maturity [5]. Accurate iden-\\ntiﬁcation of leaves provides valuable insights for breeders,\\nenabling a better understanding of growth dynamics and\\nsupporting the selection of superior crop varieties. Moreover,\\nsegmentation is critical for estimating the Leaf Area Index\\n(LAI), a key metric for understanding crop growth and canopy\\nstructure [6]. LAI estimation plays a vital role in predicting\\nphotosynthetic efﬁciency, yield potential, and resource allo-\\ncation in plants. However, manual counting is impractical for\\nlarge-scale studies under diverse ﬁeld conditions, necessitating\\nautomated solutions enabled by advancements in artiﬁcial\\nintelligence (AI) and computer vision [7].\\nClassical approaches to leaf counting have primarily relied\\non manual or semi-automated methods. These techniques often\\ninvolve human experts counting leaves in the ﬁeld [4]. Image\\nprocessing methods, such as edge detection, thresholding, and\\nregion-based segmentation, have also been applied to digital\\nimages of crops [8]. While these methods show promise in\\ncontrolled environments, their effectiveness diminishes under\\nreal-world conditions where variability in lighting, occlusion,\\nand plant morphology present signiﬁcant challenges. Further-\\nmore, many classical methods are tailored to speciﬁc crop\\ntypes, limiting their applicability across multiple species [9].\\nTo the best of our knowledge, no prior work has comprehen-\\nsively addressed early-stage leaf segmentation and counting\\nacross multiple crop types—including spring barley, spring\\nwheat, winter wheat, winter rye, and winter triticale—under\\ndiverse ﬁeld conditions, highlighting a gap in the literature.\\nRecent advancements in AI, particularly in deep learning\\nand computer vision, have transformed plant phenotyping by\\nenabling automated analysis [10, 11]. State-of-the-art models\\nsuch as convolutional neural networks (CNNs) and object\\ndetection frameworks have demonstrated remarkable success\\nin tasks such as segmentation of crop leaves, early-stage trait\\nanalysis, and biomass estimation [7, 12, 13]. Additionally,\\nhigh-throughput phenotyping platforms using unmanned aerial\\nvehicles (UAVs) and robotic systems equipped with advanced\\nimaging sensors have made large-scale data collection feasible\\n[14].\\nDespite these advancements, existing AI-driven methods\\noften face signiﬁcant limitations when applied to diverse crop\\ntypes and real-world ﬁeld conditions. Challenges such as\\nvariable lighting, occlusion from overlapping plant structures,\\nand the inherent diversity of crop morphologies pose barriers'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 2}, page_content='2\\nto robust early-stage leaf detection and segmentation [7].\\nThe present study seeks to evaluate whether distinct models\\nare necessary for leaf segmentation and identiﬁcation across\\ndifferent crop types at the early growth stage or whether a uni-\\nﬁed model, trained on a dataset encompassing multiple crops,\\ncan achieve superior performance. The rationale behind the\\nlatter approach lies in the hypothesis that shared morphological\\nfeatures among crops may enable the model to generalize\\neffectively, thereby enhancing its robustness and accuracy.\\nThis investigation critically examines the adaptability and\\ntransferability of AI models in agricultural applications, with a\\nspeciﬁc focus on early-stage crop monitoring and leaf analysis,\\naiming to advance automated phenotyping methodologies.\\nThe remainder of this paper is organized as follows: Section\\nII discusses the dataset used in this study, including data\\ncollection and annotation processes. Section III outlines the\\nmethodology employed for leaf segmentation and counting.\\nSection IV describes the training and validation of the models.\\nSection V presents the analysis and results. Section VI pro-\\nvides a discussion of the ﬁndings, and Section VII concludes\\nthe paper with ﬁnal remarks and future directions.\\nII. DATASET OVERVIEW\\nThe dataset used in this study was collected from ﬁve\\nagricultural ﬁelds at the University of Copenhagen. It included\\nimages from ﬁve different crops: spring barley, spring wheat,\\nwinter wheat, winter rye, and winter triticale. The primary\\nobjective of this dataset was to support leaf segmentation\\ntasks, with leaves manually annotated in green for visibility,\\nas shown in Fig. 1. Sample images from the dataset illustrated\\nthe diversity in crop types and imaging conditions. The manual\\nannotations focused on leaves, which were often challenging\\nto distinguish in raw images, emphasizing the dataset’s im-\\nportance in segmentation tasks. These annotations provided\\nessential ground truth data for developing and evaluating\\nsegmentation models.\\nThe dataset was acquired using two distinct platforms:\\nspring barley and spring wheat images were collected using\\nan agricultural robot, which had an overall height of approx-\\nimately 2.15 meters, a ground clearance of 80 cm, and an\\nimage resolution of 2448 × 2048. winter wheat, winter rye,\\nand winter triticale images were captured using a drone ﬂying\\nat an altitude of 8 meters with an image resolution of 5280 ×\\n395. The image sources and heights are detailed in Fig. 1.\\nThe dataset consisted of 907 training and 112 testing images\\nacross all crop types, with a notable imbalance in distribution.\\nSpeciﬁcally, spring barley had 419 training images and 43\\ntesting images, while spring wheat had 410 training images\\nand 38 testing images. The winter crops—winter wheat, winter\\nrye, and winter triticale—were evenly distributed, with each\\nhaving 25 training images and 3 testing images, as shown in\\nFig. 2.\\nIn addition to the image distribution, the number of an-\\nnotated leaves in the dataset was a critical aspect. Fig. 3\\nshows the leaf count for each crop type in the training and\\ntesting datasets. The spring barley training set contained 4,987\\nleaves, with 646 in the testing set. Similarly, spring wheat had\\nSpring Barley (Robot Images)\\nSpring Wheat (Robot Images)\\nWinter Wheat (8m Drone Images)\\nWinter Rye (8m Drone Images)\\nWinter Triticale (8m Drone Images)\\nFig. 1. Sample images from the spring and winter crops dataset, displaying\\nannotated leaves highlighted in green, which were otherwise challenging to\\ndistinguish in raw images. spring barley and spring wheat images were cap-\\ntured using an agricultural robot (overall height approximately 2.15 meters),\\nwhile winter wheat, winter rye, and winter triticale images were collected\\nusing a drone ﬂying at an altitude of 8 meters.\\nSpring Barley\\nSpring Wheat\\nWinter Wheat\\nWinter Rye\\nWinter Triticale\\n0.0\\n100.0\\n200.0\\n300.0\\n400.0\\n500.0\\nDataset Count\\n419\\n410\\n25\\n25\\n25\\n43\\n38\\n3\\n3\\n3\\nSpring and Winter Crops Dataset Distribution\\nT\\nrain Data\\nT\\nest Data\\nFig. 2. Spring and winter Crops Dataset Distribution illustrating the count of\\nimages in the training and testing datasets for each crop type.\\nSpring Barley\\nSpring Wheat\\nWinter Wheat\\nWinter Rye\\nWinter Triticale\\n0.0\\n1000.0\\n2000.0\\n3000.0\\n4000.0\\n5000.0\\n6000.0\\n7000.0\\n8000.0\\nSepal Count\\n4987\\n6990\\n2721\\n3309\\n2730\\n646\\n832\\n506\\n229\\n564\\nSpring and Winter Crops Sepal Distribution\\nT\\nrain Data\\nT\\nest Data\\nFig. 3.\\nSpring and winter Crops Leaf Distribution showing the count of\\nmanually annotated leaves across the training and testing datasets for each\\ncrop type.\\n6,990 leaves in the training set and 832 in the testing set. The\\nwinter crops had considerably fewer leaves, with winter wheat\\nhaving 2,721 leaves in training and 506 in testing, winter rye\\nhaving 3,309 in training and 229 in testing, and winter triticale\\ncontaining 2,730 in training and 564 in testing. These counts\\ndemonstrated that the training datasets were signiﬁcantly more\\ncomprehensive than the testing datasets, ensuring robust model'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 3}, page_content='3\\n−15\\n−10\\n−5\\n0\\n5\\nx\\n1\\n−6\\n−4\\n−2\\n0\\n2\\n4\\n6\\n8\\n10\\nx\\n2\\nPCA of Spring and Winter Crops (ResNet50)\\nSpring Barley\\nSpring Wheat\\nWinter Wheat\\nWinter Rye\\nWinter Triticale\\nFig. 4.\\nPCA of spring and winter crops visualized using features from a\\npretrained ResNet50 model. The clustering reﬂected height-based differences,\\nwith robot images capturing larger leaf sizes due to proximity to the ground\\nand controlled conditions, while drone images showed smaller leaf sizes\\ninﬂuenced by natural, uncontrolled environmental factors.\\ndevelopment while still providing sufﬁcient data for evalua-\\ntion.\\nTo analyze the differences between images captured by the\\nrobot and the drone, Principal Component Analysis (PCA) was\\nperformed using features extracted from a pretrained ResNet50\\nmodel. The PCA results, shown in Fig. 4, demonstrated\\ndistinct clustering based on the imaging source. This difference\\nis attributed to variations in leaf size and imaging conditions.\\nRobot-captured images, taken closer to the ground, provide\\nlarger and more detailed leaf representations under controlled\\nconditions, avoiding real-world disturbances such as wind or\\nlighting variations. In contrast, drone-captured images, taken\\nfrom a higher altitude under natural ﬁeld conditions, are\\naffected by environmental factors, leading to smaller and less\\nuniform leaf representations.\\nOverall, the spring and winter crops dataset was a valuable\\nresource for segmentation tasks, particularly for leaf identiﬁ-\\ncation in diverse crop types. The dataset’s variety in imaging\\nconditions—spanning robot-collected and drone-captured im-\\nages—added to its robustness. While the imbalance between\\nspring and winter crop samples was evident, the dataset’s\\ncarefully annotated leaves and comprehensive training sets\\nprovided a solid foundation for developing high-performance\\nsegmentation models. This dataset was well-suited for appli-\\ncations in precision agriculture, enabling researchers to build\\nsolutions for automated crop analysis and monitoring.\\nIII. ARCHITECTURE OF YOLOV11\\nThe YOLOv11 architecture introduces several critical en-\\nhancements to improve efﬁciency and detection accuracy, as\\nillustrated in Fig. 5. The design comprises three primary\\ncomponents: Backbone, Neck, and Head, each incorporating\\ninnovative mechanisms to optimize performance.\\n1) Backbone: The Backbone is the foundation for multi-\\nscale feature extraction, leveraging advancements to ensure\\ncomputational efﬁciency:\\n• C3k2 Block: This Cross-Stage Partial (CSP) bottleneck\\nemploys a kernel size of 2, replacing older, more com-\\nputationally intensive blocks. The design reduces the\\nmodel’s overall complexity while retaining its ability to\\ncapture essential features.\\n• SPPF (Spatial Pyramid Pooling Fast): Adapted from\\nYOLOv8, this block consolidates features across multiple\\nreceptive ﬁelds, enhancing the representation of objects\\nat varying scales.\\n• C2PSA (Cross-Stage Partial with Spatial Attention):\\nThis novel addition integrates spatial attention, allowing\\nthe model to focus on critical regions within the input\\nimage. It is particularly effective for detecting small\\nor occluded objects, which are common challenges in\\nagricultural datasets.\\n2) Neck: The Neck serves as a bridge, reﬁning features\\nextracted by the Backbone and preparing them for prediction.\\nEnhancements include:\\n• C3k2 Block: These blocks continue to process features\\nefﬁciently, maintaining high throughput while reducing\\nthe computational cost.\\n• C2PSA Mechanism: By reﬁning feature maps through\\nspatial attention, the Neck enhances the model’s ability\\nto distinguish between relevant and irrelevant features.\\n3) Head: The Head outputs the ﬁnal predictions, including\\nobject bounding boxes, masks, and class probabilities:\\n• C3k2 Blocks: These blocks enhance the processing\\nof multi-scale features, ensuring ﬁne-grained detection\\nacross various object sizes.\\n• CBS (Convolution-BatchNorm-SiLU): This combina-\\ntion improves feature normalization and training stability,\\nenhancing the overall performance of the network.\\n• Final Detection Layer: Outputs Masks, bounding box\\ncoordinates, objectness scores, and class probabilities\\nwith a focus on efﬁciency and accuracy, critical for real-\\ntime applications.\\nA. Applications in Agriculture\\nThe architectural enhancements in YOLOv11, as depicted\\nin Fig. 5, make it particularly well-suited for agricultural\\napplications. Its ability to detect small or occluded objects\\nis advantageous in tasks such as:\\n• Crop Monitoring [15]: Identifying and counting crop-\\nspeciﬁc traits, such as leaves, ﬂowers, or fruit, to estimate\\nyield.\\n• Weed Detection [16]: Differentiating between crops and\\nweeds for targeted herbicide application.\\n• Pest and Disease Identiﬁcation [17]: Detecting early\\nsigns of pests or diseases to mitigate potential yield\\nlosses.\\n• Automated Harvesting [18]: Recognizing ripe crops or\\nfruits for efﬁcient harvesting.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 4}, page_content='4\\nConv \\n320x320 (min|64,mc|xw)\\nConv \\n160x160 (min|128,mc|xw)\\nC3K2 \\nShortcut=False n=3xd\\nConv \\n80x80 (min|256,mc|xw)\\nC3K2 \\nShortcut=False n=6xd\\nConv \\n40x40 (min|512,mc|xw)\\nC3K2 \\nShortcut=True n=6xd\\nConv \\n20x20 (min|1024,mc|xw)\\nC3K2 \\nShortcut=True n=3xd\\nInput\\n640x640\\nC3K2 \\nShortcut=False n=3xd\\nC3K2 \\nShortcut=False n=3xd\\nUpsample\\n20x20 (min|1024,mc|xw)\\nSPFF\\nC2PSA\\nConcat\\n40x40 (min|512,mc|xw)\\nConcat\\n80x80 (min|256,mc|xw)\\nUpsample\\n40x40 (min|512,mc|xw)\\nConv \\n80x80 (min|256,mc|xw)\\nConcat\\n40x40 (min|512,mc|xw)\\nC3K2 \\nShortcut=False n=3xd\\nConv \\n20x20 (min|1024,mc|xw)\\nConcat\\n20x20 (min|1024,mc|xw)\\nC3K2 \\nShortcut=False n=3xd\\nMask\\nMask\\nMask\\nBackbone\\nNeck\\nHead\\nFig. 5. The schematic diagram of YOLOv11 illustrating its three core components: Backbone, Neck, and Head. The Backbone handles multi-scale feature\\nextraction using advanced blocks such as the C3k2 and Spatial Pyramid Pooling Fast (SPPF), designed for efﬁcient feature representation. The Neck aggregates\\nand reﬁnes these features with additional mechanisms like Cross-Stage Partial with Spatial Attention (C2PSA). Finally, the Head predicts object bounding\\nboxes, masks, and classiﬁcations using enhanced multi-scale processing.\\nThe modular design of YOLOv11 ensures ﬂexibility, en-\\nabling adaptation to various agricultural use cases. The inte-\\ngration of spatial attention mechanisms and multi-scale feature\\nprocessing improves its accuracy in diverse ﬁeld conditions,\\nfrom densely planted crops to heterogeneous landscapes.\\nIV. TRAINING AND VALIDATION SETTINGS\\nA. Training Procedure\\nThe YOLOv11 architecture was employed to perform leaf\\ndetection and counting across ﬁve crop types: spring barley,\\nspring wheat, winter wheat, winter rye, and winter triticale.\\nThe dataset was manually annotated, with sepals delineated\\nto provide ground truth data. Images were split into training\\nand testing subsets, ensuring representative distributions for\\nall crop types.\\nTo evaluate the impact of training strategies, two models\\nwere trained for each crop. In the ﬁrst approach, datasets\\nfrom all ﬁve crops were combined to train a single model\\nfor the speciﬁc crop, leveraging the combined data to improve\\ngeneralization. In the second approach, only the dataset of the\\nspeciﬁc crop under evaluation was used for training, focusing\\nthe model on crop-speciﬁc features. This dual strategy allowed\\nfor a comparative analysis of performance between generalized\\nand crop-speciﬁc training, providing insights into the beneﬁts\\nand limitations of both approaches under real-world condi-\\ntions.\\n1) Data Augmentation: To improve model generalization,\\nseveral data augmentation techniques were applied. These\\nincluded random rotations, horizontal and vertical ﬂips, ran-\\ndom cropping, and adjustments to brightness and contrast.\\nAdditionally, advanced augmentations such as Gaussian blur,\\nmedian blur, grayscale transformations, and CLAHE were\\nutilized to simulate diverse real-world imaging conditions.\\n2) Training Conﬁguration: The model was trained using\\nthe Adam optimizer with an initial learning rate of 0.0001,\\nmanaged via a cosine decay schedule. A batch size of 8 was\\nused to optimize computational efﬁciency given the hardware\\nconstraints. Early stopping was employed with a patience\\nvalue of 10 epochs, ensuring the prevention of overﬁtting.\\nTraining hyperparameters such as momentum, weight decay,\\nand warm-up strategies were ﬁne-tuned for optimal perfor-\\nmance, as outlined in TABLE I.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 5}, page_content='5\\nHardware Speciﬁcations\\nCPU\\nGoogle Colab\\nGPU\\nNVIDIA T4\\nRAM\\n40 GB\\nModel Speciﬁcations (YOLOv11)\\nOptimizer\\nAdam\\nBatch Size\\n8\\nLearning Rate\\n0.0001 (Cosine LR Decay)\\nEpochs\\n50\\nPatience\\n100 (Early Stopping)\\nNMS\\nFalse\\nLR Decay Rate\\n0.01\\nMomentum\\n0.937\\nWeight Decay\\n0.0005\\nWarmup Epochs\\n3.0\\nWarmup Momentum\\n0.8\\nWarmup Bias LR\\n0.1\\nBox Loss Coeff.\\n7.5\\nClassiﬁcation Loss Coeff.\\n0.5\\nDFL Loss Coeff.\\n1.5\\nPose Loss Coeff.\\n12.0\\nK-Object Loss Coeff.\\n1.0\\nLabel Smoothing\\n0.0\\nData Speciﬁcations\\nSpring Barley Train\\n419 Images\\nSpring Barley Test\\n43 Images\\nSpring Wheat Train\\n410 Images\\nSpring Wheat Test\\n38 Images\\nWinter Wheat Train\\n25 Images\\nWinter Wheat Test\\n3 Images\\nWinter Rye Train\\n25 Images\\nWinter Rye Test\\n3 Images\\nWinter Triticale Train\\n25 Images\\nWinter Triticale Test\\n3 Images\\nImage Size\\n640x640 Pixels\\nData Augmentation\\nBlur\\np=0.01, blur limit=(3, 7)\\nMedian Blur\\np=0.01, blur limit=(3, 7)\\nTo Gray\\np=0.01\\nCLAHE\\np=0.01, clip limit=(1, 4.0),\\ntile grid size=(8, 8)\\nTABLE I\\nYOLOV11 TRAINING HYPERPARAMETERS AND DATASET\\nSPECIFICATIONS\\n3) Hardware Speciﬁcations: The training experiments were\\nperformed on a Google Colab environment equipped with an\\nNVIDIA T4 GPU and 40 GB of RAM. This setup enabled\\nefﬁcient processing of high-resolution 640x640 pixel images\\nand accelerated training cycles.\\nB. Validation and Evaluation\\nThe performance of YOLOv11 models was evaluated on the\\nvalidation set of each crop type. Evaluation metrics primarily\\nfocused on:\\n• Detection Accuracy: The ability to correctly identify and\\nlocalize sepals within images.\\n• Loss Metrics: Training and validation loss trends were\\nmonitored throughout the training process to evaluate\\nmodel convergence and generalization.\\nV. ANALYSIS AND RESULTS\\nThe performance of YOLOv11-based models was evaluated\\nusing validation datasets for each crop type, focusing on\\nbounding box (BBox) and segmentation metrics. These evalu-\\nations assessed the models’ ability to detect and count leaves\\naccurately across spring barley, spring wheat, winter wheat,\\nwinter rye, and winter triticale. The results are summarized\\nbelow, with detailed analysis and visualizations provided in\\nFig. 6–9 and TABLE II.\\nA. Loss and Convergence Analysis\\nFig. 6 illustrates the training and validation loss curves for\\nBBox and segmentation tasks across all crop types. Models\\ntrained on the combined dataset consistently exhibited lower\\nloss values during training and validation, particularly for\\nspring crops. This reﬂects the ability of the combined dataset\\nto provide more diverse and representative features, facilitating\\nbetter generalization. In contrast, models trained on individual\\ndatasets had higher loss values, especially for winter crops, un-\\nderscoring the challenges posed by limited data and increased\\nvariability.\\nB. Precision and Recall Analysis\\nFig. 7 presents the precision and recall curves for BBox and\\nsegmentation tasks. Models trained on the combined dataset\\nconsistently achieved higher precision and recall values for\\nboth spring and winter crops, with a more noticeable advantage\\nfor bounding box tasks. For instance, spring barley and spring\\nwheat achieved over 0.9 precision and recall for BBox tasks\\nwhen using the combined dataset. However, for segmentation\\ntasks, the difference between combined and individual datasets\\nwas smaller, particularly for winter crops, where the individual\\ndatasets occasionally outperformed the combined dataset in\\nrecall. This indicates that while the combined dataset enhances\\ngeneralization, speciﬁc nuances of individual crops may still\\nrequire specialized training.\\nC. Mean Average Precision (mAP) Analysis\\nThe mAP metrics for BBox and segmentation tasks, as\\nshown in Fig. 8, further highlight the superior performance of\\nthe combined dataset. For spring crops, the mAP@50 values\\nfor both BBox and segmentation exceeded 0.85, with segmen-\\ntation metrics slightly trailing BBox metrics. For winter crops,\\nthe combined dataset still outperformed individual datasets in\\nBBox mAP, achieving values above 0.7, while segmentation\\nmAP remained lower due to the inherent challenges of fewer\\nsamples and more complex imaging conditions. The combined\\ndataset’s higher mAP values indicate its ability to leverage\\ncross-crop similarities for robust detection.\\nD. Performance Metrics Overview\\nThe quantitative metrics summarized in TABLE II under-\\nscore the combined dataset’s advantage across most tasks.\\nFor instance, spring barley achieved a BBox mAP@50 of\\n0.881 and a segmentation mAP@50 of 0.822 using the com-\\nbined dataset, compared to 0.755 and 0.604, respectively,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 6}, page_content='6\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain BBox Loss\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain BBox Loss\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain BBox Loss\\n(a)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain BBox Loss\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain BBox Loss\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain Segm Loss\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain Segm Loss\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain Segm Loss\\n(b)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain Segm Loss\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nTrain Segm Loss\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid BBox Loss\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid BBox Loss\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid BBox Loss\\n(c)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid BBox Loss\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid BBox Loss\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid Segm Loss\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid Segm Loss\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid Segm Loss\\n(d)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid Segm Loss\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n25\\n50\\n75\\n100\\nEpochs\\n0\\n2\\n4\\n6\\nV\\nalid Segm Loss\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\nFig. 6. Training and validation losses for spring barley, spring wheat, winter wheat, winter rye, and winter triticale across combined and individual datasets,\\nshowing box and segmentation losses.\\nfor the individual dataset. Similarly, for winter crops like\\nwinter triticale, the combined dataset achieved a segmentation\\nmAP@50 of 0.591, outperforming the individual dataset’s\\n0.405. These results emphasize that a larger and more diverse\\ndataset signiﬁcantly enhanced model performance, particularly\\nfor bounding box tasks, while segmentation tasks remained\\nmore sensitive to dataset variability.\\nE. Detection Insights from Visual Outputs\\nFig. 9 illustrates the visual results for all crop types. Models\\ntrained on the combined dataset provided denser and more\\naccurate detections, particularly for spring crops, with minimal\\nfalse positives or missed detections. For winter crops, while\\ndetections were generally less dense, the combined dataset still\\nproduced more reliable outputs in challenging regions with\\nocclusion or complex backgrounds. The visual outputs align\\nwith the quantitative metrics, highlighting the effectiveness of\\nleveraging diverse datasets for robust performance.\\nF. Discussion of Combined vs. Individual Training\\nOverall, the combined dataset demonstrated superior per-\\nformance across most metrics, particularly for bounding box\\ntasks, due to the larger and more diverse training pool.\\nThis generalization ability is crucial for practical applications\\nwhere crop-speciﬁc annotations may be limited. However, for\\nsegmentation tasks, individual datasets occasionally provided\\nbetter recall, indicating that crop-speciﬁc features might still\\nbeneﬁt from targeted training. This trade-off highlights the\\nimportance of balancing dataset diversity with crop-speciﬁc\\nﬁne-tuning to optimize performance across all phenotyping\\ntasks.\\nVI. DISCUSSION\\nThe application of YOLOv11 for leaf detection and count-\\ning across diverse crop types highlights the transformative\\npotential of advanced deep learning methods in automated\\nplant phenotyping. By leveraging architectural innovations\\nand real-time processing capabilities, YOLOv11 has shown'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 7}, page_content='7\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Precision\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Precision\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Precision\\n(a)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Precision\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Precision\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Recall\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Recall\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Recall\\n(b)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Recall\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox Recall\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Precision\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Precision\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Precision\\n(c)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Precision\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Precision\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Recall\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Recall\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Recall\\n(d)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Recall\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm Recall\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\nFig. 7. Precision and Recall for spring barley, spring wheat, winter wheat, winter rye, and winter triticale across combined and individual datasets, showing\\nbox and segmentation precision and recall.\\npromise in addressing the challenges of large-scale agricultural\\nassessments, where manual methods are both labor-intensive\\nand error-prone. The model’s ability to detect and quantify\\nleaves under varying environmental and imaging conditions\\nunderscores its versatility and practical utility.\\nA key strength of YOLOv11 lies in its architectural en-\\nhancements, such as the introduction of the C3k2 block and\\nthe C2PSA module. These components have substantially\\nimproved feature extraction and spatial attention, enabling\\nthe model to effectively handle complex leaf morphologies\\nand varying background conditions. The integration of such\\nmechanisms has set YOLOv11 apart from its predecessors,\\nallowing it to adapt to different imaging scenarios, including\\nhigh-resolution robot images and drone-captured data at higher\\naltitudes. Additionally, the real-time inference capabilities of\\nYOLOv11 make it well-suited for ﬁeld deployment in preci-\\nsion agriculture, where rapid decision-making is often critical.\\nHowever, certain challenges remain that highlight the need\\nfor further reﬁnement. Occlusion, caused by overlapping plant\\nstructures, continues to hinder detection performance, partic-\\nularly in densely populated or highly vegetative areas. The\\ninability to consistently separate overlapping leaves limits the\\nmodel’s ability to achieve perfect accuracy in such scenarios.\\nSimilarly, extreme lighting variations, ranging from overexpo-\\nsure in bright sunlight to underexposure in shaded regions, can\\nintroduce inconsistencies in detection. These factors, inherent\\nto real-world agricultural environments, represent areas where\\nthe model’s robustness can be improved.\\nThe variability in imaging conditions across crop types\\nfurther underscores the importance of dataset quality and\\ndiversity. While YOLOv11 performs well with larger, diverse\\ndatasets such as those for spring crops, smaller datasets for\\nwinter crops present a signiﬁcant challenge. Limited data\\navailability reduces the model’s ability to generalize effec-\\ntively, particularly when faced with complex soil textures,\\nsparse leaf distributions, or drone images captured at varying\\naltitudes. Addressing this imbalance is crucial for ensuring\\nconsistent performance across all crop types and conditions.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 8}, page_content='8\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50\\n(a)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50:95\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50:95\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50:95\\n(b)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50:95\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nBBox mAP@50:95\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50\\n(c)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50:95\\nSpring Barley\\nCombined Dataset\\nBarley Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50:95\\nSpring Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50:95\\n(d)\\nWinter Wheat\\nCombined Dataset\\nWheat Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50:95\\nWinter Rye\\nCombined Dataset\\nRye Dataset\\n0\\n50\\n100\\nEpochs\\n0.0\\n0.5\\n1.0\\n1.5\\nSegm mAP@50:95\\nWinter Triticale\\nCombined Dataset\\nTriticale Dataset\\nFig. 8. Mean Average Precision (mAP) for spring barley, spring wheat, winter wheat, winter rye, and winter triticale across combined and individual datasets,\\nshowing box and segmentation @mAP50 and @mAP50:95.\\nOne of the most valuable applications of precise leaf\\nsegmentation is in the estimation of the Leaf Area Index\\n(LAI), a key parameter for assessing plant canopy structure\\nand health [6]. LAI estimation relies heavily on accurate\\ndetection and segmentation of leaves, particularly under dense\\ncanopy conditions, where overlapping leaves and occlusion\\npresent challenges. By providing high-resolution segmentation\\noutputs, YOLOv11 can facilitate the derivation of LAI met-\\nrics, which are instrumental for understanding photosynthetic\\nefﬁciency, water use, and light interception in crops. This\\nmakes it an indispensable tool in optimizing crop management\\nand modeling resource allocation under real-world agricultural\\nconditions.\\nOne signiﬁcant ﬁnding of this study is that the combined\\ndataset generally led to improved performance, particularly for\\nbounding box tasks, largely due to its larger size and diversity.\\nHowever, this improvement was not universal. For segmenta-\\ntion tasks, individual models occasionally outperformed the\\nuniﬁed model for certain crops, such as winter wheat and\\nwinter triticale. This suggests that while a larger dataset helps\\ngeneralization, speciﬁc nuances and challenges associated\\nwith particular crops, such as unique leaf morphologies or\\nenvironmental conditions, may require specialized training.\\nFor instance, winter crops often face greater occlusion and\\nvariability in natural ﬁeld conditions, which could limit the\\nbeneﬁts of a generalized model [19].\\nOne of the most critical areas for future research is\\nthe expansion of annotated datasets to enhance the model’s\\nperformance and generalization capabilities [20]. Increasing\\nthe volume and diversity of annotated data is paramount,\\nparticularly for crops with smaller datasets, such as winter\\nwheat, winter rye, and winter triticale. Larger datasets will\\nenable the model to better capture the inherent variability\\nin leaf morphology, growth stages, and environmental con-\\nditions, leading to more robust detection across all crop types.\\nFurthermore, increasing the diversity of annotated data by\\nincluding samples from multiple geographical regions and\\nagricultural practices will enhance the model’s adaptability. By'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 9}, page_content='9\\nCrop\\nDataset\\nImages\\nInstances\\nBox P\\nBox R\\nBox mAP50\\nBox mAP50:95\\nMask P\\nMask R\\nMask mAP50\\nMask mAP50:95\\nSpring Barley\\nCombined Dataset\\n43\\n645\\n0.902\\n0.795\\n0.881\\n0.607\\n0.862\\n0.755\\n0.822\\n0.355\\nSpring Barley Dataset\\n43\\n645\\n0.828\\n0.633\\n0.755\\n0.410\\n0.699\\n0.573\\n0.604\\n0.219\\nSpring Wheat\\nCombined Dataset\\n38\\n832\\n0.884\\n0.787\\n0.865\\n0.558\\n0.813\\n0.726\\n0.755\\n0.263\\nSpring Wheat Dataset\\n38\\n832\\n0.884\\n0.787\\n0.865\\n0.558\\n0.813\\n0.726\\n0.755\\n0.263\\nWinter Wheat\\nCombined Dataset\\n3\\n506\\n0.731\\n0.613\\n0.683\\n0.345\\n0.564\\n0.452\\n0.470\\n0.151\\nWinter Wheat Dataset\\n3\\n506\\n0.705\\n0.567\\n0.631\\n0.312\\n0.574\\n0.391\\n0.421\\n0.127\\nWinter Rye\\nCombined Dataset\\n3\\n229\\n0.816\\n0.764\\n0.825\\n0.478\\n0.685\\n0.655\\n0.699\\n0.228\\nWinter Rye Dataset\\n3\\n645\\n0.802\\n0.685\\n0.764\\n0.420\\n0.668\\n0.563\\n0.554\\n0.188\\nWinter Triticale\\nCombined Dataset\\n3\\n564\\n0.735\\n0.663\\n0.732\\n0.460\\n0.654\\n0.541\\n0.591\\n0.183\\nWinter Triticale Dataset\\n3\\n564\\n0.709\\n0.534\\n0.623\\n0.351\\n0.534\\n0.385\\n0.405\\n0.110\\nTABLE II\\nEVALUATION METRICS FOR SPRING AND WINTER CROPS ACROSS COMBINED AND INDIVIDUAL DATASETS\\nSpring Barley (Robot Image)\\n(a)\\nSpring Wheat (Robot Images)\\n(b)\\n(c)\\n(d)\\n(e)\\nWinter Wheat (8m Drone Images)\\nWinter Rye (8m Drone Images)\\nWinter Triticale (8m Drone Images)\\nFig. 9.\\nPrediction results for spring barley, spring wheat (Robot images),\\nand winter wheat, rye, and triticale (8m drone images), with detected crops\\nhighlighted in color-coded annotations.\\ncapturing the variability in crop appearances across different\\nclimates, soil types, and cultivation methods, the expanded\\ndataset will provide the model with a more comprehensive\\nunderstanding of leaf characteristics. The annotation process\\nshould focus on incorporating samples from diverse ﬁeld\\nconditions, including variations in lighting, soil textures, and\\nplant densities. Expanding the range of annotated images to\\ninclude more occluded and overlapping leaves will allow the\\nmodel to learn from challenging scenarios, thereby improving\\nits ability to handle real-world complexities. Additionally,\\nintegrating annotated images from different sensor modalities,\\nsuch as multispectral or hyperspectral cameras, could provide\\nricher information for both detection and segmentation tasks.\\nTo efﬁciently generate large-scale annotated datasets, semi-\\nautomated or automated annotation pipelines should be ex-\\nplored [21]. Leveraging pre-trained models to assist with initial\\nannotations and reﬁning these using human veriﬁcation can\\nsigniﬁcantly reduce the time and effort required for data\\npreparation. This approach would be particularly useful for\\ncrops with limited datasets, allowing researchers to scale up\\nannotations without excessive manual effort [22].\\nThe impact of dataset size and diversity cannot be over-\\nstated, as it directly inﬂuences the model’s ability to generalize\\nacross crop types and environmental conditions. However,\\nthe results also indicate that the effectiveness of a combined\\ndataset depends on the speciﬁc crop and task, highlighting the\\nneed for a balanced approach that combines the beneﬁts of\\ndiverse data with crop-speciﬁc ﬁne-tuning. As the annotated\\ndataset grows, the YOLOv11 models will become increasingly\\ncapable of handling the nuances of leaf detection and counting,\\npaving the way for more reliable and scalable applications in\\nprecision agriculture.\\nVII. CONCLUSION\\nThis study demonstrated the effectiveness of YOLOv11 for\\nautomated leaf detection and counting across spring and winter\\ncrops. The use of high-resolution robot images and drone-\\ncaptured data enabled robust evaluations, with high accuracy\\nobserved for spring crops. However, challenges such as occlu-\\nsion, lighting variability, and smaller dataset sizes impacted\\nperformance for winter crops. These ﬁndings highlight the\\nimportance of larger and more diverse annotated datasets\\nto improve model robustness and generalization. Expanding\\ndataset diversity and leveraging semi-automated annotation\\ntools can address these limitations. Overall, YOLOv11 offers a\\nscalable and efﬁcient solution for leaf detection and segmen-\\ntation, advancing automated plant phenotyping in precision\\nagriculture.\\nConﬂict of Interest\\nThe authors declare that they have no conﬂicts of interest\\nrelated to this project.\\nAcknowledgment\\nThis work was part of the project, Halm til det hele, ﬁnanced\\nby the Danish Promilleafgiftsfonden for Landbrug.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.52', 'creator': 'TeX', 'creationdate': \"D:20241206043856Z00'00'\", 'source': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\LEAF-Net_A_Unified_Framework_for_Leaf_Extraction_a.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': 'LEAF-Net: A Unified Framework for Leaf Extraction and Analysis in Multi-Crop Phenotyping Using YOLOv11', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20241206043856Z00'00'\", 'trapped': '', 'modDate': \"D:20241206043856Z00'00'\", 'creationDate': \"D:20241206043856Z00'00'\", 'page': 10}, page_content='10\\nREFERENCES\\n[1] R. T. Furbank and M. Tester, “Phenomics–technologies\\nto relieve the phenotyping bottleneck,” Trends in plant\\nscience, vol. 16, no. 12, pp. 635–644, 2011.\\n[2] M. Reynolds, O. K. Atkin, M. Bennett, M. Cooper, I. C.\\nDodd, M. J. Foulkes, C. Frohberg, G. Hammer, I. R.\\nHenderson, B. Huang, et al., “Addressing research bot-\\ntlenecks to crop productivity,” Trends in Plant Science,\\nvol. 26, no. 6, pp. 607–630, 2021.\\n[3] W. Yang, H. Feng, X. Zhang, J. Zhang, J. H. Doonan,\\nW. D. Batchelor, L. Xiong, and J. Yan, “Crop phenomics\\nand high-throughput phenotyping: past decades, current\\nchallenges, and future perspectives,” Molecular plant,\\nvol. 13, no. 2, pp. 187–214, 2020.\\n[4] A. Chawade, J. van Ham, H. Blomquist, O. Bagge,\\nE. Alexandersson, and R. Ortiz, “High-throughput ﬁeld-\\nphenotyping tools for plant breeding and precision agri-\\nculture,” Agronomy, vol. 9, no. 5, p. 258, 2019.\\n[5] L. P. Ronse De Craene, “Are petals sterile stamens or\\nbracts? the origin and evolution of petals in the core\\neudicots,” Annals of Botany, vol. 100, no. 3, pp. 621–\\n630, 2007.\\n[6] P. Das, P. Rahimzadeh-Bajgiran, W. Livingston, C. D.\\nMcIntire, and A. Bergdahl, “Modeling forest canopy\\nstructure and developing a stand health index using\\nsatellite remote sensing,” Ecological Informatics, vol. 84,\\np. 102864, 2024.\\n[7] A. Kamilaris and F. X. Prenafeta-Bold´u, “Deep learning\\nin agriculture: A survey,” Computers and electronics in\\nagriculture, vol. 147, pp. 70–90, 2018.\\n[8] J. C. Rose, S. Paulus, and H. Kuhlmann, “Accuracy\\nanalysis of a multi-view stereo approach for phenotyping\\nof tomato plants at the organ level,” Sensors, vol. 15,\\nno. 5, pp. 9651–9665, 2015.\\n[9] P. Velusamy, S. Rajendran, R. K. Mahendran, S. Naseer,\\nM. Shaﬁq, and J.-G. Choi, “Unmanned aerial vehicles\\n(uav) in precision agriculture: Applications and chal-\\nlenges,” Energies, vol. 15, no. 1, p. 217, 2021.\\n[10] A. Singh, S. Jones, B. Ganapathysubramanian, S. Sarkar,\\nD. Mueller, K. Sandhu, and K. Nagasubramanian, “Chal-\\nlenges and opportunities in machine-augmented plant\\nstress phenotyping,” Trends in Plant Science, vol. 26,\\nno. 1, pp. 53–69, 2021.\\n[11] L. C. Johannsen, A. T. Khan, S. M. Jensen, and\\nJ. Kruppa-Scheetz, “Innovative leaf disease mapping:\\nUnsupervised anomaly detection for precise area estima-\\ntion,” 2024.\\n[12] S. Madec, X. Jin, H. Lu, B. De Solan, S. Liu, F. Duyme,\\nE. Heritier, and F. Baret, “Ear density estimation from\\nhigh resolution rgb imagery using deep learning tech-\\nnique,” Agricultural and forest meteorology, vol. 264,\\npp. 225–234, 2019.\\n[13] A. T. Khan, S. M. Jensen, A. R. Khan, and S. Li, “Plant\\ndisease detection model for edge computing devices,”\\nFrontiers in Plant Science, vol. 14, p. 1308528, 2023.\\n[14] W. H. Maes and K. Steppe, “Perspectives for remote\\nsensing with unmanned aerial vehicles in precision agri-\\nculture,” Trends in plant science, vol. 24, no. 2, pp. 152–\\n164, 2019.\\n[15] R. d’Andrimont, M. Yordanov, L. Martinez-Sanchez, and\\nM. Van der Velde, “Monitoring crop phenology with\\nstreet-level imagery using computer vision,” Computers\\nand Electronics in Agriculture, vol. 196, p. 106866, 2022.\\n[16] L. C. M. Junior and J. A. C. Ulson, “Real time weed\\ndetection using computer vision and deep learning,” in\\n2021 14th IEEE International Conference on Industry\\nApplications (INDUSCON), pp. 1131–1137, IEEE, 2021.\\n[17] Y. Abbaspour-Gilandeh, A. Aghabara, M. Davari, and\\nJ. M. Maja, “Feasibility of using computer vision and\\nartiﬁcial intelligence techniques in detection of some\\napple pests and diseases,” Applied Sciences, vol. 12,\\nno. 2, p. 906, 2022.\\n[18] H. Tian, T. Wang, Y. Liu, X. Qiao, and Y. Li, “Computer\\nvision technology in agricultural automation—a review,”\\nInformation Processing in Agriculture, vol. 7, no. 1,\\npp. 1–19, 2020.\\n[19] E. Hamuda, B. Mc Ginley, M. Glavin, and E. Jones, “Au-\\ntomatic crop detection under ﬁeld conditions using the\\nhsv colour space and morphological operations,” Com-\\nputers and electronics in agriculture, vol. 133, pp. 97–\\n107, 2017.\\n[20] S. U. Amin, A. Hussain, B. Kim, and S. Seo, “Deep\\nlearning based active learning technique for data annota-\\ntion and improve the overall performance of classiﬁcation\\nmodels,” Expert Systems with Applications, vol. 228,\\np. 120391, 2023.\\n[21] N. Pangakis and S. Wolken, “Keeping humans in the\\nloop: Human-centered automated annotation with gener-\\native ai,” arXiv preprint arXiv:2409.09467, 2024.\\n[22] Y. Wang, D. Stevens, P. Shah, W. Jiang, M. Liu, X. Chen,\\nR. Kuo, N. Li, B. Gong, D. Lee, et al., “Model-in-the-\\nloop (milo): Accelerating multimodal ai data annotation\\nwith llms,” arXiv preprint arXiv:2409.10702, 2024.'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-08-11T16:44:15+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'total_pages': 7, 'format': 'PDF 1.5', 'title': '', 'author': 'PRANAVI PABBISETTY', 'subject': '', 'keywords': '', 'moddate': '2023-08-11T16:44:15+00:00', 'trapped': '', 'modDate': 'D:20230811164415Z', 'creationDate': \"D:20230811164415+00'00'\", 'page': 0}, page_content='Describing the Image using CNN and LSTM \\n \\n \\nDr Kuppusamy P \\nSchool of Computer Science \\nand Engineering \\nVIT-AP University \\nAmaravati, Andhra Pradesh \\ndrpkscse@gmail.com \\n \\nPabbisetty Pranavi \\nSchool of Computer Science \\nand Engineering \\nVIT-AP University \\nAmaravati, Andhra Pradesh \\n \\n \\nLingala Meghana \\nSchool of Computer Science \\nand Engineering \\nVIT-AP University \\nAmaravathi, Andhra \\nPradesh \\n \\nAnkalugari Rachana Varsha \\nSchool of Computer Science \\nand Engineering \\nVIT-AP University \\nAmaravathi, Andhra \\nPradesh\\n \\nAbstract: \\nImage Captioning in these days people use social \\nmedia more often where Image captioning plays a major \\nrole in generating captions for the images. From creating \\nmemes to generating information for the news articles \\ncaptioning the image place a significant role. In this paper \\nwe are going to build the project using the Convolutional \\nNeural Network models and Recurrent Neural Network \\nModels like LSTM which means Long Short Term \\nMemory. The optimizer adam is used to improve the \\nmodel’s performance. Although there are many datasets, \\nthis study used the flickr dataset which is of nearly 8k \\nimages in the dataset with 5 captions each. This study \\nincludes training the images with 6 models namely LeNet \\n5, Alexnet, VGG16, VGG19, GoogleNet, ResNet 50. The \\nLSTM is used as a RNN model. These CNN and RNN \\nmodels are concatenated to become the caption prediction \\nmodels. The performance metric used here is BLEU –\\nBilingual Evaluation Understudy which is highest for \\nGoogleNet and ResNet 50.  \\n \\nKeywords: Image Captioning, describing image, \\nCaption generator \\n \\nI. INTRODUCTION \\n \\nIn recent years, there has been a remarkable \\nsurge in research and advancements in the fields of \\ncomputer vision and natural language processing \\n(NLP). One fascinating area of intersection \\nbetween these two domains is the challenging task \\nof image captioning, where machines are trained to \\ngenerate descriptive and contextually relevant \\ntextual descriptions for images. Image captioning \\naims \\nto \\nbridge \\nthe \\ngap \\nbetween \\nvisual \\nunderstanding and linguistic expression, enabling \\nmachines to comprehend images in a manner akin \\nto human cognition.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 1. Few sample images and their Captions along with \\nBLEU score \\n \\nThroughout the course of this project, we \\nwill experiment with various state-of-the-art CNN \\narchitectures, such as VGG16, VGG19, ResNet50, \\nAlexNet, LeNet, and GoogleNet, assessing their \\nimpact on the overall performance of our image \\ncaptioning model. Figure 1 shows few images in \\nthe dataset along with their captions and BLEU \\nscore[Fig1]. \\nMotivation \\n \\nThe motivation of this project is to \\nempower machines with the ability to generate \\ndescriptive captions for images, bridging the gap \\nbetween visual understanding and language. By \\nleveraging deep learning techniques, we aim to \\nenhance accessibility for the visually impaired, \\nimprove content indexing, and facilitate seamless \\nhuman-machine interactions, revolutionizing the \\nway we interact with visual data. \\n \\nContribution \\n\\uf0b7 Implement the CNN architectures for the \\nrespective models from scratch with dense'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-08-11T16:44:15+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'total_pages': 7, 'format': 'PDF 1.5', 'title': '', 'author': 'PRANAVI PABBISETTY', 'subject': '', 'keywords': '', 'moddate': '2023-08-11T16:44:15+00:00', 'trapped': '', 'modDate': 'D:20230811164415Z', 'creationDate': \"D:20230811164415+00'00'\", 'page': 1}, page_content='layers, max pooling layers, fully connected \\nlayers and output layers.  \\n\\uf0b7 Through thorough experimentation and \\nrigorous evaluation, our project aims to \\nidentify the CNN model that achieves the \\nhighest BLEU score for image captioning. \\nBy \\nsystematically \\ncomparing \\nthe \\nperformance of each architecture, we seek \\nto determine the most effective CNN for \\ngenerating descriptive and contextually \\nrelevant image captions. \\n\\uf0b7 Analyse the results for the respective \\nmodels using visualizations and BLEU \\nscore. \\n \\nII. RELATED WORK \\nImage captioning has been a subject of extensive \\nresearch in recent years, driven by the growing \\ninterest in multimodal learning and visual-\\nlinguistic understanding. Various approaches have \\nbeen proposed to tackle this challenging task, \\nleveraging the synergy between computer vision \\nand natural language processing. Some notable \\ncontributions include: \\nImage captioning using an encoder-decoder \\nframework with a long short-term memory \\n(LSTM) network. Their model generated captions \\nby attending to relevant image regions and \\nachieved promising results on benchmark datasets \\n[2]. \\nAn alternative to the LSTM-based models \\nby employing a semantic attention mechanism that \\naligns visual and semantic features. Their approach \\ndemonstrated \\nenhanced \\ninterpretability \\nand \\ngenerated captions that align better with human \\nperception. We used the encoder and decoder \\nmodel along with LSTM and CNN.[3] \\nBottom-Up \\nand \\nTop-Down \\n(BUTD) \\nattention, which utilized object detection features \\nfrom Faster R-CNN as input to the image \\ncaptioning model. This method demonstrated \\nsuperior performance in describing fine-grained \\ndetails and complex scenes.[5] \\nWhile these previous works have made \\nsubstantial progress in image captioning, several \\nchallenges remain unaddressed. For instance, the \\ngeneration of diverse and contextually consistent \\ncaptions for images with multiple objects or \\ncomplex scenes is still an open problem. Moreover, \\nscalability and efficiency remain \\nimportant \\nconsiderations in deploying image captioning \\nmodels in real-world applications. \\n \\nIII PROPOSED METHODOLOGY \\n \\nA) Dataset Description \\nThe dataset comprises a collection of 8,000 \\nhigh-quality images, each accompanied by five \\nhuman-written captions. The images in the dataset \\nare sourced from the photo-sharing platform Flickr \\nand represent a diverse range of scenes, objects, \\nand activities. The captions provided for each \\nimage capture different aspects and perspectives, \\nmaking the dataset suitable for evaluating the \\ndiversity and contextuality of image captioning \\nmodels. \\n \\nB) Hardware and Software Used: \\nThe code is tested in both Jupyter notebook \\nand Google Colab. Graphics Processing Unit GPU \\nis used for the faster computation. The deep \\nlearning framework Tensorflow is developed by \\nGoogle can support both CPU and GPU. It is used \\nto build the CNN models. \\n \\nC) Feature Extraction \\nFeature Extraction is the main part of Computer \\nvision related projects. It involves transforming \\nraw data into a more concise, informative \\nrepresentation, capturing relevant patterns and \\ncharacteristics. Common methods include PCA, \\nLDA, and deep learning techniques like CNNs. \\nEffective feature extraction enhances model \\nperformance and facilitates better decision-making \\nin various applications. We used pickle in python to \\nstore the extracted features and to load anytime we \\nwant. \\n \\nD) Preprocessing both Text and image data \\nImages are resized according to the model’s \\ninput. Images are converted into arrays which \\nrepresent features. Text data, the captions related to \\nthe images are mapped to the respective image ids. \\nStartseq and endseq are added to the each caption. \\nTokenizer is used to tokenize the captions.  \\n \\nE) Test and Train Split \\nAll the 8k images along with their respective \\ncaptions are split into 9:1 ratio. This is because to \\nincrease the training accuracy of the model. \\n \\nTable 1. Testing and Training Split \\n \\nfIf the training data is more then we may get \\nmore accurate predictions. Table 1 explains \\nhow training and testing data are split [table1] \\n \\nRatio \\nNo \\nof \\nImages \\nNo \\nof \\nCaptions \\nTraining \\n9 \\n7290 \\n36,410 \\nTesting \\n1 \\n810 \\n4046'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-08-11T16:44:15+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'total_pages': 7, 'format': 'PDF 1.5', 'title': '', 'author': 'PRANAVI PABBISETTY', 'subject': '', 'keywords': '', 'moddate': '2023-08-11T16:44:15+00:00', 'trapped': '', 'modDate': 'D:20230811164415Z', 'creationDate': \"D:20230811164415+00'00'\", 'page': 2}, page_content='F) CNN Architectures: \\nWe have used various models to find the \\nbest model which gives accurate predictions for \\nimages. The models here are LeNet 5, AlexNet, \\nVGG16, VGG19, GoogleNet, ResNet 50 \\n \\nTable 2. CNN models \\nS \\nNo \\nArchitecture \\nConv \\nLayers \\nFully \\nconnected \\nLayers \\nTotal \\nLayers \\n1 \\nLeNet 5 \\n5 \\n3 \\n8 \\n2 \\nAlexNet \\n8 \\n3 \\n11 \\n3 \\nVGG16 \\n13 \\n3 \\n16 \\n4 \\nVGG19 \\n16 \\n3 \\n19 \\n5 \\nGoogleNet \\n22 \\n1 \\n23 \\n6 \\nResNet 50 \\n53 \\n1 \\n54 \\n \\nThe models used are used to extract the \\nimage features. Table 2 explains on how layers will \\nbe present in CNN models [table2] \\nLeNet 5: LeNet is one of the earliest CNN \\narchitectures, with relatively fewer layers and small \\nfilter sizes (typically 5x5 or smaller). It is \\ncommonly used for handwritten digit recognition \\nand serves as the foundation for modern CNNs. \\nAlexNet: AlexNet, introduced in 2012, was the \\nfirst deep CNN to demonstrate the effectiveness of \\nusing ReLU activation functions. It consists of \\neight convolutional layers and is deeper than \\nLeNet, making it a breakthrough in the field of \\ncomputer vision. \\nVGG16 and VGG19: VGG16 and VGG19 are \\ncharacterized by their uniform architecture, with \\nmany layers using small 3x3 filters. They are \\nknown for their simplicity and interpretability but \\nare computationally more expensive due to their \\ndepth. \\nGoogleNet: GoogleNet (InceptionV1) introduced \\nthe concept of inception modules, which employ \\nparallel convolutional filters of different sizes to \\ncapture multi-scale features efficiently. It was \\ndesigned to strike a balance between depth and \\ncomputational cost. \\nResNet 50: ResNet 50, part of the ResNet family, \\nis known for its deep residual architecture. It \\nintroduced skip connections, allowing gradients to \\nflow more effectively during training, enabling \\ntraining of very deep networks with improved \\nperformance. \\nHere the last layer of all the models Is removed \\nbecause we are only training the model \\n \\n \\n \\n \\n \\n \\nG) Data Pipeline \\nData \\ngenerator \\nfunction \\npreprocesses \\nimage-caption pairs for image captioning model \\ntraining. It iterates through images and associated \\ncaptions, encodes sequences, splits them into input \\nand output pairs, pads input sequences, and one-hot \\nencodes output sequences.  \\nText to sequences: Each text is converted into a \\nsequence of integers based on the vocabulary index \\nof the respective words in the text. The resulting \\nsequences can be used as inputs to train and \\nevaluate natural language processing models, such \\nas text classification, sentiment analysis, or image \\ncaptioning. \\nPad sequences: It is a frequently employed \\nfunction in natural language processing. It \\nfacilitates uniform sequence lengths by padding \\nshorter sequences with zeros and truncating longer \\nsequences. This ensures compatibility with deep \\nlearning models that demand fixed input sizes, \\nenhancing overall model performance. \\n \\n0) Concatenated Model \\nDropout is used for better performance of the \\nmodel. The input is extracted features from the \\nCNN model. Now on the other hand RNN model \\nLSTM is taken along with embedding with size of \\nvocabulary. The LSTM has 256 neurons with \\nReLU activation function. These two models are \\nconcatenated to get the final model of the Image \\ncaption generator as shown in Fig 2[Fig2]. \\n \\nFinal Image Captioning Model Flowchart \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 2 Complete flow chart of the caption generator model'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-08-11T16:44:15+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'total_pages': 7, 'format': 'PDF 1.5', 'title': '', 'author': 'PRANAVI PABBISETTY', 'subject': '', 'keywords': '', 'moddate': '2023-08-11T16:44:15+00:00', 'trapped': '', 'modDate': 'D:20230811164415Z', 'creationDate': \"D:20230811164415+00'00'\", 'page': 3}, page_content='H) Training the Model \\n \\n\\uf0b7 \\nNumber of classes in the output layer – size of \\nthe vocabulary (8485) \\n\\uf0b7 \\nOutput layer activation function – softmax \\n\\uf0b7 \\nLoss function – categorical crossentropy \\n\\uf0b7 \\nOptimizer – Adam \\n\\uf0b7 \\nMetrics – Accuracy \\n\\uf0b7 \\nBatch Size – 32 \\n \\nLoss Function: Categorical Cross-Entropy is a \\nwidely \\nused \\nloss \\nfunction \\nin \\nmulticlass \\nclassification tasks, including image captioning. It \\nmeasures the dissimilarity between predicted \\nprobability distributions and actual one-hot \\nencoded class labels. It penalizes larger differences \\nbetween predicted and true labels, encouraging the \\nmodel to learn accurate class probabilities during \\ntraining. \\n \\nOptimizer: \\nAdam \\nis \\na \\npopular \\nadaptive \\noptimization algorithm utilized in training deep \\nlearning models like image captioning. It blends \\nthe advantages of RMSprop and momentum, \\nadjusting learning rates adaptively for each \\nparameter based on their past gradients. This \\nfeature enables Adam to achieve fast and efficient \\nconvergence, making it widely favoured for \\nvarious tasks. \\n \\nBatch Size: 32 is the batch size that is used during \\nthe training to avoid memory related issues like \\nGPU crashing in Google Colab.  \\n \\nEpochs: Model is trained for 30 epochs to decrease \\nthe Loss. To avoid the problems of overfitting and \\nunderfitting this number of epochs are chosen.  \\n \\nRegularization \\nTechnique: \\nDropout \\nis \\na \\nregularization method frequently applied in deep \\nlearning to mitigate overfitting. During training, a \\nspecified \\nfraction of neurons \\nis randomly \\ndeactivated, temporarily “dropped out” of the \\nnetwork. This encourages the network to learn \\nmore robust features, enhancing generalization \\ncapabilities for unseen data \\n \\nI) Testing Phase: \\nThis testing phase is the crucial one to know \\nthat our model is working or not. New images that \\nare separated from the dataset at first are need to be \\ntested to know the performance of the model. 810 \\nimages are sent for the testing phase. These images \\nare pre-processed as same as the training images. \\nNow they are also get to the feature extraction. \\nThese are sent into the data pipeline as mentioned \\nin the above. Text to sequences and padding the \\nsequences are done on the images. \\n \\nIV.  RESULTS AND DISCUSSIONS \\nThis complete experiment is done in Google colab \\nand Jupyter Notebook using the GPU for faster \\ncomputation. The results and discussions highlight \\nthe effectiveness of the proposed image captioning \\nmodel. Significant improvements in BLEU scores \\nand caption coherence are observed. However, \\nchallenges in handling complex scenes and \\nscalability \\nare \\nacknowledged. The \\nmodel’s \\npotential for real-world applications and avenues \\nfor future research are explored as in Fig 3[Fig3]. \\n \\nTable 3 Loss for each model \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 3. Graphs depicting the loss \\n \\nFig 4 a-f represents the plots of the loss in blue plot \\nand accuracy in orange plot for all the 30 \\nepochs[Fig4]. \\n \\nIn the figure 4a, 4c and 4e we can that accuracy \\ncurves are not that much curvy. So they are not \\ngiving accurate predictions even if we increase the \\nepochs. \\n \\nThe figures 4b, 4c and 4e are the models losses and \\naccuracies \\nof \\nVGG19, \\nGoogleNet \\nand \\nResNet50.These models are giving the accurate \\nprediction when compared to the above models.  \\n \\n \\n \\n \\n \\nS No \\nModel \\nLoss \\n1 \\nLeNet 5 \\n2.5080 \\n2 \\nVGG19 \\n1.9604 \\n3 \\nAlexNet \\n2.4499 \\n4 \\nGoogleNet \\n2.0408 \\n5 \\nResNet50 \\n2.0056 \\n6 \\nVGG16 \\n2.1214'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-08-11T16:44:15+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'total_pages': 7, 'format': 'PDF 1.5', 'title': '', 'author': 'PRANAVI PABBISETTY', 'subject': '', 'keywords': '', 'moddate': '2023-08-11T16:44:15+00:00', 'trapped': '', 'modDate': 'D:20230811164415Z', 'creationDate': \"D:20230811164415+00'00'\", 'page': 4}, page_content='LeNet 5                                                                                   VGG19 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n                                  Fig 4a                                                                                              Fig 4b \\n \\n \\nAlexNet                                                                                   GoogleNet \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n                                 Fig 4c                                                                                       Fig 4d \\n \\n \\nVGG16                                                                                   ResNet 50 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n                                 Fig 4e                                                                                       Fig 4f'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-08-11T16:44:15+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'total_pages': 7, 'format': 'PDF 1.5', 'title': '', 'author': 'PRANAVI PABBISETTY', 'subject': '', 'keywords': '', 'moddate': '2023-08-11T16:44:15+00:00', 'trapped': '', 'modDate': 'D:20230811164415Z', 'creationDate': \"D:20230811164415+00'00'\", 'page': 5}, page_content='L) Performance Analysis: \\nTo analyse the performance of the model we adapt \\nthe BLEU score which is meant by Bilingual \\nEvaluation Understudy.  \\nThe BLEU (Bilingual Evaluation Understudy) \\nscore is a metric used to assess the quality of \\nmachine-generated \\ntext, \\nsuch \\nas \\nmachine \\ntranslation \\nor \\nimage \\ncaptions. \\nIt \\ninvolves \\ncalculating precision by counting matching n-\\ngrams in candidate and reference texts. The brevity \\npenalty accounts for differences in text lengths. The \\nfinal BLEU score is a geometric mean of modified \\nprecision scores, adjusted for brevity. Higher \\nBLEU scores indicate better similarity between \\ncandidate and reference texts. \\n \\nTable 4. BLEU scores for the CNN models \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 5. Bar plot for the BLEU score for two different set of weights \\nshown in [Table4] \\n \\nFig 5 explains that the Bleu score for GoogleNet and \\nResNet 50 are in the same range which is more than any \\nother models. The predicted captions with this models \\nare very accurate. Whereas for the VGG16 and LeNet 5 \\nmodels are performing very poor.[Fig5] \\n \\nPredicted Captions by different models: \\n \\nWhen we run the model using different CNN \\nmodels and LSTM we get the following predicted \\ncaptions as shown in the figures Fig 6, Fig 7 and Fig8. \\nThis is how the our model is working for different \\nmodels of CNN.   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 6 Prediction on the VGG19 and CNN model \\n \\n \\nFig 7 Prediction with the ResNet 50 CNN model \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig 8 Prediction with the GoogleNet CNN Model \\n \\n \\nS No Model \\nBLEU 1 \\nBLEU2 \\n1 \\nLeNet 5 \\n0.4547 \\n0.1960 \\n2 \\nVGG19 \\n0.5338 \\n0.3053 \\n3 \\nAlexNet \\n0.4871 \\n0.2237 \\n4 \\nGoogleNet \\n0.5514 \\n0.3294 \\n5 \\nResNet50 \\n0.5570 \\n0.3335 \\n6 \\nVGG16 \\n0.3076 \\n0.1710'),\n",
       " Document(metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2023-08-11T16:44:15+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Research Pape-pabbisetty_pranavir.pdf', 'total_pages': 7, 'format': 'PDF 1.5', 'title': '', 'author': 'PRANAVI PABBISETTY', 'subject': '', 'keywords': '', 'moddate': '2023-08-11T16:44:15+00:00', 'trapped': '', 'modDate': 'D:20230811164415Z', 'creationDate': \"D:20230811164415+00'00'\", 'page': 6}, page_content=\"From the predictions given by various models, we got \\nthe good predicted captions for the models VGG19, \\nResNet 50 and GoogleNet models along with LSTM. \\nFig 6 explains the prediction of the VGG19 and LSTM \\nmodel predicting “Two dogs are playing on the \\ngrass”[Fig6]. \\nNow when we consider another prediction for same \\ncontext dogs we get the prediction “Two dogs run \\nthrough field” in the Figure 7[Fig7]. \\nFor another prediction in GoogleNet model It is also \\nidentifying the colour of the dog and giving in the \\nprediction as shown in Fig 8[Fig8]. \\n \\nV. CONCLUSION \\nIn conclusion, our study delved into the realm \\nof image captioning, with the aim of enhancing caption \\nquality \\nand \\ndiversity. \\nThrough \\nextensive \\nexperimentation, \\nwe \\nintroduced \\nan \\ninnovative \\narchitecture that harnessed attention mechanisms and \\nmultimodal fusion. Our  \\nresults showcased notable performance enhancements, \\nevident from improved BLEU scores and more \\ncontextually relevant captions. By combining cutting-\\nedge techniques from computer vision and natural \\nlanguage processing, we've propelled the field's grasp of \\nmultimodal learning and linguistic integration. While \\nchallenges persist, particularly in complex scenes and \\nscalability, our research paves the way for refining \\ncaption diversity, tackling scalability, and real-world \\napplications like content enrichment and automated \\ndescriptions. The image captioning journey persists, and \\nour contributions offer a foundation for further \\nimpactful strides in this dynamic domain. \\n \\nVI REFERENCES \\n[1].  Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., & \\nYuille, A. (2014). Deep captioning with multimodal \\nrecurrent neural networks (m-rnn). arXiv preprint \\narXiv:1412.6632. \\n \\n[2]. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., \\nSalakhudinov, R., ... & Bengio, Y. (2015, June). Show, \\nattend and tell: Neural image caption generation with \\nvisual attention. In International conference on machine \\nlearning (pp. 2048-2057). PMLR. \\n \\n[3].  O., Toshev, A., Bengio, S., & Erhan, D. (2015). \\nShow and tell: A neural image caption generator. \\nIn Proceedings of the IEEE conference on computer \\nvision and pattern recognition (pp. 3156-3164). \\n \\n[4]. Lu, J., Xiong, C., Parikh, D., & Socher, R. (2017). \\nKnowing when to look: Adaptive attention via a visual \\nsentinel for image captioning. In Proceedings of the \\nIEEE conference on computer vision and pattern \\nrecognition (pp. 375-383). \\n \\n[5]. Anderson, P., He, X., Buehler, C., Teney, D., \\nJohnson, M., Gould, S., & Zhang, L. (2018). Bottom-up \\nand top-down attention for image captioning and visual \\nquestion answering. In Proceedings of the IEEE \\nconference \\non \\ncomputer \\nvision \\nand \\npattern \\nrecognition (pp. 6077-6086).  \\n \\n[6]. Rennie, S. J., Marcheret, E., Mroueh, Y., Ross, J., & \\nGoel, V. (2017). Self-critical sequence training for \\nimage captioning. In Proceedings of the IEEE \\nconference \\non \\ncomputer \\nvision \\nand \\npattern \\nrecognition (pp. 7008-7024). \\n \\n[7]. Hodosh, M., Young, P., & Hockenmaier, J. (2013). \\nFraming image description as a ranking task: Data, \\nmodels and evaluation metrics. Journal of Artificial \\nIntelligence Research, 47, 853-899. \\n \\n[8]. Karpathy, A., & Fei-Fei, L. (2015). Deep visual-semantic \\nalignments for generating image descriptions. In Proceedings \\nof the IEEE conference on computer vision and pattern \\nrecognition (pp. 3128-3137). \\n \\n[9]. Aneja, J., Deshpande, A., & Schwing, A. G. (2018). \\nConvolutional image captioning. In Proceedings of the IEEE \\nconference on computer vision and pattern recognition (pp. \\n5561-5570). \\n \\nDataset Link: \\n \\n \\nFlickr 8k Dataset | Kaggle \\n \\nProject Link: \\n \\n \\nppranavip/ImageCaptioningProject (github.com)\")]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading all the pdf files from the directory\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/pdf_files',\n",
    "    glob ='**/*.pdf',\n",
    "    loader_cls = PyMuPDFLoader,\n",
    ")\n",
    "\n",
    "pdf_doc = dir_loader.load()\n",
    "pdf_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb732470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='Efﬁcient Estimation of Word Representations in\\nVector Space\\nTomas Mikolov\\nGoogle Inc., Mountain View, CA\\ntmikolov@google.com\\nKai Chen\\nGoogle Inc., Mountain View, CA\\nkaichen@google.com\\nGreg Corrado\\nGoogle Inc., Mountain View, CA\\ngcorrado@google.com\\nJeffrey Dean\\nGoogle Inc., Mountain View, CA\\njeff@google.com\\nAbstract\\nWe propose two novel model architectures for computing continuous vector repre-\\nsentations of words from very large data sets. The quality of these representations\\nis measured in a word similarity task, and the results are compared to the previ-\\nously best performing techniques based on different types of neural networks. We\\nobserve large improvements in accuracy at much lower computational cost, i.e. it\\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\\nmance on our test set for measuring syntactic and semantic word similarities.\\n1\\nIntroduction\\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\\nity between words, as these are represented as indices in a vocabulary. This choice has several good\\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\\ndata outperform complex systems trained on less data. An example is the popular N-gram model\\nused for statistical language modeling - today, it is possible to train N-grams on virtually all available\\ndata (trillions of words [3]).\\nHowever, the simple techniques are at their limits in many tasks. For example, the amount of\\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\\ndominated by the size of high quality transcribed speech data (often just millions of words). In\\nmachine translation, the existing corpora for many languages contain only a few billions of words\\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\\nany signiﬁcant progress, and we have to focus on more advanced techniques.\\nWith progress of machine learning techniques in recent years, it has become possible to train more\\ncomplex models on much larger data set, and they typically outperform the simple models. Probably\\nthe most successful concept is to use distributed representations of words [10]. For example, neural\\nnetwork based language models signiﬁcantly outperform N-gram models [1, 27, 17].\\n1.1\\nGoals of the Paper\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word\\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\\nfar as we know, none of the previously proposed architectures has been successfully trained on more\\n1\\narXiv:1301.3781v3  [cs.CL]  7 Sep 2013'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='than a few hundred of millions of words, with a modest dimensionality of the word vectors between\\n50 - 100.\\nWe use recently proposed techniques for measuring the quality of the resulting vector representa-\\ntions, with the expectation that not only will similar words tend to be close to each other, but that\\nwords can have multiple degrees of similarity [20]. This has been observed earlier in the context\\nof inﬂectional languages - for example, nouns can have multiple word endings, and if we search for\\nsimilar words in a subspace of the original vector space, it is possible to ﬁnd words that have similar\\nendings [13, 14].\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple\\nsyntactic regularities. Using a word offset technique where simple algebraic operations are per-\\nformed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vec-\\ntor(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model\\narchitectures that preserve the linear regularities among words. We design a new comprehensive test\\nset for measuring both syntactic and semantic regularities1, and show that many such regularities\\ncan be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\\non the dimensionality of the word vectors and on the amount of the training data.\\n1.2\\nPrevious Work\\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to\\nlearn jointly the word vector representation and a statistical language model. This work has been\\nfollowed by many others.\\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\\nﬁrst learned using neural network with a single hidden layer. The word vectors are then used to train\\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this\\nwork, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\\nlearned using a simple model.\\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word\\nvectors were made available for future research and comparison2. However, as far as we know, these\\narchitectures were signiﬁcantly more computationally expensive for training than the one proposed\\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\\nare used [23].\\n2\\nModel Architectures\\nMany different types of models were proposed for estimating continuous representations of words,\\nincluding the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\\nIn this paper, we focus on distributed representations of words learned by neural networks, as it was\\npreviously shown that they perform signiﬁcantly better than LSA for preserving linear regularities\\namong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\\nSimilar to [18], to compare different model architectures we deﬁne ﬁrst the computational complex-\\nity of a model as the number of parameters that need to be accessed to fully train the model. Next,\\nwe will try to maximize the accuracy, while minimizing the computational complexity.\\n1The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt\\n2http://ronan.collobert.com/senna/\\nhttp://metaoptimize.com/projects/wordreprs/\\nhttp://www.fit.vutbr.cz/˜imikolov/rnnlm/\\nhttp://ai.stanford.edu/˜ehhuang/\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\1301.3781v3.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='For all the following models, the training complexity is proportional to\\nO = E × T × Q,\\n(1)\\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\\ndeﬁned further for each model architecture. Common choice is E = 3 −50 and T up to one billion.\\nAll models are trained using stochastic gradient descent and backpropagation [26].\\n2.1\\nFeedforward Neural Net Language Model (NNLM)\\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\\nusing 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a\\nprojection layer P that has dimensionality N × D, using a shared projection matrix. As only N\\ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.\\nThe NNLM architecture becomes complex for computation between the projection and the hidden\\nlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of the\\nprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000\\nunits. Moreover, the hidden layer is used to compute probability distribution over all the words in the\\nvocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity\\nper each training example is\\nQ = N × D + N × D × H + H × V,\\n(2)\\nwhere the dominating term is H × V . However, several practical solutions were proposed for\\navoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized\\nmodels completely by using models that are not normalized during training [4, 9]. With binary tree\\nrepresentations of the vocabulary, the number of output units that need to be evaluated can go down\\nto around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\\ntree. This follows previous observations that the frequency of words works well for obtaining classes\\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and\\nthis further reduces the number of output units that need to be evaluated: while balanced binary tree\\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\\nneural network LMs as the computational bottleneck is in the N ×D×H term, we will later propose\\narchitectures that do not have hidden layers and thus depend heavily on the efﬁciency of the softmax\\nnormalization.\\n2.2\\nRecurrent Neural Net Language Model (RNNLM)\\nRecurrent neural network based language model has been proposed to overcome certain limitations\\nof the feedforward NNLM, such as the need to specify the context length (the order of the model N),\\nand because theoretically RNNs can efﬁciently represent more complex patterns than the shallow\\nneural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and\\noutput layer. What is special for this type of model is the recurrent matrix that connects hidden\\nlayer to itself, using time-delayed connections. This allows the recurrent model to form some kind\\nof short term memory, as information from the past can be represented by the hidden layer state that\\ngets updated based on the current input and the state of the hidden layer in the previous time step.\\nThe complexity per training example of the RNN model is\\nQ = H × H + H × V,\\n(3)\\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\\nterm H × V can be efﬁciently reduced to H × log2(V ) by using hierarchical softmax. Most of the\\ncomplexity then comes from H × H.\\n3')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_doc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3f992",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "585e275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "textspliter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 200,  #contains the few words that from the previous chunk, that makes it relative to each other\n",
    "    length_function = len,\n",
    "    separators = [\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    ")\n",
    "\n",
    "chunks = textspliter.split_documents(pdf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9794d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " total chunks divided is: 274\n"
     ]
    }
   ],
   "source": [
    "print(f\" total chunks divided is: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412f153",
   "metadata": {},
   "source": [
    "Emebeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcf53484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Nova\\rag\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prave\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n",
      "tensor([[1.0000, 0.6660, 0.1046],\n",
      "        [0.6660, 1.0000, 0.1411],\n",
      "        [0.1046, 0.1411, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81177d45",
   "metadata": {},
   "source": [
    "#to claculate simililarity scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7bc92a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Nova\\rag\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prave\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.6071415 5.506265  6.3529854]\n",
      "Query: How many people live in Berlin?\n",
      "- #0 (8.61): Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\n",
      "- #2 (6.35): In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\n",
      "- #1 (5.51): Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nQuery: How many people live in Berlin?\\n- #0 (8.61): Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\n- #2 (6.35): In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\\n- #1 (5.51): Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# 1. Load a pretrained CrossEncoder model\n",
    "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "# The texts for which to predict similarity scores\n",
    "query = \"How many people live in Berlin?\"\n",
    "passages = [\n",
    "    \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\",\n",
    "    \"Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\",\n",
    "    \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\",\n",
    "]\n",
    "\n",
    "# 2a. Either predict scores pairs of texts\n",
    "scores = model.predict([(query, passage) for passage in passages])\n",
    "print(scores)\n",
    "# => [8.607139 5.506266 6.352977]\n",
    "\n",
    "# 2b. Or rank a list of passages for a query\n",
    "ranks = model.rank(query, passages, return_documents=True)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for rank in ranks:\n",
    "    print(f\"- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text']}\")\n",
    "\"\"\"\n",
    "Query: How many people live in Berlin?\n",
    "- #0 (8.61): Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\n",
    "- #2 (6.35): In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\n",
    "- #1 (5.51): Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda24e84",
   "metadata": {},
   "source": [
    "ChromaDB (VectorDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4c8abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id2', 'id1']], 'embeddings': None, 'documents': [['This is a document about oranges', 'This is a document about pineapple']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None, None]], 'distances': [[1.1462137699127197, 1.3015384674072266]]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# switch `create_collection` to `get_or_create_collection` to avoid creating a new collection every time\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "# switch `add` to `upsert` to avoid adding the same documents every time\n",
    "collection.upsert(\n",
    "    documents=[\n",
    "        \"This is a document about pineapple\",\n",
    "        \"This is a document about oranges\"\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\"]\n",
    ")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query document about florida\"], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6c07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa380e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
